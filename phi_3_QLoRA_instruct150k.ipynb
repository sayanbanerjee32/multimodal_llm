{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1sIyQFl-RN3d3jPVutndUA2EhUaHgDza6",
      "authorship_tag": "ABX9TyOepUtJ4cWJxqujLXt9Tfv1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1054a36aa8674e12b5230cf635ce7aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3bc78efda3c4899a8635a1c2b56f4a9",
              "IPY_MODEL_6fb67f44b3b74a9eb5836aa93b412959",
              "IPY_MODEL_09e67a78ac1a4714a92599fb89881734"
            ],
            "layout": "IPY_MODEL_b2387c882ce149b9b8641094399eaf2a"
          }
        },
        "c3bc78efda3c4899a8635a1c2b56f4a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_742252afccd2431a8344d753dea32a2f",
            "placeholder": "​",
            "style": "IPY_MODEL_4cc8a66fddc142178289d913b5784ed3",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6fb67f44b3b74a9eb5836aa93b412959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ac4934f60694d07acf59ac2036cb178",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aaf6cb549cd148f48d43a2da953fab6d",
            "value": 2
          }
        },
        "09e67a78ac1a4714a92599fb89881734": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4890c624469b471f8e915bec84898e98",
            "placeholder": "​",
            "style": "IPY_MODEL_24ec080fbeee42b88c14db7159b978bc",
            "value": " 2/2 [00:45&lt;00:00, 21.85s/it]"
          }
        },
        "b2387c882ce149b9b8641094399eaf2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "742252afccd2431a8344d753dea32a2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cc8a66fddc142178289d913b5784ed3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ac4934f60694d07acf59ac2036cb178": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaf6cb549cd148f48d43a2da953fab6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4890c624469b471f8e915bec84898e98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24ec080fbeee42b88c14db7159b978bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "329196d60643458b9bdf8d5233776263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbeeebda63984f70abd0befdb9b90c58",
              "IPY_MODEL_abc4a38938b74c8087388864bbd10430",
              "IPY_MODEL_a8ecc74c69d44980bba5b933aa8b1162"
            ],
            "layout": "IPY_MODEL_7b991aa31fc2413bbd0e2a7bf0c56326"
          }
        },
        "dbeeebda63984f70abd0befdb9b90c58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5a25b1b7e4948fda24a6913dd4eaa99",
            "placeholder": "​",
            "style": "IPY_MODEL_e39dfc490bef4a22a082cddc3f11aca8",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "abc4a38938b74c8087388864bbd10430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16d0dac25b354412b53d5f5de5248503",
            "max": 3984,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a42c86e510554402af4ece7f707e635c",
            "value": 3984
          }
        },
        "a8ecc74c69d44980bba5b933aa8b1162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e44dcb76adb4830b3525cde84ec521a",
            "placeholder": "​",
            "style": "IPY_MODEL_18280a2c3bb64bd48c9bac73cef539b1",
            "value": " 3.98k/3.98k [00:00&lt;00:00, 288kB/s]"
          }
        },
        "7b991aa31fc2413bbd0e2a7bf0c56326": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5a25b1b7e4948fda24a6913dd4eaa99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e39dfc490bef4a22a082cddc3f11aca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16d0dac25b354412b53d5f5de5248503": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a42c86e510554402af4ece7f707e635c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e44dcb76adb4830b3525cde84ec521a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18280a2c3bb64bd48c9bac73cef539b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6ffd46ea8a64f7a86b4b4dbb990e35b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8c244c07c1b4ada9f002a2a53f6f3e6",
              "IPY_MODEL_af2689e2aeee4c3cba83e90d6f0a3de5",
              "IPY_MODEL_a2edc55c2f0a4cf5b5fc75024e593333"
            ],
            "layout": "IPY_MODEL_8736e9fb2c9948098937457893c975d5"
          }
        },
        "b8c244c07c1b4ada9f002a2a53f6f3e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f84d078c65804040b8b6573a5ead511f",
            "placeholder": "​",
            "style": "IPY_MODEL_2507e74041a842c0af8ac680326b3fa5",
            "value": "tokenizer.model: 100%"
          }
        },
        "af2689e2aeee4c3cba83e90d6f0a3de5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fbb5c4a587246809f2a3bcf83d65f8c",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b1675a60ef564c51b6053dc2ab3362a9",
            "value": 499723
          }
        },
        "a2edc55c2f0a4cf5b5fc75024e593333": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89013d6784d345318b8b72222d3c302c",
            "placeholder": "​",
            "style": "IPY_MODEL_f449551179b54f7baa6bc7b7ca46b4e8",
            "value": " 500k/500k [00:00&lt;00:00, 6.70MB/s]"
          }
        },
        "8736e9fb2c9948098937457893c975d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f84d078c65804040b8b6573a5ead511f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2507e74041a842c0af8ac680326b3fa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fbb5c4a587246809f2a3bcf83d65f8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1675a60ef564c51b6053dc2ab3362a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89013d6784d345318b8b72222d3c302c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f449551179b54f7baa6bc7b7ca46b4e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "acee577583354713a080f18f21e30c79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_553a91d8e321492a899c9baed7efdca9",
              "IPY_MODEL_0d56a39a9403421c9e5c56df4a01cc2d",
              "IPY_MODEL_df076792bbfe41f68bdf1e81bdf9e304"
            ],
            "layout": "IPY_MODEL_3269072587dd4522888f45a59a44cfb6"
          }
        },
        "553a91d8e321492a899c9baed7efdca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf1f7b75ad7242fd96317b6f6eca0125",
            "placeholder": "​",
            "style": "IPY_MODEL_4d65aa45a50f47558291ebc98d6fce98",
            "value": "tokenizer.json: 100%"
          }
        },
        "0d56a39a9403421c9e5c56df4a01cc2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca648b2b67a34d5aad6bbd293bb58bfa",
            "max": 1844408,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d75c25bc3c874aaba47888f8622630ab",
            "value": 1844408
          }
        },
        "df076792bbfe41f68bdf1e81bdf9e304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0559b7fbde5a48b2adfecc2e57d0689f",
            "placeholder": "​",
            "style": "IPY_MODEL_c633a3890a9e42eb8f396d2608d1e05a",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 9.31MB/s]"
          }
        },
        "3269072587dd4522888f45a59a44cfb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf1f7b75ad7242fd96317b6f6eca0125": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d65aa45a50f47558291ebc98d6fce98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca648b2b67a34d5aad6bbd293bb58bfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d75c25bc3c874aaba47888f8622630ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0559b7fbde5a48b2adfecc2e57d0689f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c633a3890a9e42eb8f396d2608d1e05a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e9b935f6fe94fe3909df72b955adf24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_217cea68116149348a69b2c6d8c24e43",
              "IPY_MODEL_fc174c3c94fa413f9397730c4b710d36",
              "IPY_MODEL_13c2968e8ff94157bd082e90b2ae69da"
            ],
            "layout": "IPY_MODEL_32b21de93622492e9a5b2ff730fb266a"
          }
        },
        "217cea68116149348a69b2c6d8c24e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d45c5fba0e9541d394d6428f23b85ee4",
            "placeholder": "​",
            "style": "IPY_MODEL_c62e93d99f0e4ae7ab9db45734ea405b",
            "value": "added_tokens.json: 100%"
          }
        },
        "fc174c3c94fa413f9397730c4b710d36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ddd4ad3295941f6afb211736643a84f",
            "max": 306,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8495c80f3e4849259af34684161d7a5f",
            "value": 306
          }
        },
        "13c2968e8ff94157bd082e90b2ae69da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a503896b0bfb49dfbf8bba91d0338b0d",
            "placeholder": "​",
            "style": "IPY_MODEL_e894875b8a654808929dcb5943b67587",
            "value": " 306/306 [00:00&lt;00:00, 7.54kB/s]"
          }
        },
        "32b21de93622492e9a5b2ff730fb266a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d45c5fba0e9541d394d6428f23b85ee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c62e93d99f0e4ae7ab9db45734ea405b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ddd4ad3295941f6afb211736643a84f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8495c80f3e4849259af34684161d7a5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a503896b0bfb49dfbf8bba91d0338b0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e894875b8a654808929dcb5943b67587": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b93b46a2f2fa49b8b74e6c378b3834bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc99642de2474bce82d0ed2651ea1358",
              "IPY_MODEL_01aa2ea61e4a488994ad0eeb600f3371",
              "IPY_MODEL_d40b47ad7da043bdbf8019ab55abc377"
            ],
            "layout": "IPY_MODEL_c7f7ed658f85406590f90851e61ac935"
          }
        },
        "dc99642de2474bce82d0ed2651ea1358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fcfd16a263a458dbe3c2d59fd3c5785",
            "placeholder": "​",
            "style": "IPY_MODEL_a9b147372feb4dbaad69e3caebb085e6",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "01aa2ea61e4a488994ad0eeb600f3371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cac4d026eb5542829f67cf13c1aa23d9",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5588cf10b1f348af91ae0bf92d1f9b08",
            "value": 665
          }
        },
        "d40b47ad7da043bdbf8019ab55abc377": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f095ab1ce22e4d1998c056ba5a91288c",
            "placeholder": "​",
            "style": "IPY_MODEL_a22aade7e75d4b6999466e47ee2960f5",
            "value": " 665/665 [00:00&lt;00:00, 13.7kB/s]"
          }
        },
        "c7f7ed658f85406590f90851e61ac935": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fcfd16a263a458dbe3c2d59fd3c5785": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9b147372feb4dbaad69e3caebb085e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cac4d026eb5542829f67cf13c1aa23d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5588cf10b1f348af91ae0bf92d1f9b08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f095ab1ce22e4d1998c056ba5a91288c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a22aade7e75d4b6999466e47ee2960f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b980276b14844a1b9b4f44a16f1d7d36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2aa2ff7370a849b886567fe6f4047f47",
              "IPY_MODEL_ee27b8404d5a4e01afdb4eae1a2f57b6",
              "IPY_MODEL_26ba8b6c18614f00815e3f5db8d16c00"
            ],
            "layout": "IPY_MODEL_cbf2312581a8478dbfa8ed9af3ba6e35"
          }
        },
        "2aa2ff7370a849b886567fe6f4047f47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e05d355c546d4686b8402c5e0ca4e630",
            "placeholder": "​",
            "style": "IPY_MODEL_57833b07e97d4bf9bd1e01689dc4e3ce",
            "value": "Map: 100%"
          }
        },
        "ee27b8404d5a4e01afdb4eae1a2f57b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e6c407aa8214503b48cbd6e0991e1cb",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4192c0c34e844b039c6d5b8020f95f8e",
            "value": 10000
          }
        },
        "26ba8b6c18614f00815e3f5db8d16c00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_712b5e3472364d24b293781eaf023b5e",
            "placeholder": "​",
            "style": "IPY_MODEL_edb0283e7f634295a2a5998256b0466c",
            "value": " 10000/10000 [00:37&lt;00:00, 276.14 examples/s]"
          }
        },
        "cbf2312581a8478dbfa8ed9af3ba6e35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e05d355c546d4686b8402c5e0ca4e630": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57833b07e97d4bf9bd1e01689dc4e3ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e6c407aa8214503b48cbd6e0991e1cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4192c0c34e844b039c6d5b8020f95f8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "712b5e3472364d24b293781eaf023b5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edb0283e7f634295a2a5998256b0466c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c15fd9cc0ce4401ba55cbbdfacf3b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9df15b8f77cf4530beedbb807dfe510f",
              "IPY_MODEL_ab1d7576171c4ecc8f0e25504ec0f54f",
              "IPY_MODEL_8e22ee7a84b44a398d41d5579d1ef284"
            ],
            "layout": "IPY_MODEL_9d69c9af8f2a4f8b8e8600fb886077e0"
          }
        },
        "9df15b8f77cf4530beedbb807dfe510f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22e645ca1d7341df9261dbc2c8cc58fc",
            "placeholder": "​",
            "style": "IPY_MODEL_c8d3d080fc944ba5af8c45516d2fd333",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ab1d7576171c4ecc8f0e25504ec0f54f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_452c20a1638b4596b090ce1c7b7553a0",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b124ea9d7a6e4251a1780b920289f300",
            "value": 2
          }
        },
        "8e22ee7a84b44a398d41d5579d1ef284": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2daf1ccd14a4cd2a80786e90b8abbf3",
            "placeholder": "​",
            "style": "IPY_MODEL_123472d69abc49d6b3fea2a5dd30f7bc",
            "value": " 2/2 [00:35&lt;00:00, 16.97s/it]"
          }
        },
        "9d69c9af8f2a4f8b8e8600fb886077e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22e645ca1d7341df9261dbc2c8cc58fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8d3d080fc944ba5af8c45516d2fd333": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "452c20a1638b4596b090ce1c7b7553a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b124ea9d7a6e4251a1780b920289f300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d2daf1ccd14a4cd2a80786e90b8abbf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "123472d69abc49d6b3fea2a5dd30f7bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayanbanerjee32/multimodal_llm/blob/main/phi_3_QLoRA_instruct150k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q2agwLMDpJC",
        "outputId": "0f024a72-558c-4901-cc9a-439c4d28f70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.45.2\n",
        "!pip install -Uq accelerate peft bitsandbytes trl dataset\n",
        "# transformers\n",
        "# !pip install -Uq flash_attn"
      ],
      "metadata": {
        "id": "GOg3sZg0IuZf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c088546-916a-4ce1-b26e-67dceca8d3da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.45.2\n",
            "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (0.4.5)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.2)\n",
            "  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (2024.8.30)\n",
            "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed tokenizers-0.20.1 transformers-4.45.2\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import os, gc\n",
        "import subprocess\n",
        "import json\n",
        "import random\n",
        "### Download Phi-3 model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, PreTrainedModel\n",
        "from transformers.trainer_callback import TrainerCallback\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import Dataset, DatasetDict\n",
        "import joblib\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "from peft.tuners import lora\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "from bitsandbytes.nn import LinearFP4\n",
        "from huggingface_hub import hf_hub_download\n",
        "import torch.cuda.amp as amp\n",
        "import dataclasses"
      ],
      "metadata": {
        "id": "ju3h7A8AtfmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzTf0O7myIO-",
        "outputId": "e42a0c97-0417-4b00-a397-5ac881b7aecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Phi-3 model"
      ],
      "metadata": {
        "id": "YsXHaUKZM1ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Phi-3 model and tokenizer\n",
        "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "# \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"right\",\n",
        "#                                            trust_remote_code=True)\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, add_eos_token=True, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "tokenizer.padding_side = 'left'"
      ],
      "metadata": {
        "id": "RrirRzvsuYaw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "329196d60643458b9bdf8d5233776263",
            "dbeeebda63984f70abd0befdb9b90c58",
            "abc4a38938b74c8087388864bbd10430",
            "a8ecc74c69d44980bba5b933aa8b1162",
            "7b991aa31fc2413bbd0e2a7bf0c56326",
            "d5a25b1b7e4948fda24a6913dd4eaa99",
            "e39dfc490bef4a22a082cddc3f11aca8",
            "16d0dac25b354412b53d5f5de5248503",
            "a42c86e510554402af4ece7f707e635c",
            "2e44dcb76adb4830b3525cde84ec521a",
            "18280a2c3bb64bd48c9bac73cef539b1",
            "b6ffd46ea8a64f7a86b4b4dbb990e35b",
            "b8c244c07c1b4ada9f002a2a53f6f3e6",
            "af2689e2aeee4c3cba83e90d6f0a3de5",
            "a2edc55c2f0a4cf5b5fc75024e593333",
            "8736e9fb2c9948098937457893c975d5",
            "f84d078c65804040b8b6573a5ead511f",
            "2507e74041a842c0af8ac680326b3fa5",
            "9fbb5c4a587246809f2a3bcf83d65f8c",
            "b1675a60ef564c51b6053dc2ab3362a9",
            "89013d6784d345318b8b72222d3c302c",
            "f449551179b54f7baa6bc7b7ca46b4e8",
            "acee577583354713a080f18f21e30c79",
            "553a91d8e321492a899c9baed7efdca9",
            "0d56a39a9403421c9e5c56df4a01cc2d",
            "df076792bbfe41f68bdf1e81bdf9e304",
            "3269072587dd4522888f45a59a44cfb6",
            "cf1f7b75ad7242fd96317b6f6eca0125",
            "4d65aa45a50f47558291ebc98d6fce98",
            "ca648b2b67a34d5aad6bbd293bb58bfa",
            "d75c25bc3c874aaba47888f8622630ab",
            "0559b7fbde5a48b2adfecc2e57d0689f",
            "c633a3890a9e42eb8f396d2608d1e05a",
            "0e9b935f6fe94fe3909df72b955adf24",
            "217cea68116149348a69b2c6d8c24e43",
            "fc174c3c94fa413f9397730c4b710d36",
            "13c2968e8ff94157bd082e90b2ae69da",
            "32b21de93622492e9a5b2ff730fb266a",
            "d45c5fba0e9541d394d6428f23b85ee4",
            "c62e93d99f0e4ae7ab9db45734ea405b",
            "0ddd4ad3295941f6afb211736643a84f",
            "8495c80f3e4849259af34684161d7a5f",
            "a503896b0bfb49dfbf8bba91d0338b0d",
            "e894875b8a654808929dcb5943b67587",
            "b93b46a2f2fa49b8b74e6c378b3834bd",
            "dc99642de2474bce82d0ed2651ea1358",
            "01aa2ea61e4a488994ad0eeb600f3371",
            "d40b47ad7da043bdbf8019ab55abc377",
            "c7f7ed658f85406590f90851e61ac935",
            "6fcfd16a263a458dbe3c2d59fd3c5785",
            "a9b147372feb4dbaad69e3caebb085e6",
            "cac4d026eb5542829f67cf13c1aa23d9",
            "5588cf10b1f348af91ae0bf92d1f9b08",
            "f095ab1ce22e4d1998c056ba5a91288c",
            "a22aade7e75d4b6999466e47ee2960f5"
          ]
        },
        "outputId": "be685ea5-ef49-4a7d-d1ab-da20557a43fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/3.98k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "329196d60643458b9bdf8d5233776263"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6ffd46ea8a64f7a86b4b4dbb990e35b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acee577583354713a080f18f21e30c79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e9b935f6fe94fe3909df72b955adf24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b93b46a2f2fa49b8b74e6c378b3834bd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downlaod image embedding"
      ],
      "metadata": {
        "id": "uDm2WxI-N5bh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QyH6rCfDXRN",
        "outputId": "73a16584-d36b-4dc4-d0f6-3ae27745636e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embeddings...\n",
            "Image: 000000401144.jpg\n",
            "Embedding shape: (512,)\n",
            "Embedding preview: [-0.13     0.1564   0.02017  0.1678   0.2393 ]...\n",
            "--------------------------------------------------\n",
            "Total number of embeddings: 81479\n"
          ]
        }
      ],
      "source": [
        "# URL of the embeddings file (replace with your actual URL)\n",
        "embeddings_url = '/content/drive/MyDrive/multimodal_llm/image_embedding/coco_image_embeddings.npz'\n",
        "\n",
        "# Load the embeddings\n",
        "print(\"Loading embeddings...\")\n",
        "embeddings = np.load(embeddings_url, allow_pickle=True)\n",
        "\n",
        "# Print embeddings and image names\n",
        "for image_name, embedding in embeddings.items():\n",
        "    print(f\"Image: {image_name}\")\n",
        "    print(f\"Embedding shape: {embedding.shape}\")\n",
        "    print(f\"Embedding preview: {embedding[:5]}...\")  # Print first 5 values\n",
        "    print(\"-\" * 50)\n",
        "    break\n",
        "\n",
        "print(f\"Total number of embeddings: {len(embeddings)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data processing"
      ],
      "metadata": {
        "id": "NwCmU3H-NABj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of URLs to download\n",
        "url = \"https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_150k.json\"\n",
        "\n",
        "# Download each file\n",
        "subprocess.run([\"wget\", \"-c\", url])\n",
        "\n",
        "# Load the downloaded JSON file\n",
        "json_file = \"llava_instruct_150k.json\"\n",
        "with open(json_file, 'r') as f:\n",
        "    data = json.load(f)\n"
      ],
      "metadata": {
        "id": "homAn3isNCOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data = data[0:10000]"
      ],
      "metadata": {
        "id": "-l600Sm5nOEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset():\n",
        "    processed_data = []\n",
        "    print(\"Processing data...\")\n",
        "    with tqdm(total=len(data)) as pbar:\n",
        "        for item in data:\n",
        "            image_file = item['image']\n",
        "            if image_file in embeddings:\n",
        "                processed_data.append({\n",
        "                    'image': image_file,\n",
        "                    'image_embedding': embeddings[image_file].tolist(),\n",
        "                    'conversation': item['conversations']\n",
        "                })\n",
        "            pbar.update(1)\n",
        "\n",
        "    print(f\"Data processing completed. Total processed items: {len(processed_data)}\")\n",
        "\n",
        "    return Dataset.from_dict({\n",
        "        \"image\": [item['image'] for item in processed_data],\n",
        "        \"image_embedding\": [item['image_embedding'] for item in processed_data],\n",
        "        \"conversation\": [item['conversation'] for item in processed_data]\n",
        "    })\n",
        "\n",
        "print(\"Creating HuggingFace dataset...\")\n",
        "hf_dataset = create_dataset()\n",
        "\n",
        "print(\"HuggingFace dataset creation completed.\")\n",
        "print(f\"Total samples in dataset: {len(hf_dataset)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OZtei2WxHlQ",
        "outputId": "08264245-9ae4-4b46-8596-dd6a7df527ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating HuggingFace dataset...\n",
            "Processing data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:57<00:00, 173.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data processing completed. Total processed items: 10000\n",
            "HuggingFace dataset creation completed.\n",
            "Total samples in dataset: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(examples):\n",
        "    image_embeddings = []\n",
        "    conversations = []\n",
        "\n",
        "    for idx, conv in enumerate(examples['conversation']):\n",
        "        image_embedding = torch.tensor(examples['image_embedding'][idx])\n",
        "        dialogue_pairs = []\n",
        "\n",
        "        for i in range(0, len(conv), 2):\n",
        "            if i + 1 < len(conv):  # Ensure we have a pair\n",
        "                human_msg = conv[i]['value'].replace('<image>', '').strip()\n",
        "                gpt_msg = conv[i + 1]['value']\n",
        "\n",
        "                dialogue = [\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Given the following information, provide a detailed and accurate response:\\n{human_msg}\\n[An image is provided for this task.]\\n\"},\n",
        "                    {\"role\": \"assistant\", \"content\": gpt_msg}\n",
        "                ]\n",
        "\n",
        "                dialogue_pairs.append(dialogue)\n",
        "                image_embeddings.append(image_embedding)\n",
        "\n",
        "        conversations.extend(dialogue_pairs)\n",
        "\n",
        "    image_embeddings = torch.stack(image_embeddings)\n",
        "\n",
        "    tokenized_conversations = tokenizer.apply_chat_template(conversations,\n",
        "                                                            return_tensors='pt', padding=True)\n",
        "\n",
        "    return {\n",
        "        \"image_embeddings\": image_embeddings,\n",
        "        \"input_ids\": tokenized_conversations,\n",
        "        \"attention_mask\": torch.ones_like(tokenized_conversations),\n",
        "        \"labels\": tokenized_conversations.clone()\n",
        "    }\n",
        "\n",
        "# Test the prepare_dataset function with a real training example\n",
        "def test_prepare_dataset():\n",
        "    # Get a batch of examples from the dataset\n",
        "    batch_size = 1  # You can adjust this as needed\n",
        "    sample_batch = hf_dataset[5:5+batch_size]\n",
        "\n",
        "    print(\"Original conversations:\")\n",
        "    # for i, sample in enumerate(sample_batch):\n",
        "    #     print(f\"\\nSample {i + 1}:\")\n",
        "    for message in sample_batch['conversation'][0]:\n",
        "        print(f\"{message['from']}: {message['value']}\")\n",
        "\n",
        "    # Process the sample batch\n",
        "    result = prepare_dataset(sample_batch)\n",
        "\n",
        "    # Print the structure of the result\n",
        "    print(\"\\nResult keys:\", result.keys())\n",
        "    print(\"Image embeddings shape:\", result['image_embeddings'].shape)\n",
        "    print(\"Input IDs shape:\", result['input_ids'].shape)\n",
        "    print(\"Attention mask shape:\", result['attention_mask'].shape)\n",
        "    print(\"Labels shape:\", result['labels'].shape)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        decoded_input = tokenizer.decode(result['input_ids'][i])\n",
        "        decoded_labels = tokenizer.decode(result['labels'][i])\n",
        "\n",
        "        print(f\"\\nRestructured input for sample {i + 1}:\")\n",
        "        print(decoded_input)\n",
        "\n",
        "        print(f\"\\nLabels for sample {i + 1}:\")\n",
        "        print(decoded_labels)\n",
        "\n",
        "        # Optionally, you can print a more readable version of the labels\n",
        "        print(\"\\nReadable labels (non-padding tokens):\")\n",
        "        readable_labels = tokenizer.decode([token for token in result['labels'][i] if token != -100])\n",
        "        print(readable_labels)\n",
        "\n",
        "    # Optionally, you can print attention mask to see where it's applied\n",
        "    print(\"\\nAttention Mask:\")\n",
        "    print(result['attention_mask'][0])\n",
        "# Run the test\n",
        "test_prepare_dataset()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnMLoUn32aLf",
        "outputId": "9fd9d98c-6d47-4b73-9754-e0041ca3a830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original conversations:\n",
            "human: <image>\n",
            "What is the girl eating in the image?\n",
            "gpt: The girl in the image is eating a dessert, which appears to be a graham cracker treat or a cookie sandwich.\n",
            "human: Describe the girl's hair color and clothing.\n",
            "gpt: The girl has blonde hair, and she is wearing a pink shirt.\n",
            "human: What color is the plate that the dessert is on?\n",
            "gpt: The dessert is on a green plate.\n",
            "human: Is the girl looking at the camera or focusing on her dessert?\n",
            "gpt: The girl is looking up at the camera while taking a bite of her dessert.\n",
            "human: Where is the girl eating her dessert?\n",
            "gpt: The girl is eating her dessert at the table.\n",
            "\n",
            "Result keys: dict_keys(['image_embeddings', 'input_ids', 'attention_mask', 'labels'])\n",
            "Image embeddings shape: torch.Size([5, 512])\n",
            "Input IDs shape: torch.Size([5, 75])\n",
            "Attention mask shape: torch.Size([5, 75])\n",
            "Labels shape: torch.Size([5, 75])\n",
            "\n",
            "Restructured input for sample 1:\n",
            "<|system|> You are a helpful assistant.<|end|><|user|> Given the following information, provide a detailed and accurate response:\n",
            "What is the girl eating in the image?\n",
            "[An image is provided for this task.]\n",
            "<|end|><|assistant|> The girl in the image is eating a dessert, which appears to be a graham cracker treat or a cookie sandwich.<|end|><|endoftext|>\n",
            "\n",
            "Labels for sample 1:\n",
            "<|system|> You are a helpful assistant.<|end|><|user|> Given the following information, provide a detailed and accurate response:\n",
            "What is the girl eating in the image?\n",
            "[An image is provided for this task.]\n",
            "<|end|><|assistant|> The girl in the image is eating a dessert, which appears to be a graham cracker treat or a cookie sandwich.<|end|><|endoftext|>\n",
            "\n",
            "Readable labels (non-padding tokens):\n",
            "<|system|> You are a helpful assistant.<|end|><|user|> Given the following information, provide a detailed and accurate response:\n",
            "What is the girl eating in the image?\n",
            "[An image is provided for this task.]\n",
            "<|end|><|assistant|> The girl in the image is eating a dessert, which appears to be a graham cracker treat or a cookie sandwich.<|end|><|endoftext|>\n",
            "\n",
            "Attention Mask:\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply tokenization and prepare the dataset\n",
        "print(\"Applying tokenization and preparing the dataset...\")\n",
        "\n",
        "hf_dataset_mapped = hf_dataset.map(\n",
        "    prepare_dataset,\n",
        "    batched=True,\n",
        "    remove_columns=hf_dataset.column_names,\n",
        "    batch_size=1024  # Adjust based on your memory constraints\n",
        ").with_format(\"torch\")\n",
        "\n",
        "# Split the dataset\n",
        "train_test_split = hf_dataset_mapped.train_test_split(test_size=0.05)\n",
        "\n",
        "# Create a DatasetDict\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': train_test_split['train'],\n",
        "    'test': train_test_split['test']\n",
        "})\n",
        "\n",
        "print(f\"Train dataset size: {len(dataset_dict['train'])}\")\n",
        "print(f\"Test dataset size: {len(dataset_dict['test'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "b980276b14844a1b9b4f44a16f1d7d36",
            "2aa2ff7370a849b886567fe6f4047f47",
            "ee27b8404d5a4e01afdb4eae1a2f57b6",
            "26ba8b6c18614f00815e3f5db8d16c00",
            "cbf2312581a8478dbfa8ed9af3ba6e35",
            "e05d355c546d4686b8402c5e0ca4e630",
            "57833b07e97d4bf9bd1e01689dc4e3ce",
            "0e6c407aa8214503b48cbd6e0991e1cb",
            "4192c0c34e844b039c6d5b8020f95f8e",
            "712b5e3472364d24b293781eaf023b5e",
            "edb0283e7f634295a2a5998256b0466c"
          ]
        },
        "id": "lbI3OPGyN3Br",
        "outputId": "02388506-47f1-4b79-ae7b-de5e4cbb1aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying tokenization and preparing the dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b980276b14844a1b9b4f44a16f1d7d36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 43134\n",
            "Test dataset size: 2271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of accessing an item:\n",
        "sample = dataset_dict['train'][0]\n",
        "print(f\"Input IDs shape: {len(sample['input_ids'])}\")\n",
        "print(f\"Attention mask shape: {len(sample['attention_mask'])}\")\n",
        "print(f\"Labels shape: {len(sample['labels'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ky0l7yUc7avS",
        "outputId": "6786b3cd-0a66-4442-d555-d8a8bcd3f523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs shape: 673\n",
            "Attention mask shape: 673\n",
            "Labels shape: 673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Projection Layer"
      ],
      "metadata": {
        "id": "inHiIuFzMxq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QLoRA set up"
      ],
      "metadata": {
        "id": "Sb90y2RNM6ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new_model = \"ms-phi3-custom\"\n",
        "lora_r = 16 #32 #64\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.05\n",
        "use_4bit = True\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "use_nested_quant = False\n",
        "output_dir = \"./multimodal-phi3_5-mini-instruct-llava_adapter\"\n",
        "num_train_epochs = 1\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "per_device_train_batch_size = 8\n",
        "per_device_eval_batch_size = 4\n",
        "gradient_accumulation_steps = 4\n",
        "gradient_checkpointing = True\n",
        "max_grad_norm = 1.0 #0.3\n",
        "learning_rate = 5e-4\n",
        "weight_decay = 0.0 #0.001\n",
        "optim = \"adamw_torch\" #\"paged_adamw_32bit\"\n",
        "lr_scheduler_type = \"linear\" #\"constant\"\n",
        "max_steps = -1\n",
        "warmup_ratio = 0.1 #0.03\n",
        "group_by_length = True\n",
        "save_steps = 25\n",
        "logging_steps = 25\n",
        "eval_steps = 5 # Evaluate every 25 steps\n",
        "max_seq_length = 256\n",
        "packing = False\n",
        "device_map = {\"\": 0}\n",
        "hf_adapter_repo=\"sayanbanerjee32/multimodal-phi3_5-mini-instruct-llava_adapter\""
      ],
      "metadata": {
        "id": "Mx6UI-r1M-0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageProjector(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim=1024):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.activation = nn.GELU()\n",
        "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.05)\n",
        "\n",
        "        # Store initial weights for both layers\n",
        "        self.register_buffer('initial_weights1', self.layer1.weight.data.clone())\n",
        "        self.register_buffer('initial_weights2', self.layer2.weight.data.clone())\n",
        "\n",
        "    def forward(self, x):\n",
        "        # # Print dtypes\n",
        "        # print(f\"Input dtype: {x.dtype}\")\n",
        "        # print(f\"Layer1 weight dtype: {self.layer1.weight.dtype}\")\n",
        "        # print(f\"Layer1 bias dtype: {self.layer1.bias.dtype}\")\n",
        "        # print(f\"Layer2 weight dtype: {self.layer2.weight.dtype}\")\n",
        "        # print(f\"Layer2 bias dtype: {self.layer2.bias.dtype}\")\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "    def get_weight_change(self):\n",
        "        current_weights1 = self.layer1.weight.data\n",
        "        current_weights2 = self.layer2.weight.data\n",
        "\n",
        "        # Ensure all tensors are on the same device\n",
        "        device = current_weights1.device\n",
        "        initial_weights1 = self.initial_weights1.to(device)\n",
        "        initial_weights2 = self.initial_weights2.to(device)\n",
        "\n",
        "        weight_diff1 = torch.norm(current_weights1 - initial_weights1).item()\n",
        "        weight_diff2 = torch.norm(current_weights2 - initial_weights2).item()\n",
        "        return weight_diff1 + weight_diff2  # Total weight change across both layers\n",
        "\n",
        "class Phi3WithProjector(PreTrainedModel):\n",
        "    supports_gradient_checkpointing = True\n",
        "    _supports_sdpa = True # Add this line\n",
        "\n",
        "    def __init__(self, phi3_model, projector, debug=False):\n",
        "        super().__init__(phi3_model.config)\n",
        "        self.phi3 = phi3_model\n",
        "        self.projector = projector\n",
        "        self.debug = debug\n",
        "\n",
        "    def debug_print(self, *args, **kwargs):\n",
        "        if self.debug:\n",
        "            print(*args, **kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, debug=False, **kwargs):\n",
        "        # Load the base Phi-3 model\n",
        "        phi3_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n",
        "\n",
        "        # Determine if it's a local path or a Hugging Face model ID\n",
        "        is_local = os.path.isdir(pretrained_model_name_or_path)\n",
        "\n",
        "        if is_local:\n",
        "            projector_path = os.path.join(pretrained_model_name_or_path, \"image_projector.pth\")\n",
        "        else:\n",
        "            try:\n",
        "                # Try to download the projector weights from the Hugging Face Hub\n",
        "                projector_path = hf_hub_download(repo_id=pretrained_model_name_or_path, filename=\"image_projector.pth\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to download projector weights: {e}\")\n",
        "                projector_path = None\n",
        "\n",
        "        if projector_path and os.path.exists(projector_path):\n",
        "            projector_state_dict = torch.load(projector_path, map_location=phi3_model.device)\n",
        "            # Check if the state dict has the expected structure\n",
        "            if 'linear.weight' in projector_state_dict:\n",
        "                input_dim = projector_state_dict['linear.weight'].size(1)\n",
        "                output_dim = projector_state_dict['linear.weight'].size(0)\n",
        "            else:\n",
        "                # If not, try to infer dimensions from the first layer's weight\n",
        "                first_key = next(iter(projector_state_dict))\n",
        "                input_dim = projector_state_dict[first_key].size(1)\n",
        "                output_dim = phi3_model.config.hidden_size  # Assuming this is the correct output dimension\n",
        "\n",
        "            projector = ImageProjector(input_dim, output_dim)\n",
        "\n",
        "            # # Convert projector weights and biases to the same dtype as the main model\n",
        "            # target_dtype = kwargs.get('torch_dtype', torch.float32)\n",
        "            # projector_state_dict = {k: v.to(target_dtype) for k, v in projector_state_dict.items()}\n",
        "\n",
        "            # Load the state dict with converted weights and biases\n",
        "            projector.load_state_dict(projector_state_dict, strict=False)\n",
        "\n",
        "            # # Ensure all parameters (including biases) are in the correct dtype\n",
        "            # for param in projector.parameters():\n",
        "            #     param.data = param.data.to(target_dtype)\n",
        "\n",
        "            print(f\"Loaded projector with input_dim={input_dim}, output_dim={output_dim}\")#, dtype={target_dtype}\")\n",
        "        else:\n",
        "            print(f\"Projector weights not found. Initializing with default dimensions.\")\n",
        "            input_dim = 512  # Default CLIP embedding size\n",
        "            output_dim = phi3_model.config.hidden_size\n",
        "            # target_dtype = kwargs.get('torch_dtype', torch.float32)\n",
        "            projector = ImageProjector(input_dim, output_dim)\n",
        "            # Ensure all parameters (including biases) are in the correct dtype\n",
        "            # for param in projector.parameters():\n",
        "            #     param.data = param.data.to(target_dtype)\n",
        "\n",
        "        # Move the projector to the same device as phi3_model\n",
        "        projector = projector.to(phi3_model.device)\n",
        "\n",
        "        # Create and return the Phi3WithProjector instance\n",
        "        model = cls(phi3_model, projector, debug=debug)\n",
        "        return model\n",
        "\n",
        "    def save_pretrained(self, save_directory):\n",
        "        print(f\"Saving model to {save_directory}\")\n",
        "\n",
        "        # Save the base model\n",
        "        self.phi3.save_pretrained(save_directory)\n",
        "\n",
        "        # Save the projector weights\n",
        "        projector_path = os.path.join(save_directory, \"image_projector.pth\")\n",
        "        projector_state = self.projector.state_dict()\n",
        "        print(f\"Projector weights stats before saving:\")\n",
        "        for name, param in projector_state.items():\n",
        "            print(f\"  {name}: mean={param.mean().item():.4f}, std={param.std().item():.4f}\")\n",
        "        torch.save(projector_state, projector_path)\n",
        "\n",
        "        # Save the config\n",
        "        self.config.save_pretrained(save_directory)\n",
        "\n",
        "        print(f\"Model saved successfully to {save_directory}\")\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None, image_embeddings=None, past_key_values=None, **kwargs):\n",
        "        with amp.autocast():\n",
        "            device = next(self.parameters()).device\n",
        "\n",
        "            if image_embeddings is not None:\n",
        "                projected_embeddings = self.projector(image_embeddings).to(torch.float16)\n",
        "                # Ensure projected_embeddings requires grad\n",
        "                if not projected_embeddings.requires_grad:\n",
        "                    projected_embeddings.requires_grad_(True)\n",
        "                projected_embeddings = projected_embeddings.unsqueeze(1)\n",
        "                self.debug_print(f\"forward projected_embeddings: {projected_embeddings.size()}\")\n",
        "\n",
        "                if past_key_values is None:  # This is the first forward pass\n",
        "                    self.debug_print(f\"forward before: {attention_mask.size() if attention_mask is not None else None}\")\n",
        "                    if 'inputs_embeds' in kwargs and kwargs['inputs_embeds'] is not None:\n",
        "                        inputs_embeds = kwargs['inputs_embeds']\n",
        "                        self.debug_print(f\"forward before inputs_embeds: {inputs_embeds.size()}\")\n",
        "                        inputs_embeds = torch.cat([projected_embeddings, inputs_embeds], dim=1)\n",
        "                        kwargs['inputs_embeds'] = inputs_embeds\n",
        "                        self.debug_print(f\"forward after inputs_embeds: {inputs_embeds.size()}\")\n",
        "                    elif input_ids is not None:\n",
        "                        self.debug_print(f\"forward input_ids: {input_ids.size()}\")\n",
        "                        inputs_embeds = self.get_input_embeddings()(input_ids.to(device))\n",
        "                        self.debug_print(f\"forward before inputs_embeds: {inputs_embeds.size()}\")\n",
        "                        inputs_embeds = torch.cat([projected_embeddings, inputs_embeds], dim=1)\n",
        "                        self.debug_print(f\"forward after inputs_embeds: {inputs_embeds.size()}\")\n",
        "                        kwargs['inputs_embeds'] = inputs_embeds\n",
        "                        input_ids = None  # Set to None to avoid conflict\n",
        "\n",
        "                    if attention_mask is not None:\n",
        "                        attention_mask = torch.cat([torch.ones(image_embeddings.size(0), 1, device=device), attention_mask.to(device)], dim=1)\n",
        "                    else:\n",
        "                        attention_mask = torch.ones(image_embeddings.size(0), inputs_embeds.size(1), device=device)\n",
        "\n",
        "                    if labels is not None:\n",
        "                        # Adjust labels to match the new sequence length\n",
        "                        labels = torch.cat([torch.full((labels.size(0), 1), -100, device=device), labels], dim=1)\n",
        "\n",
        "            if labels is not None:\n",
        "                labels = labels.to(device)\n",
        "\n",
        "            # Determine sequence length\n",
        "            if 'inputs_embeds' in kwargs and kwargs['inputs_embeds'] is not None:\n",
        "                seq_length = kwargs['inputs_embeds'].size(1)\n",
        "            elif input_ids is not None:\n",
        "                seq_length = input_ids.size(1)\n",
        "            else:\n",
        "                seq_length = attention_mask.size(1) if attention_mask is not None else None\n",
        "\n",
        "            if seq_length is None:\n",
        "                raise ValueError(\"Unable to determine sequence length. Provide either input_ids, inputs_embeds, or attention_mask.\")\n",
        "\n",
        "            # Ensure attention_mask matches the sequence length\n",
        "            if attention_mask is not None:\n",
        "                attention_mask = attention_mask[:, :seq_length]\n",
        "\n",
        "            self.debug_print(f\"forward final: input_ids shape: {input_ids.shape if input_ids is not None else None}\")\n",
        "            self.debug_print(f\"forward final: attention_mask shape: {attention_mask.shape if attention_mask is not None else None}\")\n",
        "            self.debug_print(f\"forward final: inputs_embeds shape: {kwargs.get('inputs_embeds', {}).shape if kwargs.get('inputs_embeds') is not None else None}\")\n",
        "\n",
        "            return self.phi3(input_ids=input_ids, attention_mask=attention_mask, labels=labels, past_key_values=past_key_values, **kwargs)\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **kwargs):\n",
        "        inputs = self.phi3.prepare_inputs_for_generation(input_ids, past=past, attention_mask=attention_mask, **kwargs)\n",
        "\n",
        "        if 'image_embeddings' in kwargs:\n",
        "            inputs['image_embeddings'] = kwargs['image_embeddings']\n",
        "\n",
        "            if past is None:  # First forward pass\n",
        "                # Adjust attention_mask to account for the image token\n",
        "                if attention_mask is not None:\n",
        "                    inputs['attention_mask'] = torch.cat([torch.ones((attention_mask.size(0), 1), device=attention_mask.device), attention_mask], dim=1)\n",
        "            else:  # Subsequent passes\n",
        "                # Ensure attention_mask matches the current sequence length\n",
        "                if attention_mask is not None:\n",
        "                    current_seq_length = past[0][0].size(2) + 1  # past key's sequence length + 1 for the new token\n",
        "                    inputs['attention_mask'] = attention_mask[:, :current_seq_length]\n",
        "\n",
        "            inputs.pop('position_ids', None)\n",
        "\n",
        "        # Safe printing of shapes\n",
        "        self.debug_print(f\"prepare_inputs_for_generation: input_ids shape: {inputs['input_ids'].shape if 'input_ids' in inputs else None}\")\n",
        "        self.debug_print(f\"prepare_inputs_for_generation: attention_mask shape: {inputs['attention_mask'].shape if 'attention_mask' in inputs else None}\")\n",
        "        self.debug_print(f\"prepare_inputs_for_generation: inputs_embeds shape: {inputs.get('inputs_embeds', {}).shape if inputs.get('inputs_embeds') is not None else None}\")\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.phi3.get_input_embeddings()\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.phi3.set_input_embeddings(value)\n",
        "\n",
        "    def gradient_checkpointing_enable(self, **kwargs):\n",
        "        self.phi3.gradient_checkpointing_enable(**kwargs)\n",
        "\n",
        "    def gradient_checkpointing_disable(self):\n",
        "        self.phi3.gradient_checkpointing_disable()\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        try:\n",
        "            return super().__getattr__(name)\n",
        "        except AttributeError:\n",
        "            return getattr(self.phi3, name)\n",
        "\n",
        "    def generate(self, input_ids=None, attention_mask=None, image_embeddings=None, **kwargs):\n",
        "        if image_embeddings is not None:\n",
        "            kwargs['image_embeddings'] = image_embeddings\n",
        "            self.debug_print(f\"generate input_ids: {input_ids.size()}\")\n",
        "            self.debug_print(f\"generate image_embedding: {image_embeddings.size()}\")\n",
        "\n",
        "        if attention_mask is not None and image_embeddings is not None:\n",
        "            # Add an extra attention mask token for the image embedding\n",
        "            self.debug_print(f\"generate before: {attention_mask.size()}\")\n",
        "            attention_mask = torch.cat([torch.ones(attention_mask.size(0), 1, device=attention_mask.device), attention_mask], dim=1)\n",
        "            self.debug_print(f\"generate after: {attention_mask.size()}\")\n",
        "\n",
        "        return super().generate(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n"
      ],
      "metadata": {
        "id": "-BBqvYmtS8ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path in Google Drive where you want to save the checkpoints\n",
        "gdrive_checkpoint_dir = \"/content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints\"\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(gdrive_checkpoint_dir, exist_ok=True)\n",
        "\n",
        "class SaveLatestCheckpointAndLoraCallback(TrainerCallback):\n",
        "    # def __init__(self, tokenizer):\n",
        "    #     super().__init__()\n",
        "    #     self.tokenizer = tokenizer\n",
        "\n",
        "    def on_save(self, args, state, control, **kwargs):\n",
        "        if state.is_world_process_zero:\n",
        "            checkpoint_dir = os.path.join(gdrive_checkpoint_dir, f\"checkpoint-{state.global_step}\")\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "            # Save the model and tokenizer\n",
        "            kwargs[\"model\"].save_pretrained(checkpoint_dir)\n",
        "            # Save the tokenizer\n",
        "            kwargs[\"tokenizer\"].save_pretrained(checkpoint_dir)\n",
        "            # self.tokenizer.save_pretrained(checkpoint_dir)\n",
        "\n",
        "            # Log and save the projector\n",
        "            projector = kwargs[\"model\"].base_model.model.projector\n",
        "            weight_change = projector.get_weight_change()\n",
        "            print(f\"Projector weight change at step {state.global_step}: {weight_change}\")\n",
        "\n",
        "            # # Add gradient checking here\n",
        "            # for name, param in projector.named_parameters():\n",
        "            #     if param.grad is None:\n",
        "            #         print(f\"No gradient for {name}\")\n",
        "            #     else:\n",
        "            #         print(f\"Gradient norm for {name}: {param.grad.norm().item()}\")\n",
        "\n",
        "            projector_path = os.path.join(checkpoint_dir, \"image_projector.pth\")\n",
        "            torch.save(projector.state_dict(), projector_path)\n",
        "\n",
        "            # Save LoRA weights\n",
        "            lora_state_dict = {}\n",
        "            for name, module in kwargs[\"model\"].base_model.model.phi3.named_modules():\n",
        "                if isinstance(module, lora.Linear4bit):\n",
        "                    if hasattr(module, 'lora_A'):\n",
        "                        lora_state_dict[f\"{name}.lora_A.weight\"] = module.lora_A.default.weight.data.cpu()\n",
        "                        lora_state_dict[f\"{name}.lora_B.weight\"] = module.lora_B.default.weight.data.cpu()\n",
        "                        lora_state_dict[f\"{name}.scaling\"] = module.scaling\n",
        "\n",
        "                    # Save lora_embedding_A and lora_embedding_B if they exist and are not empty\n",
        "                    if hasattr(module, 'lora_embedding_A') and module.lora_embedding_A:\n",
        "                        lora_state_dict[f\"{name}.lora_embedding_A\"] = {k: v.cpu() for k, v in module.lora_embedding_A.items()}\n",
        "                    if hasattr(module, 'lora_embedding_B') and module.lora_embedding_B:\n",
        "                        lora_state_dict[f\"{name}.lora_embedding_B\"] = {k: v.cpu() for k, v in module.lora_embedding_B.items()}\n",
        "\n",
        "            torch.save(lora_state_dict, os.path.join(checkpoint_dir, \"lora_weights.pt\"))\n",
        "\n",
        "            # Explicitly save the trainer state\n",
        "            trainer_state_path = os.path.join(checkpoint_dir, \"trainer_state.json\")\n",
        "            state_dict = dataclasses.asdict(state)\n",
        "            with open(trainer_state_path, \"w\") as f:\n",
        "                json.dump(state_dict, f, indent=2)\n",
        "\n",
        "            # Remove previous checkpoint\n",
        "            prev_checkpoint = os.path.join(gdrive_checkpoint_dir, f\"checkpoint-{state.global_step - args.save_steps}\")\n",
        "            if os.path.exists(prev_checkpoint):\n",
        "                import shutil\n",
        "                shutil.rmtree(prev_checkpoint)\n",
        "\n",
        "            print(f\"Saved checkpoint to {checkpoint_dir}\")\n",
        "\n",
        "    #         # Upload the checkpoint to Hugging Face Hub\n",
        "    #         self.upload_to_hub(checkpoint_dir, args.hub_model_id)\n",
        "\n",
        "    # def upload_to_hub(self, checkpoint_dir, hub_model_id):\n",
        "    #     api = HfApi()\n",
        "    #     api.upload_folder(\n",
        "    #         folder_path=checkpoint_dir,\n",
        "    #         repo_id=hub_model_id,\n",
        "    #         repo_type=\"model\",\n",
        "    #     )\n",
        "    #     print(f\"Uploaded checkpoint {checkpoint_dir} to Hugging Face Hub\")\n",
        "\n",
        "# Function to get the latest checkpoint\n",
        "def get_latest_checkpoint(checkpoint_dir):\n",
        "    checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith('checkpoint-')]\n",
        "    if not checkpoints:\n",
        "        return None\n",
        "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[1]))\n",
        "    return os.path.join(checkpoint_dir, latest_checkpoint)"
      ],
      "metadata": {
        "id": "CB_fUxzpiBUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_bf16_supported():\n",
        "  compute_dtype = torch.bfloat16\n",
        "#   attn_implementation = 'flash_attention_2'\n",
        "else:\n",
        "  compute_dtype = torch.float16\n",
        "#   attn_implementation = 'sdpa'\n",
        "\n",
        "# print(attn_implementation)\n",
        "print(compute_dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM0oP2VqCN64",
        "outputId": "2ef70054-6f98-4397-fc9a-258f3472594e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.bfloat16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "# # Load the model again for quantization\n",
        "# ### Download Phi-3 model\n",
        "# phi3_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_name,\n",
        "#     trust_remote_code=True,\n",
        "#     quantization_config=bnb_config,\n",
        "#     device_map=device_map,\n",
        "#     torch_dtype=compute_dtype,\n",
        "#     # attn_implementation=attn_implementation\n",
        "# )\n",
        "\n",
        "# print(phi3_model)"
      ],
      "metadata": {
        "id": "kgE8Vq_i6kMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_lora_weights(checkpoint_path, device):\n",
        "    # Check if the checkpoint_path is a local directory or a Hugging Face repo\n",
        "    if os.path.isdir(checkpoint_path):\n",
        "        # Local directory\n",
        "        lora_weights_path = os.path.join(checkpoint_path, \"lora_weights.pt\")\n",
        "        if os.path.exists(lora_weights_path):\n",
        "            lora_state_dict = torch.load(lora_weights_path, map_location=device)\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"LoRA weights file not found in {checkpoint_path}\")\n",
        "    else:\n",
        "        # Assume it's a Hugging Face repo\n",
        "        try:\n",
        "            # Try to download the file from the Hugging Face repo\n",
        "            lora_weights_path = hf_hub_download(repo_id=checkpoint_path, filename=\"lora_weights.pt\")\n",
        "            lora_state_dict = torch.load(lora_weights_path, map_location=device)\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Error loading LoRA weights from Hugging Face repo: {str(e)}\")\n",
        "\n",
        "    return lora_state_dict\n",
        "\n",
        "def load_model_with_lora_and_projector(checkpoint_path, device, bnb_config=None, debug=False, for_training=False):\n",
        "    is_cpu = device == \"cpu\"\n",
        "\n",
        "    common_kwargs = {\n",
        "        \"trust_remote_code\": True,\n",
        "        \"debug\": debug\n",
        "    }\n",
        "\n",
        "    if is_cpu:\n",
        "        device_map = None\n",
        "        model = Phi3WithProjector.from_pretrained(\n",
        "            checkpoint_path,\n",
        "            torch_dtype=torch.float32,\n",
        "            low_cpu_mem_usage=True,\n",
        "            device_map=device_map,\n",
        "            **common_kwargs\n",
        "        )\n",
        "    else:\n",
        "        device_map = {\"\": 0}  # This maps all modules to the specified device\n",
        "        model = Phi3WithProjector.from_pretrained(\n",
        "            checkpoint_path,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=device_map,  # Use the corrected device_map\n",
        "            torch_dtype=torch.float16,\n",
        "            attn_implementation='eager',\n",
        "            **common_kwargs\n",
        "        )\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Load LoRA weights\n",
        "    lora_state_dict = load_lora_weights(checkpoint_path, device)\n",
        "    print(f\"Total keys in lora_state_dict: {len(lora_state_dict)}\")\n",
        "\n",
        "    new_state_dict = {}\n",
        "    scaling_factors_loaded = 0\n",
        "\n",
        "    for name, module in model.phi3.named_modules():\n",
        "        if isinstance(module, lora.Linear4bit):\n",
        "            lora_A_key = f\"{name}.lora_A.weight\"\n",
        "            lora_B_key = f\"{name}.lora_B.weight\"\n",
        "            scaling_key = f\"{name}.scaling\"\n",
        "\n",
        "            if lora_A_key in lora_state_dict and lora_B_key in lora_state_dict:\n",
        "                new_state_dict[f\"{name}.lora_A.default.weight\"] = lora_state_dict[lora_A_key]\n",
        "                new_state_dict[f\"{name}.lora_B.default.weight\"] = lora_state_dict[lora_B_key]\n",
        "\n",
        "                if scaling_key in lora_state_dict:\n",
        "                    module.scaling = lora_state_dict[scaling_key]\n",
        "                    scaling_factors_loaded += 1\n",
        "            else:\n",
        "                print(f\"Warning: LoRA weights for {name} not found in checkpoint\")\n",
        "\n",
        "    # Load the filtered state dict\n",
        "    model.phi3.load_state_dict(new_state_dict, strict=False)\n",
        "\n",
        "    print(f\"Loaded LoRA weights: {len(new_state_dict)} / {len(lora_state_dict)}\")\n",
        "    print(f\"Loaded scaling factors: {scaling_factors_loaded}\")\n",
        "    print(f\"Total LoRA modules processed: {len(new_state_dict) // 2}\")\n",
        "\n",
        "    if not for_training:\n",
        "        # Prepare the model for inference only if not for training\n",
        "        for name, module in model.named_modules():\n",
        "            if isinstance(module, LinearFP4): # or isinstance(module, LinearFP8):\n",
        "                module.prepare_for_inference()\n",
        "    else:\n",
        "        # Ensure the model is in training mode\n",
        "        model.train()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "gHa5rGNQoLq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp -r /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoint-350 /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/"
      ],
      "metadata": {
        "id": "fHqokMu1NkeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get the latest checkpoint\n",
        "latest_checkpoint = get_latest_checkpoint(gdrive_checkpoint_dir)\n",
        "eval_first = False\n",
        "if latest_checkpoint:\n",
        "    print(f\"Loading model from checkpoint: {latest_checkpoint}\")\n",
        "    model = load_model_with_lora_and_projector(latest_checkpoint, device,\n",
        "                                               bnb_config=bnb_config, for_training = True)\n",
        "    eval_first = True\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting training from scratch.\")\n",
        "    phi3_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        trust_remote_code=True,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=device_map,\n",
        "        torch_dtype=compute_dtype,\n",
        "        attn_implementation='eager'\n",
        "    )\n",
        "    image_embedding_dim = len(hf_dataset[0]['image_embedding'])\n",
        "    projection_dim = phi3_model.config.hidden_size\n",
        "    projector = ImageProjector(image_embedding_dim, projection_dim).to(device)\n",
        "    model = Phi3WithProjector(phi3_model, projector)\n",
        "\n",
        "    # Prepare the model for k-bit training\n",
        "    model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "UXVkny4DWOdU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547,
          "referenced_widgets": [
            "1054a36aa8674e12b5230cf635ce7aa5",
            "c3bc78efda3c4899a8635a1c2b56f4a9",
            "6fb67f44b3b74a9eb5836aa93b412959",
            "09e67a78ac1a4714a92599fb89881734",
            "b2387c882ce149b9b8641094399eaf2a",
            "742252afccd2431a8344d753dea32a2f",
            "4cc8a66fddc142178289d913b5784ed3",
            "8ac4934f60694d07acf59ac2036cb178",
            "aaf6cb549cd148f48d43a2da953fab6d",
            "4890c624469b471f8e915bec84898e98",
            "24ec080fbeee42b88c14db7159b978bc"
          ]
        },
        "outputId": "805bb448-3ba0-4a2f-cfce-76f283674fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from checkpoint: /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3.5-mini-instruct.af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3.5-mini-instruct.af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1054a36aa8674e12b5230cf635ce7aa5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading adapter weights from /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-350 led to unexpected keys not found in the model:  ['phi3.model.layers.0.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.0.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.1.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.1.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.10.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.10.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.11.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.11.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.12.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.12.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.13.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.13.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.14.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.14.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.15.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.15.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.16.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.16.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.17.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.17.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.18.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.18.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.19.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.19.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.2.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.2.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.20.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.20.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.21.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.21.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.22.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.22.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.23.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.23.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.24.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.24.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.25.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.25.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.26.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.26.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.27.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.27.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.28.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.28.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.29.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.29.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.3.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.3.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.30.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.30.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.31.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.31.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.4.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.4.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.5.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.5.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.6.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.6.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.7.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.7.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.8.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.8.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.9.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.9.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.9.self_attn.o_proj.lora_B.default.weight']. \n",
            "<ipython-input-19-c85ec63e6fa4>:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  projector_state_dict = torch.load(projector_path, map_location=phi3_model.device)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "local variable 'target_dtype' referenced before assignment",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-d8fa6bd3406b>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading model from checkpoint: {latest_checkpoint}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     model = load_model_with_lora_and_projector(latest_checkpoint, device,\n\u001b[0m\u001b[1;32m     15\u001b[0m                                                bnb_config=bnb_config, for_training = True)\n\u001b[1;32m     16\u001b[0m     \u001b[0meval_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-153e88ecfc4b>\u001b[0m in \u001b[0;36mload_model_with_lora_and_projector\u001b[0;34m(checkpoint_path, device, bnb_config, debug, for_training)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mdevice_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# This maps all modules to the specified device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         model = Phi3WithProjector.from_pretrained(\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnb_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-c85ec63e6fa4>\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, debug, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m#     param.data = param.data.to(target_dtype)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded projector with input_dim={input_dim}, output_dim={output_dim}, dtype={target_dtype}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Projector weights not found. Initializing with default dimensions.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'target_dtype' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(compute_dtype) , print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGcgsGvR6uw0",
        "outputId": "a53c6eb5-11b9-45ec-89e6-dc39360aed20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float16\n",
            "Phi3WithProjector(\n",
            "  (phi3): Phi3ForCausalLM(\n",
            "    (model): Phi3Model(\n",
            "      (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
            "      (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0-31): 32 x Phi3DecoderLayer(\n",
            "          (self_attn): Phi3Attention(\n",
            "            (o_proj): lora.Linear4bit(\n",
            "              (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=3072, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "              (lora_magnitude_vector): ModuleDict()\n",
            "            )\n",
            "            (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
            "            (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
            "          )\n",
            "          (mlp): Phi3MLP(\n",
            "            (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
            "            (down_proj): lora.Linear4bit(\n",
            "              (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=8192, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=3072, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "              (lora_magnitude_vector): ModuleDict()\n",
            "            )\n",
            "            (activation_fn): SiLU()\n",
            "          )\n",
            "          (input_layernorm): Phi3RMSNorm()\n",
            "          (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (post_attention_layernorm): Phi3RMSNorm()\n",
            "        )\n",
            "      )\n",
            "      (norm): Phi3RMSNorm()\n",
            "    )\n",
            "    (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
            "  )\n",
            "  (projector): ImageProjector(\n",
            "    (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (activation): GELU(approximate='none')\n",
            "    (layer2): Linear(in_features=1024, out_features=3072, bias=True)\n",
            "    (dropout): Dropout(p=0.05, inplace=False)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "ADlAMa4t6t7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=lora_r,\n",
        "    lora_alpha=lora_alpha,\n",
        "    target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
        "    lora_dropout=lora_dropout,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8wZ9oGItcDE",
        "outputId": "5660aa40-5a8c-4b4b-cd2b-264477730ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 8912896 || all params: 2021727232 || trainable%: 0.44085551497384196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_projector_weight_changes(loaded_state_dict, initial_state_dict):\n",
        "    for key in loaded_state_dict:\n",
        "        if torch.norm(loaded_state_dict[key] - initial_state_dict[key]).item() > 0:\n",
        "            print(f\"Weights changed for {key}\")\n",
        "            return True\n",
        "    print(\"WARNING: No weight changes detected in the projector!\")\n",
        "    return False\n",
        "\n",
        "# Use this when loading the model\n",
        "initial_projector_state = copy.deepcopy(model.base_model.model.projector.state_dict())"
      ],
      "metadata": {
        "id": "Ob2geqvyIeU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "RZLnz1M4NKqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "AVcaw9k7NDdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size  = per_device_eval_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"all\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=eval_steps, # Evaluate every 25 steps\n",
        "    # Add these new arguments\n",
        "    do_eval=True,\n",
        "    eval_delay=0,  # Start evaluation immediately\n",
        "    # Enable gradient checkpointing\n",
        "    gradient_checkpointing=gradient_checkpointing,\n",
        "    # Disable data parallelism if not needed\n",
        "    ddp_find_unused_parameters=False,\n",
        "    save_total_limit=1,  # Keep only the latest checkpoint\n",
        "    hub_model_id = hf_adapter_repo,\n",
        "    push_to_hub=False,  # We'll handle this in our custom callback\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "zGBG0EJINFCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom data collator to handle pre-tokenized inputs\n",
        "def custom_data_collator(features):\n",
        "    batch = {k: [d[k] for d in features] for k in features[0].keys()}\n",
        "\n",
        "    # Stack image embeddings\n",
        "    batch['image_embeddings'] = torch.stack(batch['image_embeddings'])#.to(torch.float16)\n",
        "\n",
        "    # Pad the sequences\n",
        "    batch['input_ids'] = torch.nn.utils.rnn.pad_sequence(batch['input_ids'], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    batch['attention_mask'] = torch.nn.utils.rnn.pad_sequence(batch['attention_mask'], batch_first=True, padding_value=0)\n",
        "    batch['labels'] = torch.nn.utils.rnn.pad_sequence(batch['labels'], batch_first=True, padding_value=-100)\n",
        "\n",
        "    return batch\n",
        "\n",
        "# Function to select a random subset of the dataset\n",
        "def select_subset(dataset, fraction=0.05):\n",
        "    num_samples = int(len(dataset) * fraction)\n",
        "    indices = random.sample(range(len(dataset)), num_samples)\n",
        "    return dataset.select(indices)\n",
        "\n",
        "# Select 5% of the training and test datasets\n",
        "small_train_dataset = select_subset(dataset_dict['train'], fraction=0.2)\n",
        "small_test_dataset = select_subset(dataset_dict['test'], fraction=0.05)\n",
        "\n",
        "# Create a new DatasetDict with the smaller datasets\n",
        "small_dataset_dict = DatasetDict({\n",
        "    'train': small_train_dataset,\n",
        "    'test': small_test_dataset\n",
        "})\n",
        "\n",
        "print(f\"Small train dataset size: {len(small_dataset_dict['train'])}\")\n",
        "print(f\"Small test dataset size: {len(small_dataset_dict['test'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDU9tnyZW_CW",
        "outputId": "bc204166-5098-4328-b3bb-c6dfb9ec1e85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small train dataset size: 8626\n",
            "Small test dataset size: 113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before initializing the SFTTrainer\n",
        "model.projector.train()  # Set projector to training mode\n",
        "for param in model.projector.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "def hook_fn(module, grad_input, grad_output):\n",
        "    print(f\"Gradient for {module.__class__.__name__}:\")\n",
        "    for idx, grad in enumerate(grad_input):\n",
        "        if grad is not None:\n",
        "            print(f\"  Input gradient {idx}: {grad.norm().item()}\")\n",
        "\n",
        "# # Apply the hook to projector layers\n",
        "# model.projector.layer1.register_backward_hook(hook_fn)\n",
        "# model.projector.layer2.register_backward_hook(hook_fn)"
      ],
      "metadata": {
        "id": "tG1jGxI4B48E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_dict['train'],\n",
        "    # eval_dataset=dataset_dict['test'],\n",
        "    # train_dataset=small_dataset_dict['train'],\n",
        "    eval_dataset=small_dataset_dict['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=custom_data_collator,\n",
        "    peft_config=lora_config,\n",
        "    max_seq_length=max_seq_length,\n",
        "    packing=packing,\n",
        "    # callbacks=[SaveLatestCheckpointCallback(), SaveQLoraAdapterCallback(model)],  # Add the custom callback\n",
        "    callbacks=[SaveLatestCheckpointAndLoraCallback()],\n",
        ")\n",
        "# # Cast the model's projector to float32 before evaluation\n",
        "# model.projector = model.projector.to(torch.float32)\n",
        "# Perform initial evaluation\n",
        "if eval_first:\n",
        "    print(\"Performing initial evaluation...\")\n",
        "    # eval_results = trainer.evaluate()\n",
        "    # print(f\"Initial evaluation results: {eval_results}\")\n",
        "    # Start or resume training\n",
        "    trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
        "else:\n",
        "    # Start training\n",
        "    trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "yGi1pNF98jy5",
        "outputId": "72230159-18b6-4cdf-dfa0-998d74b8bad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing initial evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
            "\teval_steps: 5 (from args) != 50 (from trainer_state.json)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='364' max='1348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 364/1348 08:44 < 11:56:46, 0.02 it/s, Epoch 0.27/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-33a70014cee4>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# print(f\"Initial evaluation results: {eval_results}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Start or resume training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trl_activate_neftune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;31m# After training we make sure to retrieve back the original forward pass method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2051\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2052\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2053\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2054\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2388\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3484\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3485\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3487\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3531\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3532\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3533\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3534\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1642\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1644\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1645\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-abf6cc93f6e9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, labels, image_embeddings, past_key_values, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"forward final: inputs_embeds shape: {kwargs.get('inputs_embeds', {}).shape if kwargs.get('inputs_embeds') is not None else None}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphi3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3.5-mini-instruct/af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0/modeling_phi3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1244\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3.5-mini-instruct/af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0/modeling_phi3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m                 layer_outputs = self._gradient_checkpointing_func(\n\u001b[0m\u001b[1;32m   1112\u001b[0m                     \u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0;34m\"use_reentrant=False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             )\n\u001b[0;32m--> 489\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         gen = _checkpoint_without_reentrant_generator(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3.5-mini-instruct/af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0/modeling_phi3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m         attn_outputs, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    843\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3.5-mini-instruct/af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0/modeling_phi3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 )\n\u001b[1;32m    333\u001b[0m             \u001b[0mkv_seq_len\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_usable_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotary_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkv_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_rotary_pos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3.5-mini-instruct/af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0/modeling_phi3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, position_ids, seq_len)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mext_factors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mext_factors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0minv_freq_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the fine-tuned model\n",
        "# trainer.save_model()\n",
        "# # trainer.model.save_pretrained(new_model)\n",
        "# final_model_path = os.path.join(gdrive_checkpoint_dir, \"final_model\")\n",
        "# trainer.model.save_pretrained(final_model_path)\n",
        "# tokenizer.save_pretrained(final_model_path)"
      ],
      "metadata": {
        "id": "6eGu422_8mLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.base_model.model.projector.state_dict(), final_model_path + '/image_projector.pth')\n",
        "# print(f\"Projector saved to: {final_model_path}/image_projector.pth\")"
      ],
      "metadata": {
        "id": "NgYtfm2bUP-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the projector\n",
        "# projector_path = '/content/drive/MyDrive/multimodal_llm/phi-3_5/image_projector.pth'\n",
        "# os.makedirs(os.path.dirname(projector_path), exist_ok=True)\n",
        "# torch.save(model.projector.state_dict(), projector_path)\n",
        "# print(f\"Projector saved to: {projector_path}\")"
      ],
      "metadata": {
        "id": "QoBIZYRVQjaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the latest checkpoint\n",
        "latest_checkpoint = get_latest_checkpoint(gdrive_checkpoint_dir)\n",
        "loaded_projector_state = torch.load(latest_checkpoint + '/image_projector.pth')\n",
        "verify_projector_weight_changes(loaded_projector_state, initial_projector_state)"
      ],
      "metadata": {
        "id": "WV38oDxeIkQR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "9c493051-5fa7-45b7-8ab0-5801443145b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-aa93cb025ceb>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  loaded_projector_state = torch.load(latest_checkpoint + '/image_projector.pth')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'verify_projector_weight_changes' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-aa93cb025ceb>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlatest_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_latest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgdrive_checkpoint_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloaded_projector_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/image_projector.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mverify_projector_weight_changes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_projector_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_projector_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'verify_projector_weight_changes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.push_to_hub()\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=latest_checkpoint,\n",
        "    repo_id=hf_adapter_repo,\n",
        "    repo_type=\"model\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "wTx7oRvuYZ7m",
        "outputId": "74892c82-5155-4763-ccd7-60ee9457d965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/sayanbanerjee32/multimodal-phi3_5-mini-instruct-llava_adapter/commit/4b0b5dec7ec64d6a7bc4dc98b6dd559ea415e094', commit_message='Upload folder using huggingface_hub', commit_description='', oid='4b0b5dec7ec64d6a7bc4dc98b6dd559ea415e094', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp -r multimodal-phi3_5-mini-instruct-llava_adapter /content/drive/MyDrive/multimodal_llm/phi-3_5"
      ],
      "metadata": {
        "id": "UwRtBYUXWnsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this function after your CustomTextGenerator class\n",
        "def evaluate_model(model, tokenizer, eval_dataset, device):\n",
        "    model = model.to(device).to(torch.float16)\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    dataloader = DataLoader(eval_dataset, batch_size=2, collate_fn=custom_data_collator)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch = {\n",
        "                'input_ids': batch['input_ids'].to(device).long(),\n",
        "                'attention_mask': batch['attention_mask'].to(device).long(),\n",
        "                'image_embeddings': batch['image_embeddings'].to(device).to(torch.float16),\n",
        "                'labels': batch['labels'].to(device).long()\n",
        "            }\n",
        "\n",
        "            # print(f\"input_ids shape: {batch['input_ids'].shape}, dtype: {batch['input_ids'].dtype}\")\n",
        "            # print(f\"attention_mask shape: {batch['attention_mask'].shape}, dtype: {batch['attention_mask'].dtype}\")\n",
        "            # print(f\"image_embeddings shape: {batch['image_embeddings'].shape}, dtype: {batch['image_embeddings'].dtype}\")\n",
        "            # print(f\"labels shape: {batch['labels'].shape}, dtype: {batch['labels'].dtype}\")\n",
        "\n",
        "            try:\n",
        "                outputs = model(\n",
        "                    input_ids=batch['input_ids'],\n",
        "                    attention_mask=batch['attention_mask'],\n",
        "                    image_embeddings=batch['image_embeddings'],\n",
        "                    labels=batch['labels']\n",
        "                )\n",
        "                total_loss += outputs.loss.item()\n",
        "                num_batches += 1\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Error processing batch: {e}\")\n",
        "                print(f\"Model device: {next(model.parameters()).device}\")\n",
        "                for name, param in model.named_parameters():\n",
        "                    print(f\"{name} - shape: {param.shape}, dtype: {param.dtype}, device: {param.device}\")\n",
        "                raise  # Re-raise the exception to see the full traceback\n",
        "\n",
        "    return total_loss / num_batches if num_batches > 0 else 0\n",
        "\n",
        "# Add these lines before merging the model\n",
        "print(\"Evaluating model before merging...\")\n",
        "pre_merge_loss = evaluate_model(model, tokenizer, small_dataset_dict['test'], device)\n",
        "print(f\"Pre-merge loss: {pre_merge_loss}\")"
      ],
      "metadata": {
        "id": "qiaCJXZuBva5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ada3687-e977-421b-a060-43ddf7d5ad76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model before merging...\n",
            "Pre-merge loss: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sample inference code"
      ],
      "metadata": {
        "id": "i3TooP90Xd94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "w0bqPi-V5q_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom text generation class\n",
        "class CustomTextGenerator:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def generate(self, input_text, image_embedding, **generate_kwargs):\n",
        "        # Tokenize the input text\n",
        "        inputs = self.tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        input_ids = inputs[\"input_ids\"].to(self.model.device)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(self.model.device)\n",
        "\n",
        "        # Ensure image_embedding is a tensor and move it to the correct device\n",
        "        if not isinstance(image_embedding, torch.Tensor):\n",
        "            image_embedding = torch.tensor(image_embedding)\n",
        "        # image_embedding = image_embedding.to(self.model.device).unsqueeze(0)  # Add batch dimension\n",
        "        image_embedding = image_embedding.to(self.model.device)#.to(next(self.model.parameters()).dtype)\n",
        "\n",
        "        # Generate text\n",
        "        outputs = self.model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            # image_embeddings=image_embedding,\n",
        "            image_embeddings=image_embedding.unsqueeze(0),  # Add batch dimension\n",
        "            **generate_kwargs\n",
        "        )\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return generated_text"
      ],
      "metadata": {
        "id": "Hsp71KFMB0-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a sample from the validation set\n",
        "sample = dataset_dict['test'][1]\n",
        "image_embedding = sample['image_embeddings']\n",
        "\n",
        "\n",
        "def get_first_user_input(decoded_text):\n",
        "    # Find the position of the first <|assistant|> tag\n",
        "    assistant_pos = decoded_text.find('<|assistant|>')\n",
        "\n",
        "    # If <|assistant|> is found, truncate the text\n",
        "    if assistant_pos != -1:\n",
        "        return decoded_text[:assistant_pos].strip()\n",
        "    else:\n",
        "        return decoded_text.strip()\n",
        "\n",
        "# Decode the input_ids\n",
        "full_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=False)\n",
        "\n",
        "# Extract only the first user input\n",
        "input_text = get_first_user_input(full_text) + '<|assistant|>'\n",
        "print(input_text)"
      ],
      "metadata": {
        "id": "RHuBr77NB4Wi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae8462c-27a2-4b48-ab9a-5f99eb35ddf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><|system|> You are a helpful assistant.<|end|><|user|> Given the following information, provide a detailed and accurate response:\n",
            "What food and drink are present in the image?\n",
            "[An image is provided for this task.]\n",
            "<|end|><|assistant|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "# Initialize the custom text generator\n",
        "generator = CustomTextGenerator(model=model, tokenizer=tokenizer)\n",
        "generated_text = generator.generate(\n",
        "    input_text,\n",
        "    image_embedding=image_embedding,\n",
        "    # max_length=200,\n",
        "    # num_return_sequences=1,\n",
        "    # do_sample=True,\n",
        "    # temperature=0.7,\n",
        "    # top_k=50,\n",
        "    # top_p=0.95,\n",
        "    max_new_tokens=150,\n",
        "    num_return_sequences=1,\n",
        "    do_sample=True,\n",
        "    temperature=0.8,\n",
        "    top_k=40,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2,\n",
        "    no_repeat_ngram_size=3,\n",
        ")\n",
        "\n",
        "print(\"Input text:\")\n",
        "print(input_text)\n",
        "print(\"\\nGenerated text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "2_ytnYhQCItH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "eec6c521-57ff-46b5-e542-586aea8603f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-d0bdf3686c1c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Initialize the custom text generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomTextGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m generated_text = generator.generate(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimage_embedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_embedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-473e2092e055>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_text, image_embedding, **generate_kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Generate text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         outputs = self.model.generate(\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1702\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-abf6cc93f6e9>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, attention_mask, image_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"generate after: {attention_mask.size()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2047\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2048\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3041\u001b[0m                 \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m                 \u001b[0;31m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3043\u001b[0;31m                 \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3044\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3045\u001b[0m                 \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_inference(model, tokenizer, image_embedding):\n",
        "    # Log projector weights before inference\n",
        "    projector = model.projector\n",
        "    weight_change = projector.get_weight_change()\n",
        "    print(f\"Projector weight change: {weight_change}\")\n",
        "    # Ensure the model is in the correct precision (e.g., float16)\n",
        "    # model = model.to(next(model.parameters()).dtype)\n",
        "\n",
        "    generator = CustomTextGenerator(model=model, tokenizer=tokenizer)\n",
        "    input_text = \"\"\"<|system|> You are a helpful assistant.<|end|><|user|> Given the following information, provide a detailed and accurate response:\n",
        "Describe the image in detail.\n",
        "<|end|><|assistant|>\n",
        "\"\"\"\n",
        "# [An image is provided for this task.]\n",
        "    generated_text = generator.generate(\n",
        "        input_text,\n",
        "        image_embedding=image_embedding,\n",
        "        max_new_tokens=150,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        temperature=0.8,\n",
        "        top_k=40,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.2,\n",
        "        no_repeat_ngram_size=3,\n",
        "    )\n",
        "    return generated_text\n",
        "\n"
      ],
      "metadata": {
        "id": "JOxaZinoCGCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before merging\n",
        "print(\"Testing inference before merging...\")\n",
        "pre_merge_output = test_inference(model, tokenizer, sample['image_embeddings'])\n",
        "print(pre_merge_output)\n"
      ],
      "metadata": {
        "id": "D0cfVKzgQIGu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "1e9c4d16-977c-43a9-980c-1b9495c7d7ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing inference before merging...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-48a91f08cdab>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Before merging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing inference before merging...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpre_merge_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_embeddings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_merge_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-5daba9bdea05>\u001b[0m in \u001b[0;36mtest_inference\u001b[0;34m(model, tokenizer, image_embedding)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Log projector weights before inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprojector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mweight_change\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprojector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weight_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Projector weight change: {weight_change}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Ensure the model is in the correct precision (e.g., float16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-abf6cc93f6e9>\u001b[0m in \u001b[0;36mget_weight_change\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0minitial_weights2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_weights2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mweight_diff1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_weights1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weights1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mweight_diff2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_weights2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weights2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mweight_diff1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mweight_diff2\u001b[0m  \u001b[0;31m# Total weight change across both layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### merge models and save in gdrive"
      ],
      "metadata": {
        "id": "fKrJVITzcjPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del trainer\n",
        "del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "z9NBBb8IdcfH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "03706583-4667-4dd8-9a05-69c0850932d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-cb6bb38b3e0f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model_with_lora_and_projector(hf_adapter_repo, #latest_checkpoint,\n",
        "                                                  device, bnb_config = bnb_config, debug = False)\n",
        "# print(loaded_model.projector.layer1.weight.dtype, loaded_model.projector.layer1.bias.dtype)"
      ],
      "metadata": {
        "id": "yxhIm-Neoe1r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "3c15fd9cc0ce4401ba55cbbdfacf3b52",
            "9df15b8f77cf4530beedbb807dfe510f",
            "ab1d7576171c4ecc8f0e25504ec0f54f",
            "8e22ee7a84b44a398d41d5579d1ef284",
            "9d69c9af8f2a4f8b8e8600fb886077e0",
            "22e645ca1d7341df9261dbc2c8cc58fc",
            "c8d3d080fc944ba5af8c45516d2fd333",
            "452c20a1638b4596b090ce1c7b7553a0",
            "b124ea9d7a6e4251a1780b920289f300",
            "d2daf1ccd14a4cd2a80786e90b8abbf3",
            "123472d69abc49d6b3fea2a5dd30f7bc"
          ]
        },
        "outputId": "1bd2a01e-1194-42fd-df10-1bc845b32811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c15fd9cc0ce4401ba55cbbdfacf3b52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading adapter weights from sayanbanerjee32/multimodal-phi3_5-mini-instruct-llava_adapter led to unexpected keys not found in the model:  ['phi3.model.layers.0.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.0.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.1.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.1.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.10.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.10.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.11.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.11.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.12.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.12.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.13.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.13.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.14.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.14.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.15.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.15.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.16.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.16.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.17.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.17.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.18.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.18.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.19.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.19.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.2.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.2.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.20.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.20.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.21.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.21.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.22.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.22.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.23.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.23.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.24.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.24.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.25.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.25.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.26.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.26.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.27.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.27.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.28.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.28.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.29.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.29.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.3.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.3.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.30.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.30.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.31.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.31.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.4.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.4.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.5.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.5.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.6.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.6.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.7.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.7.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.8.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.8.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.9.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.9.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.9.self_attn.o_proj.lora_B.default.weight']. \n",
            "<ipython-input-31-e3339eabcc31>:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  projector_state_dict = torch.load(projector_path, map_location=phi3_model.device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded projector with input_dim=512, output_dim=3072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-153e88ecfc4b>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lora_state_dict = torch.load(lora_weights_path, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total keys in lora_state_dict: 192\n",
            "Loaded LoRA weights: 128 / 192\n",
            "Loaded scaling factors: 64\n",
            "Total LoRA modules processed: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_loaded_model(model, tokenizer, eval_dataset, device):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # # Print model parameter dtypes\n",
        "    # for name, param in model.named_parameters():\n",
        "    #     if 'projector' in name:\n",
        "    #         print(f\"{name} dtype: {param.dtype}\")\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    dataloader = DataLoader(eval_dataset, batch_size=2, collate_fn=custom_data_collator)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch = {\n",
        "                'input_ids': batch['input_ids'].to(device),\n",
        "                'attention_mask': batch['attention_mask'].to(device),\n",
        "                'image_embeddings': batch['image_embeddings'].to(device),#.to(torch.float16),\n",
        "                'labels': batch['labels'].to(device)\n",
        "            }\n",
        "\n",
        "            # Print the dtype of image_embeddings\n",
        "            # print(f\"image_embeddings dtype: {batch['image_embeddings'].dtype}\")\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask'],\n",
        "                image_embeddings=batch['image_embeddings'],\n",
        "                labels=batch['labels']\n",
        "            )\n",
        "            total_loss += outputs.loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "    return total_loss / num_batches if num_batches > 0 else 0\n",
        "\n",
        "print(\"Evaluating loaded model...\")\n",
        "loded_model_loss = evaluate_loaded_model(loaded_model, tokenizer, small_dataset_dict['test'], device)\n",
        "print(f\"loded model loss: {loded_model_loss}\")"
      ],
      "metadata": {
        "id": "AwvYXIwUwFp6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70294a16-9c2f-4e4e-e24b-5caab00d4550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating loaded model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loded model loss: 0.18099397244422058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# after loading model from PEFT adapter\n",
        "print(\"Testing inference after loading model from peft adapter...\")\n",
        "peft_adapter_output = test_inference(loaded_model, tokenizer, sample['image_embeddings'])\n",
        "print(peft_adapter_output)"
      ],
      "metadata": {
        "id": "2L4BW4CV6xYo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30a5464-782e-4452-da39-d22447eb4da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing inference after loading model from peft adapter...\n",
            "Projector weight change: 2.8105183839797974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a helpful assistant. Given the following information, provide a detailed and accurate response:\n",
            "Describe the image in detail.\n",
            " <|Animal-Based Dining Experience with an Eye for Detail!<\\|end\\|> \n",
            "In this snapshot of dine aloud's menu page on TripAdvisor® travel reviews website under \"What to do & see\", there is much more than just food options available; it captures your attention through multiple visual elements that create intrigue about their establishment which offers animal meals as well other specialty items like pizza or beef steak burgers among others - making sure visitors feel invited right away even before entering into reading further details such...   \n",
            "How does one get these delicious offerings?   What kind would you choose from them if given free reign over ordering at any time\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('end')"
      ],
      "metadata": {
        "id": "CVdaMp3fMQJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ch42Lsk__jwJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}