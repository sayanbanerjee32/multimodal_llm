{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1sIyQFl-RN3d3jPVutndUA2EhUaHgDza6",
      "authorship_tag": "ABX9TyPUiggIYFgPvIeBTXM3UX7s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ac02431301874065bf19f5ad2659708a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6302eeccd2c4cfbbd238c47ec436e2a",
              "IPY_MODEL_4d03789cfe85436fb6377e8479c4af31",
              "IPY_MODEL_93159488191748198ba37427596b7e4f"
            ],
            "layout": "IPY_MODEL_b6fb9f60fd664bcfb305ace587d0ba90"
          }
        },
        "c6302eeccd2c4cfbbd238c47ec436e2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de1c5bf0cd994cc388c4d1dc47525b98",
            "placeholder": "​",
            "style": "IPY_MODEL_e1fab703985a4fc0a90bbc80405b4e49",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "4d03789cfe85436fb6377e8479c4af31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_006807e2a6774f5da2d697e79bcbad9c",
            "max": 3984,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ed854d586fe4404b9dfaf242dd5895d",
            "value": 3984
          }
        },
        "93159488191748198ba37427596b7e4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4963c7324624c8499bf408cbf3d2c2f",
            "placeholder": "​",
            "style": "IPY_MODEL_db195e950fa34f08a1c52e8879fc106a",
            "value": " 3.98k/3.98k [00:00&lt;00:00, 251kB/s]"
          }
        },
        "b6fb9f60fd664bcfb305ace587d0ba90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de1c5bf0cd994cc388c4d1dc47525b98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1fab703985a4fc0a90bbc80405b4e49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "006807e2a6774f5da2d697e79bcbad9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ed854d586fe4404b9dfaf242dd5895d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4963c7324624c8499bf408cbf3d2c2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db195e950fa34f08a1c52e8879fc106a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed53737ac9e440bb8201f8c0ea758f02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03f9958175764136b70718afd8be5c99",
              "IPY_MODEL_c17c2efafe334ec3b8ffa880aa6e1d0c",
              "IPY_MODEL_ddc10ed9ed9c476d9589c90683e21eec"
            ],
            "layout": "IPY_MODEL_1f84a6ce0d344f55a7c0d7ab2111b181"
          }
        },
        "03f9958175764136b70718afd8be5c99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_885a532debd14604a0a1e4a4beeceb81",
            "placeholder": "​",
            "style": "IPY_MODEL_6369704d735a494d82bd800c65a48689",
            "value": "tokenizer.model: 100%"
          }
        },
        "c17c2efafe334ec3b8ffa880aa6e1d0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b475afedb9449c28655d94302e259d7",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_02bb5f5ac33e4a19a12ff7fee277b46a",
            "value": 499723
          }
        },
        "ddc10ed9ed9c476d9589c90683e21eec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dc69f38bc634627bc1f879507a4cb21",
            "placeholder": "​",
            "style": "IPY_MODEL_99d7ad675e2347f1beb2fd1713d0e7ea",
            "value": " 500k/500k [00:00&lt;00:00, 16.3MB/s]"
          }
        },
        "1f84a6ce0d344f55a7c0d7ab2111b181": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "885a532debd14604a0a1e4a4beeceb81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6369704d735a494d82bd800c65a48689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b475afedb9449c28655d94302e259d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02bb5f5ac33e4a19a12ff7fee277b46a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0dc69f38bc634627bc1f879507a4cb21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99d7ad675e2347f1beb2fd1713d0e7ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9b53127cd194bac875f7049824cb464": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_30c5f3227e5549b995724872817ab964",
              "IPY_MODEL_bca8c5b1efa94322a59407c84ee98e12",
              "IPY_MODEL_ded8a6b82c0c4db09c548258186be13c"
            ],
            "layout": "IPY_MODEL_7187526ef16d4a108a7f6d54aa1ad42e"
          }
        },
        "30c5f3227e5549b995724872817ab964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22cd0bb990104353866733cb62acb42c",
            "placeholder": "​",
            "style": "IPY_MODEL_462ee915aad74b6d8fcacb820e813346",
            "value": "tokenizer.json: 100%"
          }
        },
        "bca8c5b1efa94322a59407c84ee98e12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adfe37464e5842b7bbe8a14c346f42c2",
            "max": 1844408,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0991228785dd4169a464fe83e9d70a8b",
            "value": 1844408
          }
        },
        "ded8a6b82c0c4db09c548258186be13c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c6f579cf4a94e19a0d3f24702bd6f7c",
            "placeholder": "​",
            "style": "IPY_MODEL_7b03168b70e2435eabf42a74e7297a40",
            "value": " 1.84M/1.84M [00:01&lt;00:00, 1.08MB/s]"
          }
        },
        "7187526ef16d4a108a7f6d54aa1ad42e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22cd0bb990104353866733cb62acb42c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "462ee915aad74b6d8fcacb820e813346": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adfe37464e5842b7bbe8a14c346f42c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0991228785dd4169a464fe83e9d70a8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c6f579cf4a94e19a0d3f24702bd6f7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b03168b70e2435eabf42a74e7297a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de6d3d8782af4d10937d1ca133b38598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_571c3d580011456fb9ab9e34e907b00b",
              "IPY_MODEL_f34247b2523e49a7863282eecd0f174c",
              "IPY_MODEL_486b7bdeab9d4e178216ea37218f12f1"
            ],
            "layout": "IPY_MODEL_51b92b173dcc451c97383b3fc6a18104"
          }
        },
        "571c3d580011456fb9ab9e34e907b00b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64aa275bccb6432fa737f1caaf5bbac6",
            "placeholder": "​",
            "style": "IPY_MODEL_c21466dc185f434cae60266601fa3f1e",
            "value": "added_tokens.json: 100%"
          }
        },
        "f34247b2523e49a7863282eecd0f174c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4833ce8edc9a45048807ee1edb6c0f95",
            "max": 306,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22ad3cdf42534b089e4ee8d24f27a649",
            "value": 306
          }
        },
        "486b7bdeab9d4e178216ea37218f12f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcb21c70788d490189b27d6e7099c51d",
            "placeholder": "​",
            "style": "IPY_MODEL_4dcce80ed61145abb693d248bec46f9e",
            "value": " 306/306 [00:00&lt;00:00, 14.8kB/s]"
          }
        },
        "51b92b173dcc451c97383b3fc6a18104": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64aa275bccb6432fa737f1caaf5bbac6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c21466dc185f434cae60266601fa3f1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4833ce8edc9a45048807ee1edb6c0f95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22ad3cdf42534b089e4ee8d24f27a649": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bcb21c70788d490189b27d6e7099c51d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dcce80ed61145abb693d248bec46f9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5632f80dc764e58ae379c63f3600edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6769856111334aef9997ffa5cf5777ef",
              "IPY_MODEL_844181a2008c4797baa19441844987a3",
              "IPY_MODEL_fef9f3e974874beb888ee8567e48fc01"
            ],
            "layout": "IPY_MODEL_6b9290e78ba04564896f582d1a9839ee"
          }
        },
        "6769856111334aef9997ffa5cf5777ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d8a1ad98de1467fbb1774dabe12afa6",
            "placeholder": "​",
            "style": "IPY_MODEL_733b8148befe4eb8871cf6fdb3422038",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "844181a2008c4797baa19441844987a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6c88f000e184369ba675ff06ebea9d2",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_963ba2824ada41e9894e039b7327bf6b",
            "value": 665
          }
        },
        "fef9f3e974874beb888ee8567e48fc01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_442ec220c5194aa3960ed829509f5a73",
            "placeholder": "​",
            "style": "IPY_MODEL_65b27853e08f4bb8862c377191e9b068",
            "value": " 665/665 [00:00&lt;00:00, 45.3kB/s]"
          }
        },
        "6b9290e78ba04564896f582d1a9839ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d8a1ad98de1467fbb1774dabe12afa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "733b8148befe4eb8871cf6fdb3422038": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6c88f000e184369ba675ff06ebea9d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "963ba2824ada41e9894e039b7327bf6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "442ec220c5194aa3960ed829509f5a73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65b27853e08f4bb8862c377191e9b068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abce03b6a7ec4f8ebca12ecb3fcf37ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_65c701cd269046738129fff44ad296b7",
              "IPY_MODEL_a281941fe40046dd830b1132961f840e",
              "IPY_MODEL_609f7d3521c74f5c8042f7bfa0fdd9d0"
            ],
            "layout": "IPY_MODEL_e7397fa23fa0483ba6c1249db8e8dcb7"
          }
        },
        "65c701cd269046738129fff44ad296b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68efadcba17a4f49b0c956cf478a8967",
            "placeholder": "​",
            "style": "IPY_MODEL_425874ead1484cbb947be684fd1607f5",
            "value": "Map: 100%"
          }
        },
        "a281941fe40046dd830b1132961f840e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75b9249e3c574f188cd900d89ae5d3bd",
            "max": 157712,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b8f812c8b824b04a3603c59f8f17559",
            "value": 157712
          }
        },
        "609f7d3521c74f5c8042f7bfa0fdd9d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d2501078f99417abf477637329f66ea",
            "placeholder": "​",
            "style": "IPY_MODEL_8db675be9cc745c98b40dd0488945ed1",
            "value": " 157712/157712 [06:01&lt;00:00, 885.85 examples/s]"
          }
        },
        "e7397fa23fa0483ba6c1249db8e8dcb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68efadcba17a4f49b0c956cf478a8967": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "425874ead1484cbb947be684fd1607f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75b9249e3c574f188cd900d89ae5d3bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b8f812c8b824b04a3603c59f8f17559": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d2501078f99417abf477637329f66ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8db675be9cc745c98b40dd0488945ed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d684988b77f6441fab2683d275266ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c753a98543e34740b017cff896186242",
              "IPY_MODEL_7e9384d9472241708c8a9d48302fb708",
              "IPY_MODEL_e6583b52cfe64e8294077932c3734a51"
            ],
            "layout": "IPY_MODEL_6c24b51b583442ebafefe1d13fb2f156"
          }
        },
        "c753a98543e34740b017cff896186242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15839d3e862f4eec8f1181037b31d66c",
            "placeholder": "​",
            "style": "IPY_MODEL_802fbd5fdde14b0e9299d71f86385cfd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7e9384d9472241708c8a9d48302fb708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_502f4201c3474afeb522f73aed3a13fe",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23b7080eda9f4cc7bf9ed1719f9c39e7",
            "value": 2
          }
        },
        "e6583b52cfe64e8294077932c3734a51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_113b235b03a4424cb33eff9c1e493dc0",
            "placeholder": "​",
            "style": "IPY_MODEL_9727149286dc491baf1c69221aadf0c0",
            "value": " 2/2 [00:44&lt;00:00, 20.91s/it]"
          }
        },
        "6c24b51b583442ebafefe1d13fb2f156": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15839d3e862f4eec8f1181037b31d66c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "802fbd5fdde14b0e9299d71f86385cfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "502f4201c3474afeb522f73aed3a13fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23b7080eda9f4cc7bf9ed1719f9c39e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "113b235b03a4424cb33eff9c1e493dc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9727149286dc491baf1c69221aadf0c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c51f69187ac446a09e2f3c5783c7dc87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b644d5d5eccb4c50adb56347a0af0269",
              "IPY_MODEL_fbf28c6c750640c1a01d71a7fbf7e0aa",
              "IPY_MODEL_7e800fc855074874865166194be4a86e"
            ],
            "layout": "IPY_MODEL_df203e45a5b14f68bc5080e8eb33c03d"
          }
        },
        "b644d5d5eccb4c50adb56347a0af0269": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_509d0452f3ce4ca6809e11458c8ebf84",
            "placeholder": "​",
            "style": "IPY_MODEL_8ed1480296e9488a992dc5cb61046f23",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "fbf28c6c750640c1a01d71a7fbf7e0aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd9d913c44a64201a20f36dccd525caf",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c166fc16e9641aabf7f25b4e4ca9b73",
            "value": 2
          }
        },
        "7e800fc855074874865166194be4a86e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a68b79b535174dfb8da1db548211f4b7",
            "placeholder": "​",
            "style": "IPY_MODEL_ba2e697a1db34cee8965b63cfcd7e340",
            "value": " 2/2 [00:40&lt;00:00, 19.39s/it]"
          }
        },
        "df203e45a5b14f68bc5080e8eb33c03d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "509d0452f3ce4ca6809e11458c8ebf84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ed1480296e9488a992dc5cb61046f23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd9d913c44a64201a20f36dccd525caf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c166fc16e9641aabf7f25b4e4ca9b73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a68b79b535174dfb8da1db548211f4b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba2e697a1db34cee8965b63cfcd7e340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayanbanerjee32/multimodal_llm/blob/main/phi_3_QLoRA_instruct150k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q2agwLMDpJC",
        "outputId": "c78ef60b-6cb6-474a-e2c4-615ac2b34d47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.45.2\n",
        "!pip install trl==0.11.4\n",
        "!pip install -Uq accelerate peft bitsandbytes dataset\n",
        "# transformers trl\n",
        "# !pip install -Uq flash_attn"
      ],
      "metadata": {
        "id": "GOg3sZg0IuZf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af9d6881-cd2e-4c84-d064-5b4ea185c5df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.45.2\n",
            "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (0.4.5)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.2)\n",
            "  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (2024.8.30)\n",
            "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed tokenizers-0.20.1 transformers-4.45.2\n",
            "Collecting trl==0.11.4\n",
            "  Downloading trl-0.11.4-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl==0.11.4) (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.40.0 in /usr/local/lib/python3.10/dist-packages (from trl==0.11.4) (4.45.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl==0.11.4) (0.34.2)\n",
            "Collecting datasets (from trl==0.11.4)\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting tyro>=0.5.11 (from trl==0.11.4)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl==0.11.4) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.11.4) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.11.4) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.11.4) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.11.4) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.11.4) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.11.4) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.4.0->trl==0.11.4) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl==0.11.4) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl==0.11.4) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl==0.11.4) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl==0.11.4) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl==0.11.4) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl==0.11.4) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl==0.11.4) (0.20.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl==0.11.4) (4.66.6)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.11.4) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.11.4) (13.9.3)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11.4)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl==0.11.4) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.11.4) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->trl==0.11.4)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.11.4) (2.2.2)\n",
            "Collecting xxhash (from datasets->trl==0.11.4)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->trl==0.11.4)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch>=1.4.0->trl==0.11.4)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.11.4) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.11.4) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.11.4) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.11.4) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.11.4) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.11.4) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.11.4) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.11.4) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl==0.11.4) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl==0.11.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl==0.11.4) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl==0.11.4) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl==0.11.4) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.11.4) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.11.4) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.11.4) (2024.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.11.4) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->trl==0.11.4) (0.2.0)\n",
            "Downloading trl-0.11.4-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, shtab, fsspec, dill, multiprocess, tyro, datasets, trl\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 shtab-1.7.1 trl-0.11.4 tyro-0.8.14 xxhash-3.5.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import os, gc\n",
        "import subprocess\n",
        "import json\n",
        "import random\n",
        "### Download Phi-3 model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, PreTrainedModel\n",
        "from transformers.trainer_callback import TrainerCallback\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import Dataset, DatasetDict\n",
        "import joblib\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "from peft.tuners import lora\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "from bitsandbytes.nn import LinearFP4\n",
        "from huggingface_hub import hf_hub_download\n",
        "import torch.cuda.amp as amp\n",
        "import dataclasses"
      ],
      "metadata": {
        "id": "ju3h7A8AtfmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzTf0O7myIO-",
        "outputId": "9bd609d5-c8de-4cee-9f2a-d53934938642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Phi-3 model"
      ],
      "metadata": {
        "id": "YsXHaUKZM1ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Phi-3 model and tokenizer\n",
        "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "# \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"right\",\n",
        "#                                            trust_remote_code=True)\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, add_eos_token=True, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "tokenizer.padding_side = 'left'"
      ],
      "metadata": {
        "id": "RrirRzvsuYaw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "ac02431301874065bf19f5ad2659708a",
            "c6302eeccd2c4cfbbd238c47ec436e2a",
            "4d03789cfe85436fb6377e8479c4af31",
            "93159488191748198ba37427596b7e4f",
            "b6fb9f60fd664bcfb305ace587d0ba90",
            "de1c5bf0cd994cc388c4d1dc47525b98",
            "e1fab703985a4fc0a90bbc80405b4e49",
            "006807e2a6774f5da2d697e79bcbad9c",
            "8ed854d586fe4404b9dfaf242dd5895d",
            "e4963c7324624c8499bf408cbf3d2c2f",
            "db195e950fa34f08a1c52e8879fc106a",
            "ed53737ac9e440bb8201f8c0ea758f02",
            "03f9958175764136b70718afd8be5c99",
            "c17c2efafe334ec3b8ffa880aa6e1d0c",
            "ddc10ed9ed9c476d9589c90683e21eec",
            "1f84a6ce0d344f55a7c0d7ab2111b181",
            "885a532debd14604a0a1e4a4beeceb81",
            "6369704d735a494d82bd800c65a48689",
            "7b475afedb9449c28655d94302e259d7",
            "02bb5f5ac33e4a19a12ff7fee277b46a",
            "0dc69f38bc634627bc1f879507a4cb21",
            "99d7ad675e2347f1beb2fd1713d0e7ea",
            "e9b53127cd194bac875f7049824cb464",
            "30c5f3227e5549b995724872817ab964",
            "bca8c5b1efa94322a59407c84ee98e12",
            "ded8a6b82c0c4db09c548258186be13c",
            "7187526ef16d4a108a7f6d54aa1ad42e",
            "22cd0bb990104353866733cb62acb42c",
            "462ee915aad74b6d8fcacb820e813346",
            "adfe37464e5842b7bbe8a14c346f42c2",
            "0991228785dd4169a464fe83e9d70a8b",
            "2c6f579cf4a94e19a0d3f24702bd6f7c",
            "7b03168b70e2435eabf42a74e7297a40",
            "de6d3d8782af4d10937d1ca133b38598",
            "571c3d580011456fb9ab9e34e907b00b",
            "f34247b2523e49a7863282eecd0f174c",
            "486b7bdeab9d4e178216ea37218f12f1",
            "51b92b173dcc451c97383b3fc6a18104",
            "64aa275bccb6432fa737f1caaf5bbac6",
            "c21466dc185f434cae60266601fa3f1e",
            "4833ce8edc9a45048807ee1edb6c0f95",
            "22ad3cdf42534b089e4ee8d24f27a649",
            "bcb21c70788d490189b27d6e7099c51d",
            "4dcce80ed61145abb693d248bec46f9e",
            "a5632f80dc764e58ae379c63f3600edd",
            "6769856111334aef9997ffa5cf5777ef",
            "844181a2008c4797baa19441844987a3",
            "fef9f3e974874beb888ee8567e48fc01",
            "6b9290e78ba04564896f582d1a9839ee",
            "9d8a1ad98de1467fbb1774dabe12afa6",
            "733b8148befe4eb8871cf6fdb3422038",
            "e6c88f000e184369ba675ff06ebea9d2",
            "963ba2824ada41e9894e039b7327bf6b",
            "442ec220c5194aa3960ed829509f5a73",
            "65b27853e08f4bb8862c377191e9b068"
          ]
        },
        "outputId": "a1f54926-ea55-4cc0-b520-a590be13ca6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/3.98k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac02431301874065bf19f5ad2659708a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed53737ac9e440bb8201f8c0ea758f02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9b53127cd194bac875f7049824cb464"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de6d3d8782af4d10937d1ca133b38598"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5632f80dc764e58ae379c63f3600edd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downlaod image embedding"
      ],
      "metadata": {
        "id": "uDm2WxI-N5bh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QyH6rCfDXRN",
        "outputId": "5f1efd9d-4ec6-4873-cacd-d408361808f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embeddings...\n",
            "Image: 000000401144.jpg\n",
            "Embedding shape: (512,)\n",
            "Embedding preview: [-0.13     0.1564   0.02017  0.1678   0.2393 ]...\n",
            "--------------------------------------------------\n",
            "Total number of embeddings: 81479\n"
          ]
        }
      ],
      "source": [
        "# URL of the embeddings file (replace with your actual URL)\n",
        "embeddings_url = '/content/drive/MyDrive/multimodal_llm/image_embedding/coco_image_embeddings.npz'\n",
        "\n",
        "# Load the embeddings\n",
        "print(\"Loading embeddings...\")\n",
        "embeddings = np.load(embeddings_url, allow_pickle=True)\n",
        "\n",
        "# Print embeddings and image names\n",
        "for image_name, embedding in embeddings.items():\n",
        "    print(f\"Image: {image_name}\")\n",
        "    print(f\"Embedding shape: {embedding.shape}\")\n",
        "    print(f\"Embedding preview: {embedding[:5]}...\")  # Print first 5 values\n",
        "    print(\"-\" * 50)\n",
        "    break\n",
        "\n",
        "print(f\"Total number of embeddings: {len(embeddings)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data processing"
      ],
      "metadata": {
        "id": "NwCmU3H-NABj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of URLs to download\n",
        "url = \"https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_150k.json\"\n",
        "\n",
        "# Download each file\n",
        "subprocess.run([\"wget\", \"-c\", url])\n",
        "\n",
        "# Load the downloaded JSON file\n",
        "json_file = \"llava_instruct_150k.json\"\n",
        "with open(json_file, 'r') as f:\n",
        "    data = json.load(f)\n"
      ],
      "metadata": {
        "id": "homAn3isNCOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data = data[0:10000]"
      ],
      "metadata": {
        "id": "-l600Sm5nOEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset():\n",
        "    processed_data = []\n",
        "    print(\"Processing data...\")\n",
        "    with tqdm(total=len(data)) as pbar:\n",
        "        for item in data:\n",
        "            image_file = item['image']\n",
        "            if image_file in embeddings:\n",
        "                processed_data.append({\n",
        "                    'image': image_file,\n",
        "                    'image_embedding': embeddings[image_file].tolist(),\n",
        "                    'conversation': item['conversations']\n",
        "                })\n",
        "            pbar.update(1)\n",
        "\n",
        "    print(f\"Data processing completed. Total processed items: {len(processed_data)}\")\n",
        "\n",
        "    return Dataset.from_dict({\n",
        "        \"image\": [item['image'] for item in processed_data],\n",
        "        \"image_embedding\": [item['image_embedding'] for item in processed_data],\n",
        "        \"conversation\": [item['conversation'] for item in processed_data]\n",
        "    })\n",
        "\n",
        "print(\"Creating HuggingFace dataset...\")\n",
        "hf_dataset = create_dataset()\n",
        "\n",
        "print(\"HuggingFace dataset creation completed.\")\n",
        "print(f\"Total samples in dataset: {len(hf_dataset)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OZtei2WxHlQ",
        "outputId": "70f806ad-8a90-4a28-cf9d-35b44ec2f1bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating HuggingFace dataset...\n",
            "Processing data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157712/157712 [12:11<00:00, 215.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data processing completed. Total processed items: 157712\n",
            "HuggingFace dataset creation completed.\n",
            "Total samples in dataset: 157712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(examples):\n",
        "    image_embeddings = []\n",
        "    conversations = []\n",
        "\n",
        "    for idx, conv in enumerate(examples['conversation']):\n",
        "        image_embedding = torch.tensor(examples['image_embedding'][idx])\n",
        "        dialogue_pairs = []\n",
        "\n",
        "        for i in range(0, len(conv), 2):\n",
        "            if i + 1 < len(conv):  # Ensure we have a pair\n",
        "                human_msg = conv[i]['value'].replace('<image>', '').strip()\n",
        "                gpt_msg = conv[i + 1]['value']\n",
        "\n",
        "                dialogue = [\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Given the following information, provide a detailed and accurate response:\\n{human_msg}\\n[An image is provided for this task.]\\n\"},\n",
        "                    {\"role\": \"assistant\", \"content\": gpt_msg}\n",
        "                ]\n",
        "\n",
        "                dialogue_pairs.append(dialogue)\n",
        "                image_embeddings.append(image_embedding)\n",
        "\n",
        "        conversations.extend(dialogue_pairs)\n",
        "\n",
        "    image_embeddings = torch.stack(image_embeddings)\n",
        "\n",
        "    tokenized_conversations = tokenizer.apply_chat_template(conversations,\n",
        "                                                            return_tensors='pt', padding=True)\n",
        "\n",
        "    return {\n",
        "        \"image_embeddings\": image_embeddings,\n",
        "        \"input_ids\": tokenized_conversations,\n",
        "        \"attention_mask\": torch.ones_like(tokenized_conversations),\n",
        "        \"labels\": tokenized_conversations.clone()\n",
        "    }\n",
        "\n",
        "# Test the prepare_dataset function with a real training example\n",
        "def test_prepare_dataset():\n",
        "    # Get a batch of examples from the dataset\n",
        "    batch_size = 1  # You can adjust this as needed\n",
        "    sample_batch = hf_dataset[5:5+batch_size]\n",
        "\n",
        "    print(\"Original conversations:\")\n",
        "    # for i, sample in enumerate(sample_batch):\n",
        "    #     print(f\"\\nSample {i + 1}:\")\n",
        "    for message in sample_batch['conversation'][0]:\n",
        "        print(f\"{message['from']}: {message['value']}\")\n",
        "\n",
        "    # Process the sample batch\n",
        "    result = prepare_dataset(sample_batch)\n",
        "\n",
        "    # Print the structure of the result\n",
        "    print(\"\\nResult keys:\", result.keys())\n",
        "    print(\"Image embeddings shape:\", result['image_embeddings'].shape)\n",
        "    print(\"Input IDs shape:\", result['input_ids'].shape)\n",
        "    print(\"Attention mask shape:\", result['attention_mask'].shape)\n",
        "    print(\"Labels shape:\", result['labels'].shape)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        decoded_input = tokenizer.decode(result['input_ids'][i])\n",
        "        decoded_labels = tokenizer.decode(result['labels'][i])\n",
        "\n",
        "        print(f\"\\nRestructured input for sample {i + 1}:\")\n",
        "        print(decoded_input)\n",
        "\n",
        "        print(f\"\\nLabels for sample {i + 1}:\")\n",
        "        print(decoded_labels)\n",
        "\n",
        "        # Optionally, you can print a more readable version of the labels\n",
        "        print(\"\\nReadable labels (non-padding tokens):\")\n",
        "        readable_labels = tokenizer.decode([token for token in result['labels'][i] if token != -100])\n",
        "        print(readable_labels)\n",
        "\n",
        "    # Optionally, you can print attention mask to see where it's applied\n",
        "    print(\"\\nAttention Mask:\")\n",
        "    print(result['attention_mask'][0])\n",
        "# Run the test\n",
        "test_prepare_dataset()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnMLoUn32aLf",
        "outputId": "872fdc35-c46c-41ad-a4a1-2048e1848b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original conversations:\n",
            "human: <image>\n",
            "What is the girl eating in the image?\n",
            "gpt: The girl in the image is eating a dessert, which appears to be a graham cracker treat or a cookie sandwich.\n",
            "human: Describe the girl's hair color and clothing.\n",
            "gpt: The girl has blonde hair, and she is wearing a pink shirt.\n",
            "human: What color is the plate that the dessert is on?\n",
            "gpt: The dessert is on a green plate.\n",
            "human: Is the girl looking at the camera or focusing on her dessert?\n",
            "gpt: The girl is looking up at the camera while taking a bite of her dessert.\n",
            "human: Where is the girl eating her dessert?\n",
            "gpt: The girl is eating her dessert at the table.\n",
            "\n",
            "Result keys: dict_keys(['image_embeddings', 'input_ids', 'attention_mask', 'labels'])\n",
            "Image embeddings shape: torch.Size([5, 512])\n",
            "Input IDs shape: torch.Size([5, 75])\n",
            "Attention mask shape: torch.Size([5, 75])\n",
            "Labels shape: torch.Size([5, 75])\n",
            "\n",
            "Restructured input for sample 1:\n",
            "<|system|> You are a helpful assistant.<|end|><|user|> Given the following information, provide a detailed and accurate response:\n",
            "What is the girl eating in the image?\n",
            "[An image is provided for this task.]\n",
            "<|end|><|assistant|> The girl in the image is eating a dessert, which appears to be a graham cracker treat or a cookie sandwich.<|end|><|endoftext|>\n",
            "\n",
            "Labels for sample 1:\n",
            "<|system|> You are a helpful assistant.<|end|><|user|> Given the following information, provide a detailed and accurate response:\n",
            "What is the girl eating in the image?\n",
            "[An image is provided for this task.]\n",
            "<|end|><|assistant|> The girl in the image is eating a dessert, which appears to be a graham cracker treat or a cookie sandwich.<|end|><|endoftext|>\n",
            "\n",
            "Readable labels (non-padding tokens):\n",
            "<|system|> You are a helpful assistant.<|end|><|user|> Given the following information, provide a detailed and accurate response:\n",
            "What is the girl eating in the image?\n",
            "[An image is provided for this task.]\n",
            "<|end|><|assistant|> The girl in the image is eating a dessert, which appears to be a graham cracker treat or a cookie sandwich.<|end|><|endoftext|>\n",
            "\n",
            "Attention Mask:\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply tokenization and prepare the dataset\n",
        "print(\"Applying tokenization and preparing the dataset...\")\n",
        "\n",
        "hf_dataset_mapped = hf_dataset.map(\n",
        "    prepare_dataset,\n",
        "    batched=True,\n",
        "    remove_columns=hf_dataset.column_names,\n",
        "    batch_size=1024  # Adjust based on your memory constraints\n",
        ").with_format(\"torch\")\n",
        "\n",
        "# Split the dataset\n",
        "train_test_split = hf_dataset_mapped.train_test_split(test_size=0.05)\n",
        "\n",
        "# Create a DatasetDict\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': train_test_split['train'],\n",
        "    'test': train_test_split['test']\n",
        "})\n",
        "\n",
        "print(f\"Train dataset size: {len(dataset_dict['train'])}\")\n",
        "print(f\"Test dataset size: {len(dataset_dict['test'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "abce03b6a7ec4f8ebca12ecb3fcf37ee",
            "65c701cd269046738129fff44ad296b7",
            "a281941fe40046dd830b1132961f840e",
            "609f7d3521c74f5c8042f7bfa0fdd9d0",
            "e7397fa23fa0483ba6c1249db8e8dcb7",
            "68efadcba17a4f49b0c956cf478a8967",
            "425874ead1484cbb947be684fd1607f5",
            "75b9249e3c574f188cd900d89ae5d3bd",
            "0b8f812c8b824b04a3603c59f8f17559",
            "9d2501078f99417abf477637329f66ea",
            "8db675be9cc745c98b40dd0488945ed1"
          ]
        },
        "id": "lbI3OPGyN3Br",
        "outputId": "e8f87a66-5875-4ab7-aa8f-632b2ca08087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying tokenization and preparing the dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/157712 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abce03b6a7ec4f8ebca12ecb3fcf37ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 343340\n",
            "Test dataset size: 18071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of accessing an item:\n",
        "sample = dataset_dict['train'][0]\n",
        "print(f\"Input IDs shape: {len(sample['input_ids'])}\")\n",
        "print(f\"Attention mask shape: {len(sample['attention_mask'])}\")\n",
        "print(f\"Labels shape: {len(sample['labels'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ky0l7yUc7avS",
        "outputId": "63bb393e-fd6b-4cd8-e7fc-09ca26eba2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs shape: 640\n",
            "Attention mask shape: 640\n",
            "Labels shape: 640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Projection Layer"
      ],
      "metadata": {
        "id": "inHiIuFzMxq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QLoRA set up"
      ],
      "metadata": {
        "id": "Sb90y2RNM6ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new_model = \"ms-phi3-custom\"\n",
        "lora_r = 16 #32 #64\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.05\n",
        "use_4bit = True\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "use_nested_quant = False\n",
        "output_dir = \"./multimodal-phi3_5-mini-instruct-llava_adapter\"\n",
        "num_train_epochs = 1\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "per_device_train_batch_size = 8\n",
        "per_device_eval_batch_size = 4\n",
        "gradient_accumulation_steps = 4\n",
        "gradient_checkpointing = True\n",
        "max_grad_norm = 1.0 #0.3\n",
        "learning_rate = 5e-4\n",
        "weight_decay = 0.0 #0.001\n",
        "optim = \"adamw_torch\" #\"paged_adamw_32bit\"\n",
        "lr_scheduler_type = \"linear\" #\"constant\"\n",
        "max_steps = -1\n",
        "warmup_ratio = 0.1 #0.03\n",
        "group_by_length = True\n",
        "save_steps = 25\n",
        "logging_steps = 25\n",
        "eval_steps = 50 # Evaluate every 25 steps\n",
        "max_seq_length = 256\n",
        "packing = False\n",
        "device_map = {\"\": 0}\n",
        "hf_adapter_repo=\"sayanbanerjee32/multimodal-phi3_5-mini-instruct-llava_adapter\""
      ],
      "metadata": {
        "id": "Mx6UI-r1M-0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageProjector(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim=1024):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.activation = nn.GELU()\n",
        "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.05)\n",
        "\n",
        "        # Store initial weights for both layers\n",
        "        self.register_buffer('initial_weights1', self.layer1.weight.data.clone())\n",
        "        self.register_buffer('initial_weights2', self.layer2.weight.data.clone())\n",
        "\n",
        "    def forward(self, x):\n",
        "        # # Print dtypes\n",
        "        # print(f\"Input dtype: {x.dtype}\")\n",
        "        # print(f\"Layer1 weight dtype: {self.layer1.weight.dtype}\")\n",
        "        # print(f\"Layer1 bias dtype: {self.layer1.bias.dtype}\")\n",
        "        # print(f\"Layer2 weight dtype: {self.layer2.weight.dtype}\")\n",
        "        # print(f\"Layer2 bias dtype: {self.layer2.bias.dtype}\")\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "    def get_weight_change(self):\n",
        "        current_weights1 = self.layer1.weight.data\n",
        "        current_weights2 = self.layer2.weight.data\n",
        "\n",
        "        # Ensure all tensors are on the same device\n",
        "        device = current_weights1.device\n",
        "        initial_weights1 = self.initial_weights1.to(device)\n",
        "        initial_weights2 = self.initial_weights2.to(device)\n",
        "\n",
        "        weight_diff1 = torch.norm(current_weights1 - initial_weights1).item()\n",
        "        weight_diff2 = torch.norm(current_weights2 - initial_weights2).item()\n",
        "        return weight_diff1 + weight_diff2  # Total weight change across both layers\n",
        "\n",
        "class Phi3WithProjector(PreTrainedModel):\n",
        "    supports_gradient_checkpointing = True\n",
        "    _supports_sdpa = True # Add this line\n",
        "\n",
        "    def __init__(self, phi3_model, projector, debug=False):\n",
        "        super().__init__(phi3_model.config)\n",
        "        self.phi3 = phi3_model\n",
        "        self.projector = projector\n",
        "        self.debug = debug\n",
        "\n",
        "    def debug_print(self, *args, **kwargs):\n",
        "        if self.debug:\n",
        "            print(*args, **kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, debug=False, **kwargs):\n",
        "        # Load the base Phi-3 model\n",
        "        phi3_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n",
        "\n",
        "        # Determine if it's a local path or a Hugging Face model ID\n",
        "        is_local = os.path.isdir(pretrained_model_name_or_path)\n",
        "\n",
        "        if is_local:\n",
        "            projector_path = os.path.join(pretrained_model_name_or_path, \"image_projector.pth\")\n",
        "        else:\n",
        "            try:\n",
        "                # Try to download the projector weights from the Hugging Face Hub\n",
        "                projector_path = hf_hub_download(repo_id=pretrained_model_name_or_path, filename=\"image_projector.pth\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to download projector weights: {e}\")\n",
        "                projector_path = None\n",
        "\n",
        "        if projector_path and os.path.exists(projector_path):\n",
        "            projector_state_dict = torch.load(projector_path, map_location=phi3_model.device)\n",
        "            # Check if the state dict has the expected structure\n",
        "            if 'linear.weight' in projector_state_dict:\n",
        "                input_dim = projector_state_dict['linear.weight'].size(1)\n",
        "                output_dim = projector_state_dict['linear.weight'].size(0)\n",
        "            else:\n",
        "                # If not, try to infer dimensions from the first layer's weight\n",
        "                first_key = next(iter(projector_state_dict))\n",
        "                input_dim = projector_state_dict[first_key].size(1)\n",
        "                output_dim = phi3_model.config.hidden_size  # Assuming this is the correct output dimension\n",
        "\n",
        "            projector = ImageProjector(input_dim, output_dim)\n",
        "\n",
        "            # # Convert projector weights and biases to the same dtype as the main model\n",
        "            # target_dtype = kwargs.get('torch_dtype', torch.float32)\n",
        "            # projector_state_dict = {k: v.to(target_dtype) for k, v in projector_state_dict.items()}\n",
        "\n",
        "            # Load the state dict with converted weights and biases\n",
        "            projector.load_state_dict(projector_state_dict, strict=False)\n",
        "\n",
        "            # # Ensure all parameters (including biases) are in the correct dtype\n",
        "            # for param in projector.parameters():\n",
        "            #     param.data = param.data.to(target_dtype)\n",
        "\n",
        "            print(f\"Loaded projector with input_dim={input_dim}, output_dim={output_dim}\")#, dtype={target_dtype}\")\n",
        "        else:\n",
        "            print(f\"Projector weights not found. Initializing with default dimensions.\")\n",
        "            input_dim = 512  # Default CLIP embedding size\n",
        "            output_dim = phi3_model.config.hidden_size\n",
        "            # target_dtype = kwargs.get('torch_dtype', torch.float32)\n",
        "            projector = ImageProjector(input_dim, output_dim)\n",
        "            # Ensure all parameters (including biases) are in the correct dtype\n",
        "            # for param in projector.parameters():\n",
        "            #     param.data = param.data.to(target_dtype)\n",
        "\n",
        "        # Move the projector to the same device as phi3_model\n",
        "        projector = projector.to(phi3_model.device)\n",
        "\n",
        "        # Create and return the Phi3WithProjector instance\n",
        "        model = cls(phi3_model, projector, debug=debug)\n",
        "        return model\n",
        "\n",
        "    def save_pretrained(self, save_directory):\n",
        "        print(f\"Saving model to {save_directory}\")\n",
        "\n",
        "        # Save the base model\n",
        "        self.phi3.save_pretrained(save_directory)\n",
        "\n",
        "        # Save the projector weights\n",
        "        projector_path = os.path.join(save_directory, \"image_projector.pth\")\n",
        "        projector_state = self.projector.state_dict()\n",
        "        print(f\"Projector weights stats before saving:\")\n",
        "        for name, param in projector_state.items():\n",
        "            print(f\"  {name}: mean={param.mean().item():.4f}, std={param.std().item():.4f}\")\n",
        "        torch.save(projector_state, projector_path)\n",
        "\n",
        "        # Save the config\n",
        "        self.config.save_pretrained(save_directory)\n",
        "\n",
        "        print(f\"Model saved successfully to {save_directory}\")\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None, image_embeddings=None, past_key_values=None, **kwargs):\n",
        "        with amp.autocast():\n",
        "            device = next(self.parameters()).device\n",
        "\n",
        "            if image_embeddings is not None:\n",
        "                projected_embeddings = self.projector(image_embeddings).to(torch.float16)\n",
        "                # Ensure projected_embeddings requires grad\n",
        "                if not projected_embeddings.requires_grad:\n",
        "                    projected_embeddings.requires_grad_(True)\n",
        "                projected_embeddings = projected_embeddings.unsqueeze(1)\n",
        "                self.debug_print(f\"forward projected_embeddings: {projected_embeddings.size()}\")\n",
        "\n",
        "                if past_key_values is None:  # This is the first forward pass\n",
        "                    self.debug_print(f\"forward before: {attention_mask.size() if attention_mask is not None else None}\")\n",
        "                    if 'inputs_embeds' in kwargs and kwargs['inputs_embeds'] is not None:\n",
        "                        inputs_embeds = kwargs['inputs_embeds']\n",
        "                        self.debug_print(f\"forward before inputs_embeds: {inputs_embeds.size()}\")\n",
        "                        inputs_embeds = torch.cat([projected_embeddings, inputs_embeds], dim=1)\n",
        "                        kwargs['inputs_embeds'] = inputs_embeds\n",
        "                        self.debug_print(f\"forward after inputs_embeds: {inputs_embeds.size()}\")\n",
        "                    elif input_ids is not None:\n",
        "                        self.debug_print(f\"forward input_ids: {input_ids.size()}\")\n",
        "                        inputs_embeds = self.get_input_embeddings()(input_ids.to(device))\n",
        "                        self.debug_print(f\"forward before inputs_embeds: {inputs_embeds.size()}\")\n",
        "                        inputs_embeds = torch.cat([projected_embeddings, inputs_embeds], dim=1)\n",
        "                        self.debug_print(f\"forward after inputs_embeds: {inputs_embeds.size()}\")\n",
        "                        kwargs['inputs_embeds'] = inputs_embeds\n",
        "                        input_ids = None  # Set to None to avoid conflict\n",
        "\n",
        "                    if attention_mask is not None:\n",
        "                        attention_mask = torch.cat([torch.ones(image_embeddings.size(0), 1, device=device), attention_mask.to(device)], dim=1)\n",
        "                    else:\n",
        "                        attention_mask = torch.ones(image_embeddings.size(0), inputs_embeds.size(1), device=device)\n",
        "\n",
        "                    if labels is not None:\n",
        "                        # Adjust labels to match the new sequence length\n",
        "                        labels = torch.cat([torch.full((labels.size(0), 1), -100, device=device), labels], dim=1)\n",
        "\n",
        "            if labels is not None:\n",
        "                labels = labels.to(device)\n",
        "\n",
        "            # Determine sequence length\n",
        "            if 'inputs_embeds' in kwargs and kwargs['inputs_embeds'] is not None:\n",
        "                seq_length = kwargs['inputs_embeds'].size(1)\n",
        "            elif input_ids is not None:\n",
        "                seq_length = input_ids.size(1)\n",
        "            else:\n",
        "                seq_length = attention_mask.size(1) if attention_mask is not None else None\n",
        "\n",
        "            if seq_length is None:\n",
        "                raise ValueError(\"Unable to determine sequence length. Provide either input_ids, inputs_embeds, or attention_mask.\")\n",
        "\n",
        "            # Ensure attention_mask matches the sequence length\n",
        "            if attention_mask is not None:\n",
        "                attention_mask = attention_mask[:, :seq_length]\n",
        "\n",
        "            self.debug_print(f\"forward final: input_ids shape: {input_ids.shape if input_ids is not None else None}\")\n",
        "            self.debug_print(f\"forward final: attention_mask shape: {attention_mask.shape if attention_mask is not None else None}\")\n",
        "            self.debug_print(f\"forward final: inputs_embeds shape: {kwargs.get('inputs_embeds', {}).shape if kwargs.get('inputs_embeds') is not None else None}\")\n",
        "\n",
        "            return self.phi3(input_ids=input_ids, attention_mask=attention_mask, labels=labels, past_key_values=past_key_values, **kwargs)\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **kwargs):\n",
        "        inputs = self.phi3.prepare_inputs_for_generation(input_ids, past=past, attention_mask=attention_mask, **kwargs)\n",
        "\n",
        "        if 'image_embeddings' in kwargs:\n",
        "            inputs['image_embeddings'] = kwargs['image_embeddings']\n",
        "\n",
        "            if past is None:  # First forward pass\n",
        "                # Adjust attention_mask to account for the image token\n",
        "                if attention_mask is not None:\n",
        "                    inputs['attention_mask'] = torch.cat([torch.ones((attention_mask.size(0), 1), device=attention_mask.device), attention_mask], dim=1)\n",
        "            else:  # Subsequent passes\n",
        "                # Ensure attention_mask matches the current sequence length\n",
        "                if attention_mask is not None:\n",
        "                    current_seq_length = past[0][0].size(2) + 1  # past key's sequence length + 1 for the new token\n",
        "                    inputs['attention_mask'] = attention_mask[:, :current_seq_length]\n",
        "\n",
        "            inputs.pop('position_ids', None)\n",
        "\n",
        "        # Safe printing of shapes\n",
        "        self.debug_print(f\"prepare_inputs_for_generation: input_ids shape: {inputs['input_ids'].shape if 'input_ids' in inputs else None}\")\n",
        "        self.debug_print(f\"prepare_inputs_for_generation: attention_mask shape: {inputs['attention_mask'].shape if 'attention_mask' in inputs else None}\")\n",
        "        self.debug_print(f\"prepare_inputs_for_generation: inputs_embeds shape: {inputs.get('inputs_embeds', {}).shape if inputs.get('inputs_embeds') is not None else None}\")\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.phi3.get_input_embeddings()\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.phi3.set_input_embeddings(value)\n",
        "\n",
        "    def gradient_checkpointing_enable(self, **kwargs):\n",
        "        self.phi3.gradient_checkpointing_enable(**kwargs)\n",
        "\n",
        "    def gradient_checkpointing_disable(self):\n",
        "        self.phi3.gradient_checkpointing_disable()\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        try:\n",
        "            return super().__getattr__(name)\n",
        "        except AttributeError:\n",
        "            return getattr(self.phi3, name)\n",
        "\n",
        "    def generate(self, input_ids=None, attention_mask=None, image_embeddings=None, **kwargs):\n",
        "        if image_embeddings is not None:\n",
        "            kwargs['image_embeddings'] = image_embeddings\n",
        "            self.debug_print(f\"generate input_ids: {input_ids.size()}\")\n",
        "            self.debug_print(f\"generate image_embedding: {image_embeddings.size()}\")\n",
        "\n",
        "        if attention_mask is not None and image_embeddings is not None:\n",
        "            # Add an extra attention mask token for the image embedding\n",
        "            self.debug_print(f\"generate before: {attention_mask.size()}\")\n",
        "            attention_mask = torch.cat([torch.ones(attention_mask.size(0), 1, device=attention_mask.device), attention_mask], dim=1)\n",
        "            self.debug_print(f\"generate after: {attention_mask.size()}\")\n",
        "\n",
        "        return super().generate(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n"
      ],
      "metadata": {
        "id": "-BBqvYmtS8ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path in Google Drive where you want to save the checkpoints\n",
        "gdrive_checkpoint_dir = \"/content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints\"\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(gdrive_checkpoint_dir, exist_ok=True)\n",
        "\n",
        "class SaveLatestCheckpointAndLoraCallback(TrainerCallback):\n",
        "    # def __init__(self, tokenizer):\n",
        "    #     super().__init__()\n",
        "    #     self.tokenizer = tokenizer\n",
        "\n",
        "    def on_save(self, args, state, control, **kwargs):\n",
        "        if state.is_world_process_zero:\n",
        "            checkpoint_dir = os.path.join(gdrive_checkpoint_dir, f\"checkpoint-{state.global_step}\")\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "            # Save the model and tokenizer\n",
        "            kwargs[\"model\"].save_pretrained(checkpoint_dir)\n",
        "            # Save the tokenizer\n",
        "            kwargs[\"tokenizer\"].save_pretrained(checkpoint_dir)\n",
        "            # self.tokenizer.save_pretrained(checkpoint_dir)\n",
        "\n",
        "            # Log and save the projector\n",
        "            projector = kwargs[\"model\"].base_model.model.projector\n",
        "            weight_change = projector.get_weight_change()\n",
        "            print(f\"Projector weight change at step {state.global_step}: {weight_change}\")\n",
        "\n",
        "            # # Add gradient checking here\n",
        "            # for name, param in projector.named_parameters():\n",
        "            #     if param.grad is None:\n",
        "            #         print(f\"No gradient for {name}\")\n",
        "            #     else:\n",
        "            #         print(f\"Gradient norm for {name}: {param.grad.norm().item()}\")\n",
        "\n",
        "            projector_path = os.path.join(checkpoint_dir, \"image_projector.pth\")\n",
        "            torch.save(projector.state_dict(), projector_path)\n",
        "\n",
        "            # Save LoRA weights\n",
        "            lora_state_dict = {}\n",
        "            for name, module in kwargs[\"model\"].base_model.model.phi3.named_modules():\n",
        "                if isinstance(module, lora.Linear4bit):\n",
        "                    if hasattr(module, 'lora_A'):\n",
        "                        lora_state_dict[f\"{name}.lora_A.weight\"] = module.lora_A.default.weight.data.cpu()\n",
        "                        lora_state_dict[f\"{name}.lora_B.weight\"] = module.lora_B.default.weight.data.cpu()\n",
        "                        lora_state_dict[f\"{name}.scaling\"] = module.scaling\n",
        "\n",
        "                    # Save lora_embedding_A and lora_embedding_B if they exist and are not empty\n",
        "                    if hasattr(module, 'lora_embedding_A') and module.lora_embedding_A:\n",
        "                        lora_state_dict[f\"{name}.lora_embedding_A\"] = {k: v.cpu() for k, v in module.lora_embedding_A.items()}\n",
        "                    if hasattr(module, 'lora_embedding_B') and module.lora_embedding_B:\n",
        "                        lora_state_dict[f\"{name}.lora_embedding_B\"] = {k: v.cpu() for k, v in module.lora_embedding_B.items()}\n",
        "\n",
        "            torch.save(lora_state_dict, os.path.join(checkpoint_dir, \"lora_weights.pt\"))\n",
        "\n",
        "            # Explicitly save the trainer state\n",
        "            trainer_state_path = os.path.join(checkpoint_dir, \"trainer_state.json\")\n",
        "            state_dict = dataclasses.asdict(state)\n",
        "            with open(trainer_state_path, \"w\") as f:\n",
        "                json.dump(state_dict, f, indent=2)\n",
        "\n",
        "            # Remove previous checkpoint\n",
        "            prev_checkpoint = os.path.join(gdrive_checkpoint_dir, f\"checkpoint-{state.global_step - args.save_steps}\")\n",
        "            if os.path.exists(prev_checkpoint):\n",
        "                import shutil\n",
        "                shutil.rmtree(prev_checkpoint)\n",
        "\n",
        "            print(f\"Saved checkpoint to {checkpoint_dir}\")\n",
        "\n",
        "    #         # Upload the checkpoint to Hugging Face Hub\n",
        "    #         self.upload_to_hub(checkpoint_dir, args.hub_model_id)\n",
        "\n",
        "    # def upload_to_hub(self, checkpoint_dir, hub_model_id):\n",
        "    #     api = HfApi()\n",
        "    #     api.upload_folder(\n",
        "    #         folder_path=checkpoint_dir,\n",
        "    #         repo_id=hub_model_id,\n",
        "    #         repo_type=\"model\",\n",
        "    #     )\n",
        "    #     print(f\"Uploaded checkpoint {checkpoint_dir} to Hugging Face Hub\")\n",
        "\n",
        "# Function to get the latest checkpoint\n",
        "def get_latest_checkpoint(checkpoint_dir):\n",
        "    checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith('checkpoint-')]\n",
        "    if not checkpoints:\n",
        "        return None\n",
        "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[1]))\n",
        "    return os.path.join(checkpoint_dir, latest_checkpoint)"
      ],
      "metadata": {
        "id": "CB_fUxzpiBUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_bf16_supported():\n",
        "  compute_dtype = torch.bfloat16\n",
        "#   attn_implementation = 'flash_attention_2'\n",
        "else:\n",
        "  compute_dtype = torch.float16\n",
        "#   attn_implementation = 'sdpa'\n",
        "\n",
        "# print(attn_implementation)\n",
        "print(compute_dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM0oP2VqCN64",
        "outputId": "2346ebbf-d226-4dc4-f56c-7d8317bb3683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.bfloat16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n"
      ],
      "metadata": {
        "id": "kgE8Vq_i6kMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_lora_weights(checkpoint_path, device):\n",
        "    # Check if the checkpoint_path is a local directory or a Hugging Face repo\n",
        "    if os.path.isdir(checkpoint_path):\n",
        "        # Local directory\n",
        "        lora_weights_path = os.path.join(checkpoint_path, \"lora_weights.pt\")\n",
        "        if os.path.exists(lora_weights_path):\n",
        "            lora_state_dict = torch.load(lora_weights_path, map_location=device)\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"LoRA weights file not found in {checkpoint_path}\")\n",
        "    else:\n",
        "        # Assume it's a Hugging Face repo\n",
        "        try:\n",
        "            # Try to download the file from the Hugging Face repo\n",
        "            lora_weights_path = hf_hub_download(repo_id=checkpoint_path, filename=\"lora_weights.pt\")\n",
        "            lora_state_dict = torch.load(lora_weights_path, map_location=device)\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Error loading LoRA weights from Hugging Face repo: {str(e)}\")\n",
        "\n",
        "    return lora_state_dict\n",
        "\n",
        "def load_model_with_lora_and_projector(checkpoint_path, device, bnb_config=None, debug=False, for_training=False):\n",
        "    is_cpu = device == \"cpu\"\n",
        "\n",
        "    common_kwargs = {\n",
        "        \"trust_remote_code\": True,\n",
        "        \"debug\": debug\n",
        "    }\n",
        "\n",
        "    if is_cpu:\n",
        "        device_map = None\n",
        "        model = Phi3WithProjector.from_pretrained(\n",
        "            checkpoint_path,\n",
        "            torch_dtype=torch.float32,\n",
        "            low_cpu_mem_usage=True,\n",
        "            device_map=device_map,\n",
        "            **common_kwargs\n",
        "        )\n",
        "    else:\n",
        "        device_map = {\"\": 0}  # This maps all modules to the specified device\n",
        "        model = Phi3WithProjector.from_pretrained(\n",
        "            checkpoint_path,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=device_map,  # Use the corrected device_map\n",
        "            torch_dtype=torch.float16,\n",
        "            attn_implementation='eager',\n",
        "            **common_kwargs\n",
        "        )\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Load LoRA weights\n",
        "    lora_state_dict = load_lora_weights(checkpoint_path, device)\n",
        "    print(f\"Total keys in lora_state_dict: {len(lora_state_dict)}\")\n",
        "\n",
        "    new_state_dict = {}\n",
        "    scaling_factors_loaded = 0\n",
        "\n",
        "    for name, module in model.phi3.named_modules():\n",
        "        if isinstance(module, lora.Linear4bit):\n",
        "            lora_A_key = f\"{name}.lora_A.weight\"\n",
        "            lora_B_key = f\"{name}.lora_B.weight\"\n",
        "            scaling_key = f\"{name}.scaling\"\n",
        "\n",
        "            if lora_A_key in lora_state_dict and lora_B_key in lora_state_dict:\n",
        "                new_state_dict[f\"{name}.lora_A.default.weight\"] = lora_state_dict[lora_A_key]\n",
        "                new_state_dict[f\"{name}.lora_B.default.weight\"] = lora_state_dict[lora_B_key]\n",
        "\n",
        "                if scaling_key in lora_state_dict:\n",
        "                    module.scaling = lora_state_dict[scaling_key]\n",
        "                    scaling_factors_loaded += 1\n",
        "            else:\n",
        "                print(f\"Warning: LoRA weights for {name} not found in checkpoint\")\n",
        "\n",
        "    # Load the filtered state dict\n",
        "    model.phi3.load_state_dict(new_state_dict, strict=False)\n",
        "\n",
        "    print(f\"Loaded LoRA weights: {len(new_state_dict)} / {len(lora_state_dict)}\")\n",
        "    print(f\"Loaded scaling factors: {scaling_factors_loaded}\")\n",
        "    print(f\"Total LoRA modules processed: {len(new_state_dict) // 2}\")\n",
        "\n",
        "    if not for_training:\n",
        "        # Prepare the model for inference only if not for training\n",
        "        for name, module in model.named_modules():\n",
        "            if isinstance(module, LinearFP4): # or isinstance(module, LinearFP8):\n",
        "                module.prepare_for_inference()\n",
        "    else:\n",
        "        # Ensure the model is in training mode\n",
        "        model.train()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "gHa5rGNQoLq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp -r /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoint-350 /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/"
      ],
      "metadata": {
        "id": "fHqokMu1NkeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the latest checkpoint\n",
        "latest_checkpoint = get_latest_checkpoint(gdrive_checkpoint_dir)\n",
        "eval_first = False\n",
        "if latest_checkpoint:\n",
        "    print(f\"Loading model from checkpoint: {latest_checkpoint}\")\n",
        "    model = load_model_with_lora_and_projector(latest_checkpoint, device,\n",
        "                                               bnb_config=bnb_config, for_training = True)\n",
        "    eval_first = True\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting training from scratch.\")\n",
        "    phi3_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        trust_remote_code=True,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=device_map,\n",
        "        torch_dtype=compute_dtype,\n",
        "        attn_implementation='eager'\n",
        "    )\n",
        "    image_embedding_dim = len(hf_dataset[0]['image_embedding'])\n",
        "    projection_dim = phi3_model.config.hidden_size\n",
        "    projector = ImageProjector(image_embedding_dim, projection_dim).to(device)\n",
        "    model = Phi3WithProjector(phi3_model, projector)\n",
        "\n",
        "    # Prepare the model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "UXVkny4DWOdU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "d684988b77f6441fab2683d275266ad1",
            "c753a98543e34740b017cff896186242",
            "7e9384d9472241708c8a9d48302fb708",
            "e6583b52cfe64e8294077932c3734a51",
            "6c24b51b583442ebafefe1d13fb2f156",
            "15839d3e862f4eec8f1181037b31d66c",
            "802fbd5fdde14b0e9299d71f86385cfd",
            "502f4201c3474afeb522f73aed3a13fe",
            "23b7080eda9f4cc7bf9ed1719f9c39e7",
            "113b235b03a4424cb33eff9c1e493dc0",
            "9727149286dc491baf1c69221aadf0c0"
          ]
        },
        "outputId": "9fe663ce-c9de-43ca-f842-0d778e7d5fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from checkpoint: /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-402\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d684988b77f6441fab2683d275266ad1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading adapter weights from /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-402 led to unexpected keys not found in the model:  ['phi3.model.layers.0.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.0.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.1.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.1.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.10.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.10.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.11.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.11.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.12.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.12.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.13.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.13.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.14.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.14.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.15.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.15.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.16.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.16.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.17.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.17.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.18.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.18.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.19.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.19.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.2.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.2.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.20.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.20.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.21.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.21.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.22.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.22.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.23.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.23.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.24.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.24.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.25.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.25.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.26.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.26.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.27.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.27.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.28.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.28.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.29.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.29.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.3.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.3.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.30.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.30.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.31.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.31.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.4.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.4.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.5.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.5.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.6.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.6.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.7.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.7.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.8.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.8.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.9.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.9.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.9.self_attn.o_proj.lora_B.default.weight']. \n",
            "<ipython-input-55-e3339eabcc31>:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  projector_state_dict = torch.load(projector_path, map_location=phi3_model.device)\n",
            "<ipython-input-59-153e88ecfc4b>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lora_state_dict = torch.load(lora_weights_path, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded projector with input_dim=512, output_dim=3072\n",
            "Total keys in lora_state_dict: 192\n",
            "Loaded LoRA weights: 128 / 192\n",
            "Loaded scaling factors: 64\n",
            "Total LoRA modules processed: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(compute_dtype) , print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGcgsGvR6uw0",
        "outputId": "f9800548-9880-4409-98c6-056b2617b1f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float16\n",
            "Phi3WithProjector(\n",
            "  (phi3): Phi3ForCausalLM(\n",
            "    (model): Phi3Model(\n",
            "      (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
            "      (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0-31): 32 x Phi3DecoderLayer(\n",
            "          (self_attn): Phi3Attention(\n",
            "            (o_proj): lora.Linear4bit(\n",
            "              (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=3072, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "              (lora_magnitude_vector): ModuleDict()\n",
            "            )\n",
            "            (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
            "            (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
            "          )\n",
            "          (mlp): Phi3MLP(\n",
            "            (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
            "            (down_proj): lora.Linear4bit(\n",
            "              (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=8192, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=3072, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "              (lora_magnitude_vector): ModuleDict()\n",
            "            )\n",
            "            (activation_fn): SiLU()\n",
            "          )\n",
            "          (input_layernorm): Phi3RMSNorm()\n",
            "          (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (post_attention_layernorm): Phi3RMSNorm()\n",
            "        )\n",
            "      )\n",
            "      (norm): Phi3RMSNorm()\n",
            "    )\n",
            "    (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
            "  )\n",
            "  (projector): ImageProjector(\n",
            "    (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (activation): GELU(approximate='none')\n",
            "    (layer2): Linear(in_features=1024, out_features=3072, bias=True)\n",
            "    (dropout): Dropout(p=0.05, inplace=False)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "ADlAMa4t6t7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=lora_r,\n",
        "    lora_alpha=lora_alpha,\n",
        "    target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
        "    lora_dropout=lora_dropout,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8wZ9oGItcDE",
        "outputId": "59901cd9-b476-408f-a779-78e06e178552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 8912896 || all params: 2021727232 || trainable%: 0.44085551497384196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_projector_weight_changes(loaded_state_dict, initial_state_dict):\n",
        "    for key in loaded_state_dict:\n",
        "        if torch.norm(loaded_state_dict[key] - initial_state_dict[key]).item() > 0:\n",
        "            print(f\"Weights changed for {key}\")\n",
        "            return True\n",
        "    print(\"WARNING: No weight changes detected in the projector!\")\n",
        "    return False\n",
        "\n",
        "# Use this when loading the model\n",
        "initial_projector_state = copy.deepcopy(model.base_model.model.projector.state_dict())"
      ],
      "metadata": {
        "id": "Ob2geqvyIeU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "RZLnz1M4NKqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "AVcaw9k7NDdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size  = per_device_eval_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"all\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=eval_steps, # Evaluate every 25 steps\n",
        "    # Add these new arguments\n",
        "    do_eval=True,\n",
        "    eval_delay=0,  # Start evaluation immediately\n",
        "    # Enable gradient checkpointing\n",
        "    gradient_checkpointing=gradient_checkpointing,\n",
        "    # Disable data parallelism if not needed\n",
        "    ddp_find_unused_parameters=False,\n",
        "    save_total_limit=1,  # Keep only the latest checkpoint\n",
        "    hub_model_id = hf_adapter_repo,\n",
        "    push_to_hub=False,  # We'll handle this in our custom callback\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "zGBG0EJINFCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom data collator to handle pre-tokenized inputs\n",
        "def custom_data_collator(features):\n",
        "    batch = {k: [d[k] for d in features] for k in features[0].keys()}\n",
        "\n",
        "    # Stack image embeddings\n",
        "    batch['image_embeddings'] = torch.stack(batch['image_embeddings'])#.to(torch.float16)\n",
        "\n",
        "    # Pad the sequences\n",
        "    batch['input_ids'] = torch.nn.utils.rnn.pad_sequence(batch['input_ids'], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    batch['attention_mask'] = torch.nn.utils.rnn.pad_sequence(batch['attention_mask'], batch_first=True, padding_value=0)\n",
        "    batch['labels'] = torch.nn.utils.rnn.pad_sequence(batch['labels'], batch_first=True, padding_value=-100)\n",
        "\n",
        "    return batch\n",
        "\n",
        "# Function to select a random subset of the dataset\n",
        "def select_subset(dataset, fraction=0.05):\n",
        "    num_samples = int(len(dataset) * fraction)\n",
        "    indices = random.sample(range(len(dataset)), num_samples)\n",
        "    return dataset.select(indices)\n",
        "\n",
        "# Select 5% of the training and test datasets\n",
        "small_train_dataset = select_subset(dataset_dict['train'], fraction=0.2)\n",
        "small_test_dataset = select_subset(dataset_dict['test'], fraction=0.05)\n",
        "\n",
        "# Create a new DatasetDict with the smaller datasets\n",
        "small_dataset_dict = DatasetDict({\n",
        "    'train': small_train_dataset,\n",
        "    'test': small_test_dataset\n",
        "})\n",
        "\n",
        "print(f\"Small train dataset size: {len(small_dataset_dict['train'])}\")\n",
        "print(f\"Small test dataset size: {len(small_dataset_dict['test'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDU9tnyZW_CW",
        "outputId": "1fe89b03-a2d6-4aa6-c795-e52d134b9f84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small train dataset size: 68668\n",
            "Small test dataset size: 903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before initializing the SFTTrainer\n",
        "model.projector.train()  # Set projector to training mode\n",
        "for param in model.projector.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "def hook_fn(module, grad_input, grad_output):\n",
        "    print(f\"Gradient for {module.__class__.__name__}:\")\n",
        "    for idx, grad in enumerate(grad_input):\n",
        "        if grad is not None:\n",
        "            print(f\"  Input gradient {idx}: {grad.norm().item()}\")\n",
        "\n",
        "# # Apply the hook to projector layers\n",
        "# model.projector.layer1.register_backward_hook(hook_fn)\n",
        "# model.projector.layer2.register_backward_hook(hook_fn)"
      ],
      "metadata": {
        "id": "tG1jGxI4B48E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_dict['train'],\n",
        "    # eval_dataset=dataset_dict['test'],\n",
        "    # train_dataset=small_dataset_dict['train'],\n",
        "    eval_dataset=small_dataset_dict['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=custom_data_collator,\n",
        "    peft_config=lora_config,\n",
        "    max_seq_length=max_seq_length,\n",
        "    packing=packing,\n",
        "    # callbacks=[SaveLatestCheckpointCallback(), SaveQLoraAdapterCallback(model)],  # Add the custom callback\n",
        "    callbacks=[SaveLatestCheckpointAndLoraCallback()],\n",
        ")\n",
        "# # Cast the model's projector to float32 before evaluation\n",
        "# model.projector = model.projector.to(torch.float32)\n",
        "# Perform initial evaluation\n",
        "if eval_first:\n",
        "    print(\"Performing initial evaluation...\")\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"Initial evaluation results: {eval_results}\")\n",
        "    # Start or resume training\n",
        "    trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
        "else:\n",
        "    # Start training\n",
        "    trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yGi1pNF98jy5",
        "outputId": "2e3b8b92-d636-4d4f-be2f-e4c01c2b7d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing initial evaluation...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='452' max='226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [226/226 55:49]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial evaluation results: {'eval_loss': 3.195535182952881, 'eval_model_preparation_time': 0.006, 'eval_runtime': 513.1472, 'eval_samples_per_second': 1.76, 'eval_steps_per_second': 0.44}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='693' max='10729' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  693/10729 4:22:02 < 151:40:06, 0.02 it/s, Epoch 0.06/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Model Preparation Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.431500</td>\n",
              "      <td>0.286102</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.575900</td>\n",
              "      <td>0.254819</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.545500</td>\n",
              "      <td>0.308900</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.580500</td>\n",
              "      <td>0.403610</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.711700</td>\n",
              "      <td>0.397242</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Projector weight change at step 425: 5.682918667793274\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-425\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Projector weight change at step 450: 5.699537515640259\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-450\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Projector weight change at step 475: 5.718528628349304\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-475\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Projector weight change at step 500: 5.737922430038452\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Projector weight change at step 525: 5.775274634361267\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-525\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Projector weight change at step 550: 5.8188323974609375\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-550\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Projector weight change at step 575: 5.864935755729675\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-575\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Projector weight change at step 600: 5.934548258781433\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Projector weight change at step 625: 6.132174253463745\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-625\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Projector weight change at step 650: 6.395779132843018\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-650\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Projector weight change at step 675: 6.591826796531677\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-675\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='909' max='10729' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  909/10729 7:46:45 < 151:16:12, 0.02 it/s, Epoch 0.08/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Model Preparation Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.431500</td>\n",
              "      <td>0.286102</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.575900</td>\n",
              "      <td>0.254819</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.545500</td>\n",
              "      <td>0.308900</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.580500</td>\n",
              "      <td>0.403610</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.711700</td>\n",
              "      <td>0.397242</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.595600</td>\n",
              "      <td>0.573870</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.520900</td>\n",
              "      <td>0.513050</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>7.528600</td>\n",
              "      <td>8.223042</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.148900</td>\n",
              "      <td>0.889543</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.453100</td>\n",
              "      <td>0.282658</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Projector weight change at step 700: 6.769166707992554\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Projector weight change at step 725: 7.083357453346252\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Projector weight change at step 750: 7.28429651260376\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Projector weight change at step 775: 8.066297054290771\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Projector weight change at step 800: 11.14905858039856\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Projector weight change at step 825: 12.756167650222778\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Projector weight change at step 850: 12.982306003570557\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Projector weight change at step 875: 13.006444931030273\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Projector weight change at step 900: 13.008190155029297\n",
            "Saved checkpoint to /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-f9d2503bea7e>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Initial evaluation results: {eval_results}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Start or resume training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trl_activate_neftune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;31m# After training we make sure to retrieve back the original forward pass method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2051\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2052\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2053\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2054\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2388\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3516\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3518\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3520\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2241\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the fine-tuned model\n",
        "# trainer.save_model()\n",
        "# # trainer.model.save_pretrained(new_model)\n",
        "# final_model_path = os.path.join(gdrive_checkpoint_dir, \"final_model\")\n",
        "# trainer.model.save_pretrained(final_model_path)\n",
        "# tokenizer.save_pretrained(final_model_path)"
      ],
      "metadata": {
        "id": "6eGu422_8mLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.base_model.model.projector.state_dict(), final_model_path + '/image_projector.pth')\n",
        "# print(f\"Projector saved to: {final_model_path}/image_projector.pth\")"
      ],
      "metadata": {
        "id": "NgYtfm2bUP-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the projector\n",
        "# projector_path = '/content/drive/MyDrive/multimodal_llm/phi-3_5/image_projector.pth'\n",
        "# os.makedirs(os.path.dirname(projector_path), exist_ok=True)\n",
        "# torch.save(model.projector.state_dict(), projector_path)\n",
        "# print(f\"Projector saved to: {projector_path}\")"
      ],
      "metadata": {
        "id": "QoBIZYRVQjaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the latest checkpoint\n",
        "latest_checkpoint = get_latest_checkpoint(gdrive_checkpoint_dir)\n",
        "loaded_projector_state = torch.load(latest_checkpoint + '/image_projector.pth')\n",
        "verify_projector_weight_changes(loaded_projector_state, initial_projector_state)"
      ],
      "metadata": {
        "id": "WV38oDxeIkQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "163fdaed-fcbc-48d8-ac17-12982ecc81cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights changed for layer1.weight\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-aa93cb025ceb>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  loaded_projector_state = torch.load(latest_checkpoint + '/image_projector.pth')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # trainer.push_to_hub()\n",
        "# api = HfApi()\n",
        "# api.upload_folder(\n",
        "#     folder_path=latest_checkpoint,\n",
        "#     repo_id=hf_adapter_repo,\n",
        "#     repo_type=\"model\",\n",
        "# )"
      ],
      "metadata": {
        "id": "wTx7oRvuYZ7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp -r multimodal-phi3_5-mini-instruct-llava_adapter /content/drive/MyDrive/multimodal_llm/phi-3_5"
      ],
      "metadata": {
        "id": "UwRtBYUXWnsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this function after your CustomTextGenerator class\n",
        "def evaluate_model(model, tokenizer, eval_dataset, device):\n",
        "    model = model.to(device).to(torch.float16)\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    dataloader = DataLoader(eval_dataset, batch_size=2, collate_fn=custom_data_collator)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch = {\n",
        "                'input_ids': batch['input_ids'].to(device).long(),\n",
        "                'attention_mask': batch['attention_mask'].to(device).long(),\n",
        "                'image_embeddings': batch['image_embeddings'].to(device).to(torch.float16),\n",
        "                'labels': batch['labels'].to(device).long()\n",
        "            }\n",
        "\n",
        "            # print(f\"input_ids shape: {batch['input_ids'].shape}, dtype: {batch['input_ids'].dtype}\")\n",
        "            # print(f\"attention_mask shape: {batch['attention_mask'].shape}, dtype: {batch['attention_mask'].dtype}\")\n",
        "            # print(f\"image_embeddings shape: {batch['image_embeddings'].shape}, dtype: {batch['image_embeddings'].dtype}\")\n",
        "            # print(f\"labels shape: {batch['labels'].shape}, dtype: {batch['labels'].dtype}\")\n",
        "\n",
        "            try:\n",
        "                outputs = model(\n",
        "                    input_ids=batch['input_ids'],\n",
        "                    attention_mask=batch['attention_mask'],\n",
        "                    image_embeddings=batch['image_embeddings'],\n",
        "                    labels=batch['labels']\n",
        "                )\n",
        "                total_loss += outputs.loss.item()\n",
        "                num_batches += 1\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Error processing batch: {e}\")\n",
        "                print(f\"Model device: {next(model.parameters()).device}\")\n",
        "                for name, param in model.named_parameters():\n",
        "                    print(f\"{name} - shape: {param.shape}, dtype: {param.dtype}, device: {param.device}\")\n",
        "                raise  # Re-raise the exception to see the full traceback\n",
        "\n",
        "    return total_loss / num_batches if num_batches > 0 else 0\n",
        "\n",
        "# Add these lines before merging the model\n",
        "print(\"Evaluating model before merging...\")\n",
        "pre_merge_loss = evaluate_model(model, tokenizer, small_dataset_dict['test'], device)\n",
        "print(f\"Pre-merge loss: {pre_merge_loss}\")"
      ],
      "metadata": {
        "id": "qiaCJXZuBva5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada725ac-ef66-461a-b5d7-7a2455ec7125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model before merging...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-merge loss: 0.3213212061673403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sample inference code"
      ],
      "metadata": {
        "id": "i3TooP90Xd94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "w0bqPi-V5q_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom text generation class\n",
        "class CustomTextGenerator:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def generate(self, input_text, image_embedding, **generate_kwargs):\n",
        "        # Tokenize the input text\n",
        "        inputs = self.tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        input_ids = inputs[\"input_ids\"].to(self.model.device)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(self.model.device)\n",
        "\n",
        "        # Ensure image_embedding is a tensor and move it to the correct device\n",
        "        if not isinstance(image_embedding, torch.Tensor):\n",
        "            image_embedding = torch.tensor(image_embedding)\n",
        "        # image_embedding = image_embedding.to(self.model.device).unsqueeze(0)  # Add batch dimension\n",
        "        image_embedding = image_embedding.to(self.model.device)#.to(next(self.model.parameters()).dtype)\n",
        "\n",
        "        # Generate text\n",
        "        outputs = self.model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            # image_embeddings=image_embedding,\n",
        "            image_embeddings=image_embedding.unsqueeze(0),  # Add batch dimension\n",
        "            **generate_kwargs\n",
        "        )\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return generated_text"
      ],
      "metadata": {
        "id": "Hsp71KFMB0-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a sample from the validation set\n",
        "sample = dataset_dict['test'][1]\n",
        "image_embedding = sample['image_embeddings']\n",
        "\n",
        "\n",
        "def get_first_user_input(decoded_text):\n",
        "    # Find the position of the first <|assistant|> tag\n",
        "    assistant_pos = decoded_text.find('<|assistant|>')\n",
        "\n",
        "    # If <|assistant|> is found, truncate the text\n",
        "    if assistant_pos != -1:\n",
        "        return decoded_text[:assistant_pos].strip()\n",
        "    else:\n",
        "        return decoded_text.strip()\n",
        "\n",
        "# Decode the input_ids\n",
        "full_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=False)\n",
        "\n",
        "# Extract only the first user input\n",
        "input_text = get_first_user_input(full_text) + '<|assistant|>'\n",
        "print(input_text)"
      ],
      "metadata": {
        "id": "RHuBr77NB4Wi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e2f1593-7b7a-49de-d70e-a5ba02238438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><|system|> You are a helpful assistant.<|end|><|user|> Given the following information, provide a detailed and accurate response:\n",
            "What's happening in the scene?\n",
            "[An image is provided for this task.]\n",
            "<|end|><|assistant|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "# Initialize the custom text generator\n",
        "generator = CustomTextGenerator(model=model, tokenizer=tokenizer)\n",
        "generated_text = generator.generate(\n",
        "    input_text,\n",
        "    image_embedding=image_embedding,\n",
        "    # max_length=200,\n",
        "    # num_return_sequences=1,\n",
        "    # do_sample=True,\n",
        "    # temperature=0.7,\n",
        "    # top_k=50,\n",
        "    # top_p=0.95,\n",
        "    max_new_tokens=150,\n",
        "    num_return_sequences=1,\n",
        "    do_sample=True,\n",
        "    temperature=0.8,\n",
        "    top_k=40,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2,\n",
        "    no_repeat_ngram_size=3,\n",
        ")\n",
        "\n",
        "print(\"Input text:\")\n",
        "print(input_text)\n",
        "print(\"\\nGenerated text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "2_ytnYhQCItH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6c3195-fd2c-4f96-b62d-97c96474fcff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text:\n",
            "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><|system|> You are a helpful assistant.<|end|><|user|> Given the following information, provide a detailed and accurate response:\n",
            "What's happening in the scene?\n",
            "[An image is provided for this task.]\n",
            "<|end|><|assistant|>\n",
            "\n",
            "Generated text:\n",
            "You are a helpful assistant. Given the following information, provide a detailed and accurate response:\n",
            "What's happening in the scene?\n",
            "[An image is provided for this task.]\n",
            ".</p> \n",
            "### Respond to it!  \n",
            "Describe what you see on television or computer screens at home as depicted by these photos (15 points).    */\n",
            "[Same accuracy required]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_inference(model, tokenizer, image_embedding):\n",
        "    # Log projector weights before inference\n",
        "    projector = model.projector\n",
        "    weight_change = projector.get_weight_change()\n",
        "    print(f\"Projector weight change: {weight_change}\")\n",
        "    # Ensure the model is in the correct precision (e.g., float16)\n",
        "    # model = model.to(next(model.parameters()).dtype)\n",
        "\n",
        "    generator = CustomTextGenerator(model=model, tokenizer=tokenizer)\n",
        "    input_text = \"\"\"<|system|> You are a helpful assistant.<|end|><|user|> Given the following information, provide a detailed and accurate response:\n",
        "Describe the image in detail.\n",
        "<|end|><|assistant|>\n",
        "\"\"\"\n",
        "# [An image is provided for this task.]\n",
        "    generated_text = generator.generate(\n",
        "        input_text,\n",
        "        image_embedding=image_embedding,\n",
        "        max_new_tokens=150,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        temperature=0.8,\n",
        "        top_k=40,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.2,\n",
        "        no_repeat_ngram_size=3,\n",
        "    )\n",
        "    return generated_text\n",
        "\n"
      ],
      "metadata": {
        "id": "JOxaZinoCGCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before merging\n",
        "print(\"Testing inference before merging...\")\n",
        "pre_merge_output = test_inference(model, tokenizer, sample['image_embeddings'])\n",
        "print(pre_merge_output)\n"
      ],
      "metadata": {
        "id": "D0cfVKzgQIGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e5ebdb2-7256-49da-e71d-6ef106f475fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing inference before merging...\n",
            "Projector weight change: 13.009765625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a helpful assistant. Given the following information, provide a detailed and accurate response:\n",
            "Describe the image in detail.\n",
            "\n",
            "[An illustration is provided for this task.] \n",
            "Can you describe what's happening on that snowy street? There appears to be two motorcycles parked side by one another along with three cars spaced out over several lanes of traffic across from them—a total count exceeding five vehicles being present at once within close proximity each other or near roadside barriers like fences (if any). Multiple people can also been seen throughout different spots surrounding these bikes; some more distant than others while likely enjoying their time together amidst winter weather conditions outside town limits where they might have come into view during an event such as riding club meetups held nearby parks located far away down south-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### merge models and save in gdrive"
      ],
      "metadata": {
        "id": "fKrJVITzcjPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del trainer\n",
        "del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "z9NBBb8IdcfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model_with_lora_and_projector(latest_checkpoint, #hf_adapter_repo\n",
        "                                                  device, bnb_config = bnb_config, debug = False)\n",
        "# print(loaded_model.projector.layer1.weight.dtype, loaded_model.projector.layer1.bias.dtype)"
      ],
      "metadata": {
        "id": "yxhIm-Neoe1r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "c51f69187ac446a09e2f3c5783c7dc87",
            "b644d5d5eccb4c50adb56347a0af0269",
            "fbf28c6c750640c1a01d71a7fbf7e0aa",
            "7e800fc855074874865166194be4a86e",
            "df203e45a5b14f68bc5080e8eb33c03d",
            "509d0452f3ce4ca6809e11458c8ebf84",
            "8ed1480296e9488a992dc5cb61046f23",
            "dd9d913c44a64201a20f36dccd525caf",
            "3c166fc16e9641aabf7f25b4e4ca9b73",
            "a68b79b535174dfb8da1db548211f4b7",
            "ba2e697a1db34cee8965b63cfcd7e340"
          ]
        },
        "outputId": "fd1343a6-8612-4fdf-8cd5-4166434f5579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c51f69187ac446a09e2f3c5783c7dc87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading adapter weights from /content/drive/MyDrive/multimodal_llm/phi-3_5/checkpoints/checkpoint-900 led to unexpected keys not found in the model:  ['phi3.model.layers.0.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.0.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.1.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.1.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.10.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.10.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.11.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.11.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.12.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.12.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.13.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.13.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.14.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.14.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.15.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.15.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.16.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.16.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.17.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.17.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.18.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.18.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.19.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.19.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.2.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.2.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.20.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.20.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.21.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.21.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.22.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.22.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.23.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.23.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.24.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.24.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.25.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.25.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.26.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.26.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.27.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.27.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.28.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.28.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.29.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.29.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.3.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.3.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.30.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.30.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.31.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.31.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.4.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.4.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.5.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.5.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.6.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.6.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.7.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.7.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.8.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.8.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.9.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.9.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.9.self_attn.o_proj.lora_B.default.weight']. \n",
            "<ipython-input-55-e3339eabcc31>:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  projector_state_dict = torch.load(projector_path, map_location=phi3_model.device)\n",
            "<ipython-input-59-153e88ecfc4b>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lora_state_dict = torch.load(lora_weights_path, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded projector with input_dim=512, output_dim=3072\n",
            "Total keys in lora_state_dict: 192\n",
            "Loaded LoRA weights: 128 / 192\n",
            "Loaded scaling factors: 64\n",
            "Total LoRA modules processed: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_loaded_model(model, tokenizer, eval_dataset, device):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # # Print model parameter dtypes\n",
        "    # for name, param in model.named_parameters():\n",
        "    #     if 'projector' in name:\n",
        "    #         print(f\"{name} dtype: {param.dtype}\")\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    dataloader = DataLoader(eval_dataset, batch_size=2, collate_fn=custom_data_collator)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch = {\n",
        "                'input_ids': batch['input_ids'].to(device),\n",
        "                'attention_mask': batch['attention_mask'].to(device),\n",
        "                'image_embeddings': batch['image_embeddings'].to(device),#.to(torch.float16),\n",
        "                'labels': batch['labels'].to(device)\n",
        "            }\n",
        "\n",
        "            # Print the dtype of image_embeddings\n",
        "            # print(f\"image_embeddings dtype: {batch['image_embeddings'].dtype}\")\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask'],\n",
        "                image_embeddings=batch['image_embeddings'],\n",
        "                labels=batch['labels']\n",
        "            )\n",
        "            total_loss += outputs.loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "    return total_loss / num_batches if num_batches > 0 else 0\n",
        "\n",
        "print(\"Evaluating loaded model...\")\n",
        "loded_model_loss = evaluate_loaded_model(loaded_model, tokenizer, small_dataset_dict['test'], device)\n",
        "print(f\"loded model loss: {loded_model_loss}\")"
      ],
      "metadata": {
        "id": "AwvYXIwUwFp6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bb93934-122d-4b45-e4dd-c830466dad09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating loaded model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loded model loss: 0.29114556910915185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# after loading model from PEFT adapter\n",
        "print(\"Testing inference after loading model from peft adapter...\")\n",
        "peft_adapter_output = test_inference(loaded_model, tokenizer, sample['image_embeddings'])\n",
        "print(peft_adapter_output)"
      ],
      "metadata": {
        "id": "2L4BW4CV6xYo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2365067b-ce61-498a-c74c-03d61a093804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing inference after loading model from peft adapter...\n",
            "Projector weight change: 13.008190155029297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-e3339eabcc31>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a helpful assistant. Given the following information, provide a detailed and accurate response:\n",
            "Describe the image in detail.\n",
            "d \"What is this photo about?\"] \n",
            "Can you describe what's happening with these dogs on leashes outside of an airport gate area near planed jets at night time while people stand nearby them or watch as they move around outdoors among other objects like trees along railroad tracks by their side before boarding onto flights for travel purposes later that day during springtime season weather conditions where it appears to be cloudy skies overhead but not raining heavily yet when viewed from afar over three men who seem more focused towards observing dog owners walking down streets than focusing directly upon each animal themselves across several street corners located within both city blocks including one close enough just behind someone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('end')"
      ],
      "metadata": {
        "id": "CVdaMp3fMQJM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "340d85d0-42df-46d3-925a-f7a180c73596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d0HqyX8jCh4Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}