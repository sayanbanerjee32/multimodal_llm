{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1sIyQFl-RN3d3jPVutndUA2EhUaHgDza6",
      "authorship_tag": "ABX9TyNEvxK/W6T4w6f/q9NXf3Rz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7e850595fd784ad683d2e0c51d330958": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d9776781f1c4dfe9d02ec7309424c60",
              "IPY_MODEL_066d1b655fa2488cb298787eba7f9e27",
              "IPY_MODEL_76b808dceb7548aa80cf6360f70f1759"
            ],
            "layout": "IPY_MODEL_e463c1d087c2458ea93729b2e22e9d69"
          }
        },
        "2d9776781f1c4dfe9d02ec7309424c60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_476dd13b6fef4583be23a015d7e46d75",
            "placeholder": "​",
            "style": "IPY_MODEL_6a75a015597641988afb5b253dbe281a",
            "value": "Map: 100%"
          }
        },
        "066d1b655fa2488cb298787eba7f9e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3662d63288b406e8a407845e93fa84a",
            "max": 157712,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c770bdf4a2294199a063e1987e41450e",
            "value": 157712
          }
        },
        "76b808dceb7548aa80cf6360f70f1759": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8de2b7b08cb2439381ef603d849a5aa6",
            "placeholder": "​",
            "style": "IPY_MODEL_8c9a54334004479fa96bd2ecfd3a8f10",
            "value": " 157712/157712 [05:56&lt;00:00, 857.33 examples/s]"
          }
        },
        "e463c1d087c2458ea93729b2e22e9d69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "476dd13b6fef4583be23a015d7e46d75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a75a015597641988afb5b253dbe281a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3662d63288b406e8a407845e93fa84a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c770bdf4a2294199a063e1987e41450e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8de2b7b08cb2439381ef603d849a5aa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c9a54334004479fa96bd2ecfd3a8f10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b92dafb1ce0c444eb6c1f682d0a4295b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9ff90e967224116801a134613807af5",
              "IPY_MODEL_61f4db8f376c4206894bb867cc5712f5",
              "IPY_MODEL_3bfff8cbf07d4041bbc5d1c81beb5522"
            ],
            "layout": "IPY_MODEL_3b63be07cf4c4a018a45184bcdbc014b"
          }
        },
        "e9ff90e967224116801a134613807af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f34578cf11c8431b8878855d5c3d8946",
            "placeholder": "​",
            "style": "IPY_MODEL_f4fd4eec2d034b46a701305f9101b826",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "61f4db8f376c4206894bb867cc5712f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59daa0fe133f453797c15fe04328c875",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d65394db1bf4e81aacb7e65c9f0984b",
            "value": 2
          }
        },
        "3bfff8cbf07d4041bbc5d1c81beb5522": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21ba01a13c22434f9daf5c0d4719946d",
            "placeholder": "​",
            "style": "IPY_MODEL_088a45e1e1d24ae1b6a008117b4b1f80",
            "value": " 2/2 [00:44&lt;00:00, 21.18s/it]"
          }
        },
        "3b63be07cf4c4a018a45184bcdbc014b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f34578cf11c8431b8878855d5c3d8946": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4fd4eec2d034b46a701305f9101b826": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59daa0fe133f453797c15fe04328c875": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d65394db1bf4e81aacb7e65c9f0984b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21ba01a13c22434f9daf5c0d4719946d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "088a45e1e1d24ae1b6a008117b4b1f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a36c712e9234a159c39a759dc846ff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d75772fdc54f4d42be4275889f179f05",
              "IPY_MODEL_9cc7d5850fa5417ab748082d5f17fa93",
              "IPY_MODEL_570c101a3d674074ae82bdbda4e676b5"
            ],
            "layout": "IPY_MODEL_9314edf15b7e40bb8f81e09a960843c9"
          }
        },
        "d75772fdc54f4d42be4275889f179f05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c36cf1536e1c4c8585f206d125aa13ac",
            "placeholder": "​",
            "style": "IPY_MODEL_697d9b25772d421ab3f6efcdbb1b95f6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9cc7d5850fa5417ab748082d5f17fa93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bda5770bda74e989005dab8f5bcb83d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_235c5d9e633b4ea3aa41438fd75dcfdb",
            "value": 2
          }
        },
        "570c101a3d674074ae82bdbda4e676b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be13cbf4000c48b394ceccd46f8c1944",
            "placeholder": "​",
            "style": "IPY_MODEL_204615da3e0749be96571823fe398f4f",
            "value": " 2/2 [00:51&lt;00:00, 24.12s/it]"
          }
        },
        "9314edf15b7e40bb8f81e09a960843c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c36cf1536e1c4c8585f206d125aa13ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "697d9b25772d421ab3f6efcdbb1b95f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bda5770bda74e989005dab8f5bcb83d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "235c5d9e633b4ea3aa41438fd75dcfdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be13cbf4000c48b394ceccd46f8c1944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "204615da3e0749be96571823fe398f4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayanbanerjee32/multimodal_llm/blob/main/phi_3_QLoRA_instruct150k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.44.2\n",
        "!pip install -Uq accelerate peft bitsandbytes trl dataset bitsandbytes\n",
        "# !pip install -Uq flash_attn"
      ],
      "metadata": {
        "id": "GOg3sZg0IuZf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34de5852-fc99-4303-aa8c-b3cf8fd2076f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.44.2 in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.44.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.44.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.44.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.44.2) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import os, gc\n",
        "import subprocess\n",
        "import json\n",
        "import random\n",
        "### Download Phi-3 model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, PreTrainedModel\n",
        "from transformers.trainer_callback import TrainerCallback\n",
        "from datasets import Dataset, DatasetDict\n",
        "import joblib\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "ju3h7A8AtfmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzTf0O7myIO-",
        "outputId": "369affa3-0b19-4e40-a445-21f3dc2db51b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Phi-3 model"
      ],
      "metadata": {
        "id": "YsXHaUKZM1ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Phi-3 model and tokenizer\n",
        "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"right\",\n",
        "                                           trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "RrirRzvsuYaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downlaod image embedding"
      ],
      "metadata": {
        "id": "uDm2WxI-N5bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q2agwLMDpJC",
        "outputId": "73e8a863-8a56-4999-eb1b-0ac77fb2ff7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QyH6rCfDXRN",
        "outputId": "543adfb0-e67e-4d37-eed7-e0dbb2c4936b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embeddings...\n",
            "Image: 000000401144.jpg\n",
            "Embedding shape: (512,)\n",
            "Embedding preview: [-0.13     0.1564   0.02017  0.1678   0.2393 ]...\n",
            "--------------------------------------------------\n",
            "Total number of embeddings: 81479\n"
          ]
        }
      ],
      "source": [
        "# URL of the embeddings file (replace with your actual URL)\n",
        "embeddings_url = '/content/drive/MyDrive/multimodel_llm/image_embedding/coco_image_embeddings.npz'\n",
        "\n",
        "# Load the embeddings\n",
        "print(\"Loading embeddings...\")\n",
        "embeddings = np.load(embeddings_url, allow_pickle=True)\n",
        "\n",
        "# Print embeddings and image names\n",
        "for image_name, embedding in embeddings.items():\n",
        "    print(f\"Image: {image_name}\")\n",
        "    print(f\"Embedding shape: {embedding.shape}\")\n",
        "    print(f\"Embedding preview: {embedding[:5]}...\")  # Print first 5 values\n",
        "    print(\"-\" * 50)\n",
        "    break\n",
        "\n",
        "print(f\"Total number of embeddings: {len(embeddings)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data processing"
      ],
      "metadata": {
        "id": "NwCmU3H-NABj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of URLs to download\n",
        "url = \"https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_150k.json\"\n",
        "\n",
        "# Download each file\n",
        "subprocess.run([\"wget\", \"-c\", url])\n",
        "\n",
        "# Load the downloaded JSON file\n",
        "json_file = \"llava_instruct_150k.json\"\n",
        "with open(json_file, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Function to convert conversation format\n",
        "# def convert_conversation(conversation):\n",
        "#     system_message = \"<|system|>\\nYou are a helpful assistant.<|end|>\\n\"\n",
        "#     user_message = \"\"\n",
        "#     assistant_message = \"\"\n",
        "\n",
        "#     for item in conversation:\n",
        "#         if item['from'] == 'human':\n",
        "#             user_message = f\"<|user|>\\n{item['value']}<|end|>\\n\"\n",
        "#         elif item['from'] == 'gpt':\n",
        "#             assistant_message = f\"<|assistant|>\\n{item['value']}<|end|>\\n\"\n",
        "\n",
        "#     return system_message + user_message + assistant_message\n",
        "\n",
        "# Process and tokenize the data\n",
        "# Process and tokenize the data\n",
        "\n",
        "\n",
        "# tokenized_data = []\n",
        "# for item in tqdm(data, desc=\"Tokenizing data\"):\n",
        "#     image_file = item['image']\n",
        "#     if image_file in embeddings:\n",
        "#         image_embedding = torch.tensor(embeddings[image_file], dtype=torch.float32, device=device)\n",
        "\n",
        "#         # Use the existing conversation format\n",
        "#         conversation = [\n",
        "#             {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
        "#         ] + [{\"role\": \"user\" if msg['from'] == 'human' else \"assistant\", \"content\": msg['value']}\n",
        "#              for msg in item['conversations']]\n",
        "\n",
        "#         # Apply chat template and tokenize directly\n",
        "#         tokenized_conversation = tokenizer.apply_chat_template(conversation, return_tensors='pt').to(device)\n",
        "\n",
        "#         tokenized_item = {\n",
        "#             'image': image_file,\n",
        "#             'image_embedding': image_embedding,\n",
        "#             'tokenized_conversation': tokenized_conversation\n",
        "#         }\n",
        "#         tokenized_data.append(tokenized_item)\n",
        "\n",
        "# print(f\"Total tokenized items: {len(tokenized_data)}\")\n",
        "# print(f\"Sample tokenized item:\")\n",
        "# print(f\"Image: {tokenized_data[0]['image']}\")\n",
        "# print(f\"Image embedding shape: {tokenized_data[0]['image_embedding'].shape}\")\n",
        "# print(f\"Tokenized conversation shape: {tokenized_data[0]['tokenized_conversation'].shape}\")\n",
        "# print(f\"Image embedding device: {tokenized_data[0]['image_embedding'].device}\")\n",
        "# print(f\"Tokenized conversation device: {tokenized_data[0]['tokenized_conversation'].device}\")"
      ],
      "metadata": {
        "id": "homAn3isNCOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data = data[0:100]"
      ],
      "metadata": {
        "id": "-l600Sm5nOEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset():\n",
        "    processed_data = []\n",
        "    print(\"Processing data...\")\n",
        "    with tqdm(total=len(data)) as pbar:\n",
        "        for item in data:\n",
        "            image_file = item['image']\n",
        "            if image_file in embeddings:\n",
        "                processed_data.append({\n",
        "                    'image': image_file,\n",
        "                    'image_embedding': embeddings[image_file].tolist(),\n",
        "                    'conversation': item['conversations']\n",
        "                })\n",
        "            pbar.update(1)\n",
        "\n",
        "    print(f\"Data processing completed. Total processed items: {len(processed_data)}\")\n",
        "\n",
        "    return Dataset.from_dict({\n",
        "        \"image\": [item['image'] for item in processed_data],\n",
        "        \"image_embedding\": [item['image_embedding'] for item in processed_data],\n",
        "        \"conversation\": [item['conversation'] for item in processed_data]\n",
        "    })\n",
        "\n",
        "print(\"Creating HuggingFace dataset...\")\n",
        "hf_dataset = create_dataset()\n",
        "\n",
        "print(\"HuggingFace dataset creation completed.\")\n",
        "print(f\"Total samples in dataset: {len(hf_dataset)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OZtei2WxHlQ",
        "outputId": "8683f85a-9ddb-4d84-f115-a94861db9643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating HuggingFace dataset...\n",
            "Processing data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157712/157712 [12:31<00:00, 209.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data processing completed. Total processed items: 157712\n",
            "HuggingFace dataset creation completed.\n",
            "Total samples in dataset: 157712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Applying tokenization and preparing the dataset...\")\n",
        "\n",
        "# def prepare_dataset(examples):\n",
        "#     image_embeddings = torch.stack([torch.tensor(item) for item in examples['image_embedding']])\n",
        "\n",
        "#     conversations = []\n",
        "#     for conv in examples['conversation']:\n",
        "#         dialogue = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "\n",
        "#         for i, message in enumerate(conv):\n",
        "#             if message['from'] == 'human':\n",
        "#                 content = message['value'].replace('<image>', '').strip()  # Remove '<image>' and strip whitespace\n",
        "#                 if i == 0:\n",
        "#                     content = f\"Given the following information, provide a detailed and accurate response:\\n{content}\\n[An image is provided for this task.]\\n\"\n",
        "#                 dialogue.append({\"role\": \"user\", \"content\": content})\n",
        "#             elif message['from'] == 'gpt':\n",
        "#                 dialogue.append({\"role\": \"assistant\", \"content\": message['value']})\n",
        "\n",
        "#         conversations.append(dialogue)\n",
        "\n",
        "#     tokenized_conversations = tokenizer.apply_chat_template(conversations,\n",
        "#                                                             return_tensors='pt', padding=True)\n",
        "\n",
        "#     return {\n",
        "#         \"image_embeddings\": image_embeddings,\n",
        "#         \"input_ids\": tokenized_conversations,\n",
        "#         \"attention_mask\": torch.ones_like(tokenized_conversations),\n",
        "#         \"labels\": tokenized_conversations.clone()\n",
        "#     }\n",
        "\n",
        "def prepare_dataset(examples):\n",
        "    image_embeddings = []\n",
        "    conversations = []\n",
        "\n",
        "    for idx, conv in enumerate(examples['conversation']):\n",
        "        image_embedding = torch.tensor(examples['image_embedding'][idx])\n",
        "        dialogue_pairs = []\n",
        "\n",
        "        for i in range(0, len(conv), 2):\n",
        "            if i + 1 < len(conv):  # Ensure we have a pair\n",
        "                human_msg = conv[i]['value'].replace('<image>', '').strip()\n",
        "                gpt_msg = conv[i + 1]['value']\n",
        "\n",
        "                dialogue = [\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Given the following information, provide a detailed and accurate response:\\n{human_msg}\\n[An image is provided for this task.]\\n\"},\n",
        "                    {\"role\": \"assistant\", \"content\": gpt_msg}\n",
        "                ]\n",
        "\n",
        "                dialogue_pairs.append(dialogue)\n",
        "                image_embeddings.append(image_embedding)\n",
        "\n",
        "        conversations.extend(dialogue_pairs)\n",
        "\n",
        "    image_embeddings = torch.stack(image_embeddings)\n",
        "\n",
        "    tokenized_conversations = tokenizer.apply_chat_template(conversations,\n",
        "                                                            return_tensors='pt', padding=True)\n",
        "\n",
        "    return {\n",
        "        \"image_embeddings\": image_embeddings,\n",
        "        \"input_ids\": tokenized_conversations,\n",
        "        \"attention_mask\": torch.ones_like(tokenized_conversations),\n",
        "        \"labels\": tokenized_conversations.clone()\n",
        "    }\n",
        "\n",
        "# Test the prepare_dataset function with a real training example\n",
        "def test_prepare_dataset():\n",
        "    # Get a batch of examples from the dataset\n",
        "    batch_size = 1  # You can adjust this as needed\n",
        "    sample_batch = hf_dataset[5:5+batch_size]\n",
        "\n",
        "    print(\"Original conversations:\")\n",
        "    # for i, sample in enumerate(sample_batch):\n",
        "    #     print(f\"\\nSample {i + 1}:\")\n",
        "    for message in sample_batch['conversation'][0]:\n",
        "        print(f\"{message['from']}: {message['value']}\")\n",
        "\n",
        "    # Process the sample batch\n",
        "    result = prepare_dataset(sample_batch)\n",
        "\n",
        "    # Print the structure of the result\n",
        "    print(\"\\nResult keys:\", result.keys())\n",
        "    print(\"Image embeddings shape:\", result['image_embeddings'].shape)\n",
        "    print(\"Input IDs shape:\", result['input_ids'].shape)\n",
        "    print(\"Attention mask shape:\", result['attention_mask'].shape)\n",
        "    print(\"Labels shape:\", result['labels'].shape)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        decoded_input = tokenizer.decode(result['input_ids'][i])\n",
        "        decoded_labels = tokenizer.decode(result['labels'][i])\n",
        "\n",
        "        print(f\"\\nRestructured input for sample {i + 1}:\")\n",
        "        print(decoded_input)\n",
        "\n",
        "        print(f\"\\nLabels for sample {i + 1}:\")\n",
        "        print(decoded_labels)\n",
        "\n",
        "        # Optionally, you can print a more readable version of the labels\n",
        "        print(\"\\nReadable labels (non-padding tokens):\")\n",
        "        readable_labels = tokenizer.decode([token for token in result['labels'][i] if token != -100])\n",
        "        print(readable_labels)\n",
        "\n",
        "    # Optionally, you can print attention mask to see where it's applied\n",
        "    print(\"\\nAttention Mask:\")\n",
        "    print(result['attention_mask'][0])\n",
        "# Run the test\n",
        "test_prepare_dataset()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnMLoUn32aLf",
        "outputId": "fd6b3ab5-c9f7-4c87-dd43-406012837100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original conversations:\n",
            "human: <image>\n",
            "What is the girl eating in the image?\n",
            "gpt: The girl in the image is eating a dessert, which appears to be a graham cracker treat or a cookie sandwich.\n",
            "human: Describe the girl's hair color and clothing.\n",
            "gpt: The girl has blonde hair, and she is wearing a pink shirt.\n",
            "human: What color is the plate that the dessert is on?\n",
            "gpt: The dessert is on a green plate.\n",
            "human: Is the girl looking at the camera or focusing on her dessert?\n",
            "gpt: The girl is looking up at the camera while taking a bite of her dessert.\n",
            "human: Where is the girl eating her dessert?\n",
            "gpt: The girl is eating her dessert at the table.\n",
            "\n",
            "Result keys: dict_keys(['image_embeddings', 'input_ids', 'attention_mask', 'labels'])\n",
            "Image embeddings shape: torch.Size([5, 512])\n",
            "Input IDs shape: torch.Size([5, 75])\n",
            "Attention mask shape: torch.Size([5, 75])\n",
            "Labels shape: torch.Size([5, 75])\n",
            "\n",
            "Restructured input for sample 1:\n",
            "<|system|> You are a helpful assistant.<|end|><|user|> Given the following information, provide a detailed and accurate response:\n",
            "What is the girl eating in the image?\n",
            "[An image is provided for this task.]\n",
            "<|end|><|assistant|> The girl in the image is eating a dessert, which appears to be a graham cracker treat or a cookie sandwich.<|end|><|endoftext|>\n",
            "\n",
            "Labels for sample 1:\n",
            "<|system|> You are a helpful assistant.<|end|><|user|> Given the following information, provide a detailed and accurate response:\n",
            "What is the girl eating in the image?\n",
            "[An image is provided for this task.]\n",
            "<|end|><|assistant|> The girl in the image is eating a dessert, which appears to be a graham cracker treat or a cookie sandwich.<|end|><|endoftext|>\n",
            "\n",
            "Readable labels (non-padding tokens):\n",
            "<|system|> You are a helpful assistant.<|end|><|user|> Given the following information, provide a detailed and accurate response:\n",
            "What is the girl eating in the image?\n",
            "[An image is provided for this task.]\n",
            "<|end|><|assistant|> The girl in the image is eating a dessert, which appears to be a graham cracker treat or a cookie sandwich.<|end|><|endoftext|>\n",
            "\n",
            "Attention Mask:\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply tokenization and prepare the dataset\n",
        "print(\"Applying tokenization and preparing the dataset...\")\n",
        "\n",
        "\n",
        "# def prepare_dataset(examples):\n",
        "#     image_embeddings = torch.stack([torch.tensor(item) for item in examples['image_embedding']])\n",
        "\n",
        "#     conversations = [\n",
        "#         [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}] +\n",
        "#         [{\"role\": \"user\" if msg['from'] == 'human' else \"assistant\", \"content\": msg['value']}\n",
        "#          for msg in conv]\n",
        "#         for conv in examples['conversation']\n",
        "#     ]\n",
        "\n",
        "#     tokenized_conversations = tokenizer.apply_chat_template(conversations,\n",
        "#                                                              return_tensors='pt', padding=True)\n",
        "\n",
        "#     return {\n",
        "#         \"image_embeddings\": image_embeddings,\n",
        "#         \"input_ids\": tokenized_conversations,\n",
        "#         \"attention_mask\": torch.ones_like(tokenized_conversations),\n",
        "#         \"labels\": tokenized_conversations.clone()\n",
        "#     }\n",
        "\n",
        "\n",
        "hf_dataset_mapped = hf_dataset.map(\n",
        "    prepare_dataset,\n",
        "    batched=True,\n",
        "    remove_columns=hf_dataset.column_names,\n",
        "    batch_size=1024  # Adjust based on your memory constraints\n",
        ").with_format(\"torch\")\n",
        "\n",
        "# Split the dataset\n",
        "train_test_split = hf_dataset_mapped.train_test_split(test_size=0.05)\n",
        "\n",
        "# Create a DatasetDict\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': train_test_split['train'],\n",
        "    'test': train_test_split['test']\n",
        "})\n",
        "\n",
        "print(f\"Train dataset size: {len(dataset_dict['train'])}\")\n",
        "print(f\"Test dataset size: {len(dataset_dict['test'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "7e850595fd784ad683d2e0c51d330958",
            "2d9776781f1c4dfe9d02ec7309424c60",
            "066d1b655fa2488cb298787eba7f9e27",
            "76b808dceb7548aa80cf6360f70f1759",
            "e463c1d087c2458ea93729b2e22e9d69",
            "476dd13b6fef4583be23a015d7e46d75",
            "6a75a015597641988afb5b253dbe281a",
            "e3662d63288b406e8a407845e93fa84a",
            "c770bdf4a2294199a063e1987e41450e",
            "8de2b7b08cb2439381ef603d849a5aa6",
            "8c9a54334004479fa96bd2ecfd3a8f10"
          ]
        },
        "id": "lbI3OPGyN3Br",
        "outputId": "c845b11b-735c-49c7-d5e2-543e4c943ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying tokenization and preparing the dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/157712 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e850595fd784ad683d2e0c51d330958"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 325269\n",
            "Test dataset size: 36142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of accessing an item:\n",
        "sample = dataset_dict['train'][0]\n",
        "print(f\"Input IDs shape: {len(sample['input_ids'])}\")\n",
        "print(f\"Attention mask shape: {len(sample['attention_mask'])}\")\n",
        "print(f\"Labels shape: {len(sample['labels'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ky0l7yUc7avS",
        "outputId": "9b6b1d94-1cce-4fbb-f449-0ad76109c6f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs shape: 754\n",
            "Attention mask shape: 754\n",
            "Labels shape: 754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Projection Layer"
      ],
      "metadata": {
        "id": "inHiIuFzMxq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Projection Layer\n",
        "# class SimpleResBlock(nn.Module):\n",
        "#     def __init__(self, input_dim, output_dim):\n",
        "#         super().__init__()\n",
        "#         self.pre_norm = nn.LayerNorm(input_dim)\n",
        "#         self.proj = nn.Sequential(\n",
        "#             nn.Linear(input_dim, output_dim),\n",
        "#             nn.GELU(),\n",
        "#             nn.Linear(output_dim, output_dim)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.pre_norm(x)\n",
        "#         return x + self.proj(x)\n",
        "\n",
        "# class ImageProjector(nn.Module):\n",
        "#     def __init__(self, input_dim, output_dim):\n",
        "#         super().__init__()\n",
        "#         self.proj = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.proj(x)"
      ],
      "metadata": {
        "id": "qr8kfaceDfwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torch.utils.data import Dataset, random_split\n",
        "\n",
        "# class Phi3Dataset(Dataset):\n",
        "#     def __init__(self, tokenized_data, projector, tokenizer):\n",
        "#         self.data = tokenized_data\n",
        "#         self.projector = projector\n",
        "#         self.tokenizer = tokenizer\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         item = self.data[idx]\n",
        "#         image_embedding = item['image_embedding']\n",
        "#         conversation = item['tokenized_conversation']\n",
        "\n",
        "#         projected_image = self.projector(image_embedding.unsqueeze(0)).squeeze(0)\n",
        "\n",
        "#         # Combine projected image and conversation\n",
        "#         combined_input = torch.cat([projected_image, conversation.squeeze(0)])\n",
        "\n",
        "#         # Create attention mask\n",
        "#         attention_mask = torch.ones_like(combined_input)\n",
        "\n",
        "#         # Prepare labels (shift right, set first token to -100)\n",
        "#         labels = torch.cat([-100 * torch.ones(projected_image.shape[0], dtype=torch.long), conversation.squeeze(0)[:-1]])\n",
        "\n",
        "#         return {\n",
        "#             \"input_ids\": combined_input,\n",
        "#             \"attention_mask\": attention_mask,\n",
        "#             \"labels\": labels\n",
        "#         }\n",
        "\n",
        "# # Usage:\n",
        "# image_embedding_dim = tokenized_data[0]['image_embedding'].shape[-1]\n",
        "# projection_dim = 1024  # Adjust as needed\n",
        "# projector = SimpleResBlock(image_embedding_dim, projection_dim)\n",
        "# full_dataset = Phi3Dataset(tokenized_data, projector, tokenizer)\n",
        "\n",
        "# # Split the dataset\n",
        "# train_size = int(0.9 * len(full_dataset))\n",
        "# test_size = len(full_dataset) - train_size\n",
        "# train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "# print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "# print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "# # Example of accessing an item:\n",
        "# sample = train_dataset[0]\n",
        "# print(f\"Input IDs shape: {sample['input_ids'].shape}\")\n",
        "# print(f\"Attention mask shape: {sample['attention_mask'].shape}\")\n",
        "# print(f\"Labels shape: {sample['labels'].shape}\")\n"
      ],
      "metadata": {
        "id": "MBLPxlTiM6OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QLoRA set up"
      ],
      "metadata": {
        "id": "Sb90y2RNM6ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new_model = \"ms-phi3-custom\"\n",
        "lora_r = 32 #64\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.05\n",
        "use_4bit = True\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "use_nested_quant = False\n",
        "output_dir = \"./results\"\n",
        "num_train_epochs = 1\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "per_device_train_batch_size = 8\n",
        "per_device_eval_batch_size = 4\n",
        "gradient_accumulation_steps = 8\n",
        "gradient_checkpointing = True\n",
        "max_grad_norm = 0.3\n",
        "learning_rate = 5e-4\n",
        "weight_decay = 0.001\n",
        "optim = \"paged_adamw_32bit\"\n",
        "lr_scheduler_type = \"constant\"\n",
        "max_steps = -1\n",
        "warmup_ratio = 0.03\n",
        "group_by_length = True\n",
        "save_steps = 25\n",
        "logging_steps = 25\n",
        "eval_steps = 50 # Evaluate every 25 steps\n",
        "max_seq_length = 256\n",
        "packing = False\n",
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "Mx6UI-r1M-0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import PreTrainedModel\n",
        "\n",
        "# class Phi3WithProjector(PreTrainedModel):\n",
        "#     supports_gradient_checkpointing = True\n",
        "\n",
        "#     def __init__(self, phi3_model, projector):\n",
        "#         super().__init__(phi3_model.config)\n",
        "#         self.phi3 = phi3_model\n",
        "#         self.projector = projector\n",
        "\n",
        "#     def forward(self, input_ids=None, attention_mask=None, image_embeddings=None, labels=None, **kwargs):\n",
        "#         device = next(self.parameters()).device\n",
        "\n",
        "#         if image_embeddings is not None:\n",
        "#             image_embeddings = image_embeddings.to(device)\n",
        "#             projected_images = self.projector(image_embeddings)\n",
        "#             projected_images = projected_images.unsqueeze(1)\n",
        "\n",
        "#             if 'inputs_embeds' in kwargs and kwargs['inputs_embeds'] is not None:\n",
        "#                 inputs_embeds = kwargs['inputs_embeds']\n",
        "#                 inputs_embeds = torch.cat([projected_images, inputs_embeds], dim=1)\n",
        "#                 kwargs['inputs_embeds'] = inputs_embeds\n",
        "#             elif input_ids is not None:\n",
        "#                 inputs_embeds = self.get_input_embeddings()(input_ids.to(device))\n",
        "#                 inputs_embeds = torch.cat([projected_images, inputs_embeds], dim=1)\n",
        "#                 kwargs['inputs_embeds'] = inputs_embeds\n",
        "#                 input_ids = None  # Set to None to avoid conflict\n",
        "\n",
        "#             if attention_mask is not None:\n",
        "#                 attention_mask = torch.cat([torch.ones(image_embeddings.size(0), 1, device=device), attention_mask.to(device)], dim=1)\n",
        "#             else:\n",
        "#                 attention_mask = torch.ones(image_embeddings.size(0), inputs_embeds.size(1), device=device)\n",
        "\n",
        "#             if labels is not None:\n",
        "#                 # Adjust labels to match the new sequence length\n",
        "#                 labels = torch.cat([torch.full((labels.size(0), 1), -100, device=device), labels], dim=1)\n",
        "\n",
        "#         if labels is not None:\n",
        "#             labels = labels.to(device)\n",
        "\n",
        "#         # Ensure attention_mask matches the sequence length\n",
        "#         if 'inputs_embeds' in kwargs:\n",
        "#             seq_length = kwargs['inputs_embeds'].size(1)\n",
        "#         elif input_ids is not None:\n",
        "#             seq_length = input_ids.size(1)\n",
        "#         else:\n",
        "#             raise ValueError(\"Either input_ids or inputs_embeds should be provided\")\n",
        "\n",
        "#         if attention_mask is not None and attention_mask.size(1) != seq_length:\n",
        "#             attention_mask = attention_mask[:, :seq_length]\n",
        "\n",
        "#         return self.phi3(input_ids=input_ids, attention_mask=attention_mask, labels=labels, **kwargs)\n",
        "\n",
        "#     def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **kwargs):\n",
        "#         inputs = self.phi3.prepare_inputs_for_generation(input_ids, past=past, attention_mask=attention_mask, **kwargs)\n",
        "#         if 'image_embeddings' in kwargs:\n",
        "#             inputs['image_embeddings'] = kwargs['image_embeddings']\n",
        "\n",
        "#             # Adjust attention_mask if it's present\n",
        "#             if attention_mask is not None:\n",
        "#                 inputs['attention_mask'] = torch.cat([torch.ones(attention_mask.size(0), 1, device=attention_mask.device), attention_mask], dim=1)\n",
        "\n",
        "#             # Remove position_ids as they're not used\n",
        "#             inputs.pop('position_ids', None)\n",
        "\n",
        "#         return inputs\n",
        "\n",
        "#     def get_input_embeddings(self):\n",
        "#         return self.phi3.get_input_embeddings()\n",
        "\n",
        "#     def set_input_embeddings(self, value):\n",
        "#         self.phi3.set_input_embeddings(value)\n",
        "\n",
        "#     def gradient_checkpointing_enable(self, **kwargs):\n",
        "#         self.phi3.gradient_checkpointing_enable(**kwargs)\n",
        "\n",
        "#     def gradient_checkpointing_disable(self):\n",
        "#         self.phi3.gradient_checkpointing_disable()\n",
        "\n",
        "#     def __getattr__(self, name):\n",
        "#         try:\n",
        "#             return super().__getattr__(name)\n",
        "#         except AttributeError:\n",
        "#             return getattr(self.phi3, name)\n",
        "\n",
        "#     def generate(self, input_ids=None, attention_mask=None, image_embeddings=None, **kwargs):\n",
        "#         if image_embeddings is not None:\n",
        "#             kwargs['image_embeddings'] = image_embeddings\n",
        "\n",
        "#         if attention_mask is not None and image_embeddings is not None:\n",
        "#             # Add an extra attention mask token for the image embedding\n",
        "#             attention_mask = torch.cat([torch.ones(attention_mask.size(0), 1, device=attention_mask.device), attention_mask], dim=1)\n",
        "\n",
        "#         return super().generate(input_ids=input_ids, attention_mask=attention_mask, **kwargs)"
      ],
      "metadata": {
        "id": "EzZ3uJIZNJtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # import os\n",
        "# # import torch\n",
        "# from transformers import PreTrainedModel\n",
        "\n",
        "# class ImageProjector(torch.nn.Module):\n",
        "#     def __init__(self, input_dim, output_dim):\n",
        "#         super().__init__()\n",
        "#         self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.linear(x)\n",
        "\n",
        "# class Phi3WithProjector(PreTrainedModel):\n",
        "#     supports_gradient_checkpointing = True\n",
        "\n",
        "#     def __init__(self, phi3_model, projector):\n",
        "#         super().__init__(phi3_model.config)\n",
        "#         self.phi3 = phi3_model\n",
        "#         self.projector = projector\n",
        "\n",
        "#     @classmethod\n",
        "#     def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
        "#         # Load the base Phi-3 model\n",
        "#         phi3_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n",
        "\n",
        "#         # Load the projector weights\n",
        "#         projector_path = os.path.join(pretrained_model_name_or_path, \"image_projector.pth\")\n",
        "#         if os.path.exists(projector_path):\n",
        "#             projector_state_dict = torch.load(projector_path, map_location=phi3_model.device)\n",
        "\n",
        "#             # Check if the state dict has the expected structure\n",
        "#             if 'linear.weight' in projector_state_dict:\n",
        "#                 input_dim = projector_state_dict['linear.weight'].size(1)\n",
        "#                 output_dim = projector_state_dict['linear.weight'].size(0)\n",
        "#             else:\n",
        "#                 # If not, try to infer dimensions from the first layer's weight\n",
        "#                 first_key = next(iter(projector_state_dict))\n",
        "#                 input_dim = projector_state_dict[first_key].size(1)\n",
        "#                 output_dim = phi3_model.config.hidden_size  # Assuming this is the correct output dimension\n",
        "\n",
        "#             projector = ImageProjector(input_dim, output_dim)\n",
        "\n",
        "#             # Try to load the state dict, ignoring mismatched keys\n",
        "#             projector.load_state_dict(projector_state_dict, strict=False)\n",
        "#             print(f\"Loaded projector with input_dim={input_dim}, output_dim={output_dim}\")\n",
        "#         else:\n",
        "#             print(f\"Projector weights not found at {projector_path}. Initializing with default dimensions.\")\n",
        "#             input_dim = 512  # Default CLIP embedding size\n",
        "#             output_dim = phi3_model.config.hidden_size\n",
        "#             projector = ImageProjector(input_dim, output_dim)\n",
        "\n",
        "#         # Create and return the Phi3WithProjector instance\n",
        "#         model = cls(phi3_model, projector)\n",
        "#         return model\n",
        "\n",
        "#     def save_pretrained(self, save_directory):\n",
        "#         # Save the base model\n",
        "#         self.phi3.save_pretrained(save_directory)\n",
        "\n",
        "#         # Save the projector weights\n",
        "#         projector_path = os.path.join(save_directory, \"image_projector.pth\")\n",
        "#         torch.save(self.projector.state_dict(), projector_path)\n",
        "\n",
        "#         # Save the config\n",
        "#         self.config.save_pretrained(save_directory)\n",
        "\n",
        "#     def forward(self, input_ids=None, attention_mask=None, image_embeddings=None, labels=None, **kwargs):\n",
        "#         device = next(self.parameters()).device\n",
        "\n",
        "#         if image_embeddings is not None:\n",
        "#             image_embeddings = image_embeddings.to(device)\n",
        "#             projected_images = self.projector(image_embeddings)\n",
        "#             projected_images = projected_images.unsqueeze(1)\n",
        "\n",
        "#             if 'inputs_embeds' in kwargs and kwargs['inputs_embeds'] is not None:\n",
        "#                 inputs_embeds = kwargs['inputs_embeds']\n",
        "#                 inputs_embeds = torch.cat([projected_images, inputs_embeds], dim=1)\n",
        "#                 kwargs['inputs_embeds'] = inputs_embeds\n",
        "#             elif input_ids is not None:\n",
        "#                 inputs_embeds = self.get_input_embeddings()(input_ids.to(device))\n",
        "#                 inputs_embeds = torch.cat([projected_images, inputs_embeds], dim=1)\n",
        "#                 kwargs['inputs_embeds'] = inputs_embeds\n",
        "#                 input_ids = None  # Set to None to avoid conflict\n",
        "\n",
        "#             if attention_mask is not None:\n",
        "#                 attention_mask = torch.cat([torch.ones(image_embeddings.size(0), 1, device=device), attention_mask.to(device)], dim=1)\n",
        "#             else:\n",
        "#                 attention_mask = torch.ones(image_embeddings.size(0), inputs_embeds.size(1), device=device)\n",
        "\n",
        "#             if labels is not None:\n",
        "#                 # Adjust labels to match the new sequence length\n",
        "#                 labels = torch.cat([torch.full((labels.size(0), 1), -100, device=device), labels], dim=1)\n",
        "\n",
        "#         if labels is not None:\n",
        "#             labels = labels.to(device)\n",
        "\n",
        "#         # Ensure attention_mask matches the sequence length\n",
        "#         if 'inputs_embeds' in kwargs:\n",
        "#             seq_length = kwargs['inputs_embeds'].size(1)\n",
        "#         elif input_ids is not None:\n",
        "#             seq_length = input_ids.size(1)\n",
        "#         else:\n",
        "#             raise ValueError(\"Either input_ids or inputs_embeds should be provided\")\n",
        "\n",
        "#         if attention_mask is not None and attention_mask.size(1) != seq_length:\n",
        "#             attention_mask = attention_mask[:, :seq_length]\n",
        "\n",
        "#         return self.phi3(input_ids=input_ids, attention_mask=attention_mask, labels=labels, **kwargs)\n",
        "\n",
        "#     def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **kwargs):\n",
        "#         inputs = self.phi3.prepare_inputs_for_generation(input_ids, past=past, attention_mask=attention_mask, **kwargs)\n",
        "#         if 'image_embeddings' in kwargs:\n",
        "#             inputs['image_embeddings'] = kwargs['image_embeddings']\n",
        "\n",
        "#             # Adjust attention_mask if it's present\n",
        "#             if attention_mask is not None:\n",
        "#                 inputs['attention_mask'] = torch.cat([torch.ones(attention_mask.size(0), 1, device=attention_mask.device), attention_mask], dim=1)\n",
        "\n",
        "#             # Remove position_ids as they're not used\n",
        "#             inputs.pop('position_ids', None)\n",
        "\n",
        "#         return inputs\n",
        "\n",
        "#     def get_input_embeddings(self):\n",
        "#         return self.phi3.get_input_embeddings()\n",
        "\n",
        "#     def set_input_embeddings(self, value):\n",
        "#         self.phi3.set_input_embeddings(value)\n",
        "\n",
        "#     def gradient_checkpointing_enable(self, **kwargs):\n",
        "#         self.phi3.gradient_checkpointing_enable(**kwargs)\n",
        "\n",
        "#     def gradient_checkpointing_disable(self):\n",
        "#         self.phi3.gradient_checkpointing_disable()\n",
        "\n",
        "#     def __getattr__(self, name):\n",
        "#         try:\n",
        "#             return super().__getattr__(name)\n",
        "#         except AttributeError:\n",
        "#             return getattr(self.phi3, name)\n",
        "\n",
        "#     def generate(self, input_ids=None, attention_mask=None, image_embeddings=None, **kwargs):\n",
        "#         if image_embeddings is not None:\n",
        "#             kwargs['image_embeddings'] = image_embeddings\n",
        "\n",
        "#         if attention_mask is not None and image_embeddings is not None:\n",
        "#             # Add an extra attention mask token for the image embedding\n",
        "#             attention_mask = torch.cat([torch.ones(attention_mask.size(0), 1, device=attention_mask.device), attention_mask], dim=1)\n",
        "\n",
        "#         return super().generate(input_ids=input_ids, attention_mask=attention_mask, **kwargs)"
      ],
      "metadata": {
        "id": "ejLw1cn_ImOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageProjector(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim=1024):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.activation = nn.GELU()  # Using GELU activation, but you can experiment with others\n",
        "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.05)  # Adding dropout for regularization\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "class Phi3WithProjector(PreTrainedModel):\n",
        "    supports_gradient_checkpointing = True\n",
        "\n",
        "    def __init__(self, phi3_model, projector, debug=False):\n",
        "        super().__init__(phi3_model.config)\n",
        "        self.phi3 = phi3_model\n",
        "        self.projector = projector\n",
        "        self.debug = debug\n",
        "\n",
        "    def debug_print(self, *args, **kwargs):\n",
        "        if self.debug:\n",
        "            print(*args, **kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, debug=False, **kwargs):\n",
        "        # Load the base Phi-3 model\n",
        "        phi3_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n",
        "\n",
        "        # Determine if it's a local path or a Hugging Face model ID\n",
        "        is_local = os.path.isdir(pretrained_model_name_or_path)\n",
        "\n",
        "        if is_local:\n",
        "            projector_path = os.path.join(pretrained_model_name_or_path, \"image_projector.pth\")\n",
        "        else:\n",
        "            try:\n",
        "                # Try to download the projector weights from the Hugging Face Hub\n",
        "                projector_path = hf_hub_download(repo_id=pretrained_model_name_or_path, filename=\"image_projector.pth\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to download projector weights: {e}\")\n",
        "                projector_path = None\n",
        "\n",
        "        if projector_path and os.path.exists(projector_path):\n",
        "            projector_state_dict = torch.load(projector_path, map_location=phi3_model.device)\n",
        "            # Check if the state dict has the expected structure\n",
        "            if 'linear.weight' in projector_state_dict:\n",
        "                input_dim = projector_state_dict['linear.weight'].size(1)\n",
        "                output_dim = projector_state_dict['linear.weight'].size(0)\n",
        "            else:\n",
        "                # If not, try to infer dimensions from the first layer's weight\n",
        "                first_key = next(iter(projector_state_dict))\n",
        "                input_dim = projector_state_dict[first_key].size(1)\n",
        "                output_dim = phi3_model.config.hidden_size  # Assuming this is the correct output dimension\n",
        "\n",
        "            projector = ImageProjector(input_dim, output_dim)\n",
        "\n",
        "            # Try to load the state dict, ignoring mismatched keys\n",
        "            projector.load_state_dict(projector_state_dict, strict=False)\n",
        "            print(f\"Loaded projector with input_dim={input_dim}, output_dim={output_dim}\")\n",
        "        else:\n",
        "            print(f\"Projector weights not found. Initializing with default dimensions.\")\n",
        "            input_dim = 512  # Default CLIP embedding size\n",
        "            output_dim = phi3_model.config.hidden_size\n",
        "            projector = ImageProjector(input_dim, output_dim)\n",
        "\n",
        "        # Move the projector to the same device as phi3_model\n",
        "        projector = projector.to(phi3_model.device)\n",
        "\n",
        "        # Create and return the Phi3WithProjector instance\n",
        "        model = cls(phi3_model, projector, debug=debug)\n",
        "        return model\n",
        "\n",
        "    def save_pretrained(self, save_directory):\n",
        "        # Save the base model\n",
        "        self.phi3.save_pretrained(save_directory)\n",
        "\n",
        "        # Save the projector weights\n",
        "        projector_path = os.path.join(save_directory, \"image_projector.pth\")\n",
        "        torch.save(self.projector.state_dict(), projector_path)\n",
        "\n",
        "        # Save the config\n",
        "        self.config.save_pretrained(save_directory)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, image_embeddings=None, labels=None, past_key_values=None, **kwargs):\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        if image_embeddings is not None:\n",
        "            image_embeddings = image_embeddings.to(device)\n",
        "            projected_images = self.projector(image_embeddings)\n",
        "            projected_images = projected_images.unsqueeze(1)\n",
        "            self.debug_print(f\"forward projected_images: {projected_images.size()}\")\n",
        "\n",
        "            if past_key_values is None:  # This is the first forward pass\n",
        "                self.debug_print(f\"forward before: {attention_mask.size() if attention_mask is not None else None}\")\n",
        "                if 'inputs_embeds' in kwargs and kwargs['inputs_embeds'] is not None:\n",
        "                    inputs_embeds = kwargs['inputs_embeds']\n",
        "                    self.debug_print(f\"forward before inputs_embeds: {inputs_embeds.size()}\")\n",
        "                    inputs_embeds = torch.cat([projected_images, inputs_embeds], dim=1)\n",
        "                    kwargs['inputs_embeds'] = inputs_embeds\n",
        "                    self.debug_print(f\"forward after inputs_embeds: {inputs_embeds.size()}\")\n",
        "                elif input_ids is not None:\n",
        "                    self.debug_print(f\"forward input_ids: {input_ids.size()}\")\n",
        "                    inputs_embeds = self.get_input_embeddings()(input_ids.to(device))\n",
        "                    self.debug_print(f\"forward before inputs_embeds: {inputs_embeds.size()}\")\n",
        "                    inputs_embeds = torch.cat([projected_images, inputs_embeds], dim=1)\n",
        "                    self.debug_print(f\"forward after inputs_embeds: {inputs_embeds.size()}\")\n",
        "                    kwargs['inputs_embeds'] = inputs_embeds\n",
        "                    input_ids = None  # Set to None to avoid conflict\n",
        "\n",
        "                if attention_mask is not None:\n",
        "                    attention_mask = torch.cat([torch.ones(image_embeddings.size(0), 1, device=device), attention_mask.to(device)], dim=1)\n",
        "                else:\n",
        "                    attention_mask = torch.ones(image_embeddings.size(0), inputs_embeds.size(1), device=device)\n",
        "\n",
        "                if labels is not None:\n",
        "                    # Adjust labels to match the new sequence length\n",
        "                    labels = torch.cat([torch.full((labels.size(0), 1), -100, device=device), labels], dim=1)\n",
        "\n",
        "        if labels is not None:\n",
        "            labels = labels.to(device)\n",
        "\n",
        "        # Determine sequence length\n",
        "        if 'inputs_embeds' in kwargs and kwargs['inputs_embeds'] is not None:\n",
        "            seq_length = kwargs['inputs_embeds'].size(1)\n",
        "        elif input_ids is not None:\n",
        "            seq_length = input_ids.size(1)\n",
        "        else:\n",
        "            seq_length = attention_mask.size(1) if attention_mask is not None else None\n",
        "\n",
        "        if seq_length is None:\n",
        "            raise ValueError(\"Unable to determine sequence length. Provide either input_ids, inputs_embeds, or attention_mask.\")\n",
        "\n",
        "        # Ensure attention_mask matches the sequence length\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask[:, :seq_length]\n",
        "\n",
        "        self.debug_print(f\"forward final: input_ids shape: {input_ids.shape if input_ids is not None else None}\")\n",
        "        self.debug_print(f\"forward final: attention_mask shape: {attention_mask.shape if attention_mask is not None else None}\")\n",
        "        self.debug_print(f\"forward final: inputs_embeds shape: {kwargs.get('inputs_embeds', {}).shape if kwargs.get('inputs_embeds') is not None else None}\")\n",
        "\n",
        "        return self.phi3(input_ids=input_ids, attention_mask=attention_mask, labels=labels, past_key_values=past_key_values, **kwargs)\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **kwargs):\n",
        "        inputs = self.phi3.prepare_inputs_for_generation(input_ids, past=past, attention_mask=attention_mask, **kwargs)\n",
        "\n",
        "        if 'image_embeddings' in kwargs:\n",
        "            inputs['image_embeddings'] = kwargs['image_embeddings']\n",
        "\n",
        "            if past is None:  # First forward pass\n",
        "                # Adjust attention_mask to account for the image token\n",
        "                if attention_mask is not None:\n",
        "                    inputs['attention_mask'] = torch.cat([torch.ones((attention_mask.size(0), 1), device=attention_mask.device), attention_mask], dim=1)\n",
        "            else:  # Subsequent passes\n",
        "                # Ensure attention_mask matches the current sequence length\n",
        "                if attention_mask is not None:\n",
        "                    current_seq_length = past[0][0].size(2) + 1  # past key's sequence length + 1 for the new token\n",
        "                    inputs['attention_mask'] = attention_mask[:, :current_seq_length]\n",
        "\n",
        "            inputs.pop('position_ids', None)\n",
        "\n",
        "        # Safe printing of shapes\n",
        "        self.debug_print(f\"prepare_inputs_for_generation: input_ids shape: {inputs['input_ids'].shape if 'input_ids' in inputs else None}\")\n",
        "        self.debug_print(f\"prepare_inputs_for_generation: attention_mask shape: {inputs['attention_mask'].shape if 'attention_mask' in inputs else None}\")\n",
        "        self.debug_print(f\"prepare_inputs_for_generation: inputs_embeds shape: {inputs.get('inputs_embeds', {}).shape if inputs.get('inputs_embeds') is not None else None}\")\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.phi3.get_input_embeddings()\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.phi3.set_input_embeddings(value)\n",
        "\n",
        "    def gradient_checkpointing_enable(self, **kwargs):\n",
        "        self.phi3.gradient_checkpointing_enable(**kwargs)\n",
        "\n",
        "    def gradient_checkpointing_disable(self):\n",
        "        self.phi3.gradient_checkpointing_disable()\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        try:\n",
        "            return super().__getattr__(name)\n",
        "        except AttributeError:\n",
        "            return getattr(self.phi3, name)\n",
        "\n",
        "    def generate(self, input_ids=None, attention_mask=None, image_embeddings=None, **kwargs):\n",
        "        if image_embeddings is not None:\n",
        "            kwargs['image_embeddings'] = image_embeddings\n",
        "            self.debug_print(f\"generate input_ids: {input_ids.size()}\")\n",
        "            self.debug_print(f\"generate image_embedding: {image_embeddings.size()}\")\n",
        "\n",
        "        if attention_mask is not None and image_embeddings is not None:\n",
        "            # Add an extra attention mask token for the image embedding\n",
        "            self.debug_print(f\"generate before: {attention_mask.size()}\")\n",
        "            attention_mask = torch.cat([torch.ones(attention_mask.size(0), 1, device=attention_mask.device), attention_mask], dim=1)\n",
        "            self.debug_print(f\"generate after: {attention_mask.size()}\")\n",
        "\n",
        "        return super().generate(input_ids=input_ids, attention_mask=attention_mask, **kwargs)"
      ],
      "metadata": {
        "id": "-BBqvYmtS8ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path in Google Drive where you want to save the checkpoints\n",
        "gdrive_checkpoint_dir = \"/content/drive/MyDrive/multimodel_llm/phi3_checkpoints\"\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(gdrive_checkpoint_dir, exist_ok=True)\n",
        "import dataclasses\n",
        "\n",
        "class SaveLatestCheckpointCallback(TrainerCallback):\n",
        "    def on_save(self, args, state, control, **kwargs):\n",
        "        if state.is_world_process_zero:\n",
        "            checkpoint_dir = os.path.join(gdrive_checkpoint_dir, f\"checkpoint-{state.global_step}\")\n",
        "\n",
        "            # Save the model and tokenizer\n",
        "            kwargs[\"model\"].save_pretrained(checkpoint_dir)\n",
        "            kwargs[\"tokenizer\"].save_pretrained(checkpoint_dir)\n",
        "\n",
        "            # Save the projector separately\n",
        "            projector_path = os.path.join(checkpoint_dir, \"image_projector.pth\")\n",
        "            # torch.save(kwargs[\"model\"].projector.state_dict(), projector_path)\n",
        "            torch.save(kwargs[\"model\"].base_model.model.projector.state_dict(), projector_path)\n",
        "\n",
        "            # Explicitly save the trainer state\n",
        "            trainer_state_path = os.path.join(checkpoint_dir, \"trainer_state.json\")\n",
        "            state_dict = dataclasses.asdict(state)\n",
        "            with open(trainer_state_path, \"w\") as f:\n",
        "                json.dump(state_dict, f, indent=2)\n",
        "\n",
        "            # Remove previous checkpoint\n",
        "            prev_checkpoint = os.path.join(gdrive_checkpoint_dir, f\"checkpoint-{state.global_step - args.save_steps}\")\n",
        "            if os.path.exists(prev_checkpoint):\n",
        "                import shutil\n",
        "                shutil.rmtree(prev_checkpoint)\n",
        "\n",
        "# Function to get the latest checkpoint\n",
        "def get_latest_checkpoint(checkpoint_dir):\n",
        "    checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith('checkpoint-')]\n",
        "    if not checkpoints:\n",
        "        return None\n",
        "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[1]))\n",
        "    return os.path.join(checkpoint_dir, latest_checkpoint)"
      ],
      "metadata": {
        "id": "CB_fUxzpiBUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_bf16_supported():\n",
        "  compute_dtype = torch.bfloat16\n",
        "#   attn_implementation = 'flash_attention_2'\n",
        "else:\n",
        "  compute_dtype = torch.float16\n",
        "#   attn_implementation = 'sdpa'\n",
        "\n",
        "# print(attn_implementation)\n",
        "print(compute_dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM0oP2VqCN64",
        "outputId": "9c89a2a5-f019-4d26-d943-8c00497981d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.bfloat16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "# # Load the model again for quantization\n",
        "# ### Download Phi-3 model\n",
        "# phi3_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_name,\n",
        "#     trust_remote_code=True,\n",
        "#     quantization_config=bnb_config,\n",
        "#     device_map=device_map,\n",
        "#     torch_dtype=compute_dtype,\n",
        "#     # attn_implementation=attn_implementation\n",
        "# )\n",
        "\n",
        "# print(phi3_model)"
      ],
      "metadata": {
        "id": "kgE8Vq_i6kMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize the projector\n",
        "# image_embedding_dim = len(hf_dataset[0]['image_embedding'])\n",
        "# projection_dim = phi3_model.config.hidden_size  # Get dimension from the model\n",
        "# projector = ImageProjector(image_embedding_dim, projection_dim).to(device)\n",
        "\n",
        "# # Combine Phi-3 with the projector\n",
        "# model = Phi3WithProjector(phi3_model, projector)\n",
        "\n",
        "# Get the latest checkpoint\n",
        "latest_checkpoint = get_latest_checkpoint(gdrive_checkpoint_dir)\n",
        "\n",
        "if latest_checkpoint:\n",
        "    print(f\"Loading model from checkpoint: {latest_checkpoint}\")\n",
        "    model = Phi3WithProjector.from_pretrained(\n",
        "        latest_checkpoint,\n",
        "        trust_remote_code=True,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=device_map,\n",
        "        torch_dtype=compute_dtype,\n",
        "    )\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting training from scratch.\")\n",
        "    phi3_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        trust_remote_code=True,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=device_map,\n",
        "        torch_dtype=compute_dtype,\n",
        "    )\n",
        "    image_embedding_dim = len(hf_dataset[0]['image_embedding'])\n",
        "    projection_dim = phi3_model.config.hidden_size\n",
        "    projector = ImageProjector(image_embedding_dim, projection_dim).to(device)\n",
        "    model = Phi3WithProjector(phi3_model, projector)\n",
        "\n",
        "# Prepare the model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "UXVkny4DWOdU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "b92dafb1ce0c444eb6c1f682d0a4295b",
            "e9ff90e967224116801a134613807af5",
            "61f4db8f376c4206894bb867cc5712f5",
            "3bfff8cbf07d4041bbc5d1c81beb5522",
            "3b63be07cf4c4a018a45184bcdbc014b",
            "f34578cf11c8431b8878855d5c3d8946",
            "f4fd4eec2d034b46a701305f9101b826",
            "59daa0fe133f453797c15fe04328c875",
            "1d65394db1bf4e81aacb7e65c9f0984b",
            "21ba01a13c22434f9daf5c0d4719946d",
            "088a45e1e1d24ae1b6a008117b4b1f80"
          ]
        },
        "outputId": "c74abf59-b400-4853-f301-9302d0a816f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from checkpoint: /content/drive/MyDrive/multimodel_llm/phi3_checkpoints/checkpoint-150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b92dafb1ce0c444eb6c1f682d0a4295b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading adapter weights from /content/drive/MyDrive/multimodel_llm/phi3_checkpoints/checkpoint-150 led to unexpected keys not found in the model:  ['phi3.model.layers.0.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.0.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.1.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.1.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.10.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.10.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.11.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.11.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.12.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.12.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.13.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.13.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.14.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.14.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.15.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.15.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.16.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.16.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.17.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.17.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.18.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.18.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.19.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.19.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.2.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.2.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.20.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.20.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.21.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.21.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.22.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.22.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.23.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.23.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.24.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.24.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.25.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.25.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.26.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.26.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.27.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.27.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.28.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.28.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.29.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.29.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.3.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.3.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.30.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.30.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.31.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.31.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.4.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.4.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.5.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.5.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.6.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.6.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.7.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.7.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.8.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.8.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'phi3.model.layers.9.mlp.down_proj.lora_A.default.weight', 'phi3.model.layers.9.mlp.down_proj.lora_B.default.weight', 'phi3.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'phi3.model.layers.9.self_attn.o_proj.lora_B.default.weight']. \n",
            "<ipython-input-18-3dd996f11520>:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  projector_state_dict = torch.load(projector_path, map_location=phi3_model.device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded projector with input_dim=512, output_dim=3072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(compute_dtype) , print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGcgsGvR6uw0",
        "outputId": "97adb1eb-155d-4c98-eb93-0974f15b6e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float16\n",
            "Phi3WithProjector(\n",
            "  (phi3): Phi3ForCausalLM(\n",
            "    (model): Phi3Model(\n",
            "      (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
            "      (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0-31): 32 x Phi3DecoderLayer(\n",
            "          (self_attn): Phi3Attention(\n",
            "            (o_proj): lora.Linear4bit(\n",
            "              (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=3072, out_features=32, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=32, out_features=3072, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "              (lora_magnitude_vector): ModuleDict()\n",
            "            )\n",
            "            (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
            "            (rotary_emb): Phi3RotaryEmbedding()\n",
            "          )\n",
            "          (mlp): Phi3MLP(\n",
            "            (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
            "            (down_proj): lora.Linear4bit(\n",
            "              (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=8192, out_features=32, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=32, out_features=3072, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "              (lora_magnitude_vector): ModuleDict()\n",
            "            )\n",
            "            (activation_fn): SiLU()\n",
            "          )\n",
            "          (input_layernorm): Phi3RMSNorm()\n",
            "          (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (post_attention_layernorm): Phi3RMSNorm()\n",
            "        )\n",
            "      )\n",
            "      (norm): Phi3RMSNorm()\n",
            "    )\n",
            "    (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
            "  )\n",
            "  (projector): ImageProjector(\n",
            "    (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (activation): GELU(approximate='none')\n",
            "    (layer2): Linear(in_features=1024, out_features=3072, bias=True)\n",
            "    (dropout): Dropout(p=0.05, inplace=False)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "ADlAMa4t6t7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=lora_r,\n",
        "    lora_alpha=lora_alpha,\n",
        "    target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
        "    lora_dropout=lora_dropout,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDJ1TYQR67du",
        "outputId": "294b9695-fec2-40e3-bc7a-423476389e5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 17825792 || all params: 2030640128 || trainable%: 0.8778410194009522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ik0MZQg8bjn",
        "outputId": "5feab58e-7d66-45cb-fa20-80ba6de588a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): Phi3WithProjector(\n",
            "      (phi3): Phi3ForCausalLM(\n",
            "        (model): Phi3Model(\n",
            "          (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
            "          (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (layers): ModuleList(\n",
            "            (0-31): 32 x Phi3DecoderLayer(\n",
            "              (self_attn): Phi3Attention(\n",
            "                (o_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=3072, out_features=32, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=32, out_features=3072, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
            "                (rotary_emb): Phi3RotaryEmbedding()\n",
            "              )\n",
            "              (mlp): Phi3MLP(\n",
            "                (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
            "                (down_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=8192, out_features=32, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=32, out_features=3072, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (activation_fn): SiLU()\n",
            "              )\n",
            "              (input_layernorm): Phi3RMSNorm()\n",
            "              (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "              (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "              (post_attention_layernorm): Phi3RMSNorm()\n",
            "            )\n",
            "          )\n",
            "          (norm): Phi3RMSNorm()\n",
            "        )\n",
            "        (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
            "      )\n",
            "      (projector): ImageProjector(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (activation): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=3072, bias=True)\n",
            "        (dropout): Dropout(p=0.05, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "RZLnz1M4NKqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "AVcaw9k7NDdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size  = per_device_eval_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"all\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=eval_steps, # Evaluate every 25 steps\n",
        "    # Add these new arguments\n",
        "    do_eval=True,\n",
        "    eval_delay=0,  # Start evaluation immediately\n",
        "    # Enable gradient checkpointing\n",
        "    gradient_checkpointing=gradient_checkpointing,\n",
        "    # Disable data parallelism if not needed\n",
        "    ddp_find_unused_parameters=False,\n",
        "    save_total_limit=1,  # Keep only the latest checkpoint\n",
        ")\n",
        "\n",
        "# Custom data collator to handle pre-tokenized inputs\n",
        "def custom_data_collator(features):\n",
        "    batch = {k: [d[k] for d in features] for k in features[0].keys()}\n",
        "\n",
        "    # Stack image embeddings\n",
        "    batch['image_embeddings'] = torch.stack(batch['image_embeddings'])\n",
        "\n",
        "    # Pad the sequences\n",
        "    batch['input_ids'] = torch.nn.utils.rnn.pad_sequence(batch['input_ids'], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    batch['attention_mask'] = torch.nn.utils.rnn.pad_sequence(batch['attention_mask'], batch_first=True, padding_value=0)\n",
        "    batch['labels'] = torch.nn.utils.rnn.pad_sequence(batch['labels'], batch_first=True, padding_value=-100)\n",
        "\n",
        "    return batch"
      ],
      "metadata": {
        "id": "zGBG0EJINFCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to select a random subset of the dataset\n",
        "def select_subset(dataset, fraction=0.05):\n",
        "    num_samples = int(len(dataset) * fraction)\n",
        "    indices = random.sample(range(len(dataset)), num_samples)\n",
        "    return dataset.select(indices)\n",
        "\n",
        "# Select 5% of the training and test datasets\n",
        "small_train_dataset = select_subset(dataset_dict['train'], fraction=0.1)\n",
        "small_test_dataset = select_subset(dataset_dict['test'], fraction=0.1)\n",
        "\n",
        "# Create a new DatasetDict with the smaller datasets\n",
        "small_dataset_dict = DatasetDict({\n",
        "    'train': small_train_dataset,\n",
        "    'test': small_test_dataset\n",
        "})\n",
        "\n",
        "print(f\"Small train dataset size: {len(small_dataset_dict['train'])}\")\n",
        "print(f\"Small test dataset size: {len(small_dataset_dict['test'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDU9tnyZW_CW",
        "outputId": "c9e81204-ab20-48ca-f6e6-1b2538e8b5e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small train dataset size: 32526\n",
            "Small test dataset size: 3614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    # train_dataset=dataset_dict['train'],\n",
        "    # eval_dataset=dataset_dict['test'],\n",
        "    train_dataset=small_dataset_dict['train'],\n",
        "    eval_dataset=small_dataset_dict['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=custom_data_collator,\n",
        "    peft_config=lora_config,\n",
        "    max_seq_length=max_seq_length,\n",
        "    packing=packing,\n",
        "    callbacks=[SaveLatestCheckpointCallback()],  # Add the custom callback\n",
        ")\n",
        "\n",
        "# Perform initial evaluation\n",
        "print(\"Performing initial evaluation...\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Initial evaluation results: {eval_results}\")\n",
        "\n",
        "# # Start training\n",
        "trainer.train()\n",
        "# Start or resume training\n",
        "# trainer.train(resume_from_checkpoint=latest_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yGi1pNF98jy5",
        "outputId": "e99d69bb-fb7b-4acd-d741-ceb08dec633f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing initial evaluation...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1808' max='904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [904/904 5:39:05]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial evaluation results: {'eval_loss': 9.622665405273438, 'eval_model_preparation_time': 0.0042, 'eval_runtime': 6453.6574, 'eval_samples_per_second': 0.56, 'eval_steps_per_second': 0.14}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='101' max='1016' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 101/1016 5:52:58 < 54:22:24, 0.00 it/s, Epoch 0.10/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Model Preparation Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.368500</td>\n",
              "      <td>0.232070</td>\n",
              "      <td>0.004200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='572' max='904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [572/904 1:07:49 < 39:26, 0.14 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='202' max='1016' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 202/1016 15:24:48 < 62:43:58, 0.00 it/s, Epoch 0.20/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Model Preparation Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.368500</td>\n",
              "      <td>0.232070</td>\n",
              "      <td>0.004200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.359500</td>\n",
              "      <td>0.227313</td>\n",
              "      <td>0.004200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.365700</td>\n",
              "      <td>0.220686</td>\n",
              "      <td>0.004200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.362100</td>\n",
              "      <td>0.216167</td>\n",
              "      <td>0.004200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-cb43eaf38992>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# # Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m# Start or resume training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# trainer.train(resume_from_checkpoint=latest_checkpoint)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trl_activate_neftune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;31m# After training we make sure to retrieve back the original forward pass method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1939\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2278\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2279\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2281\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3348\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3349\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3351\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2195\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2196\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "# trainer.model.save_pretrained(new_model)\n",
        "final_model_path = os.path.join(gdrive_checkpoint_dir, \"final_model\")\n",
        "trainer.model.save_pretrained(final_model_path)\n",
        "tokenizer.save_pretrained(final_model_path)"
      ],
      "metadata": {
        "id": "6eGu422_8mLp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9e7dfa7-a5e1-4049-d4e3-f8cd01759ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/multimodel_llm/phi3_checkpoints/final_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/multimodel_llm/phi3_checkpoints/final_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/multimodel_llm/phi3_checkpoints/final_model/tokenizer.model',\n",
              " '/content/drive/MyDrive/multimodel_llm/phi3_checkpoints/final_model/added_tokens.json',\n",
              " '/content/drive/MyDrive/multimodel_llm/phi3_checkpoints/final_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.base_model.model.projector.state_dict(), final_model_path + '/image_projector.pth')\n",
        "print(f\"Projector saved to: {final_model_path}/image_projector.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgYtfm2bUP-F",
        "outputId": "45512b98-1889-4f95-f3d3-6170d19252a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Projector saved to: /content/drive/MyDrive/multimodel_llm/phi3_checkpoints/final_model/image_projector.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r results /content/drive/MyDrive/multimodel_llm/"
      ],
      "metadata": {
        "id": "UwRtBYUXWnsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sample inference code"
      ],
      "metadata": {
        "id": "i3TooP90Xd94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "w0bqPi-V5q_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom text generation class\n",
        "class CustomTextGenerator:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def generate(self, input_text, image_embedding, **generate_kwargs):\n",
        "        # Tokenize the input text\n",
        "        inputs = self.tokenizer(input_text, return_tensors=\"pt\")\n",
        "        input_ids = inputs[\"input_ids\"].to(self.model.device)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(self.model.device)\n",
        "\n",
        "        # Ensure image_embedding is a tensor and move it to the correct device\n",
        "        if not isinstance(image_embedding, torch.Tensor):\n",
        "            image_embedding = torch.tensor(image_embedding)\n",
        "        image_embedding = image_embedding.to(self.model.device)\n",
        "\n",
        "        # Adjust attention_mask to account for the image embedding token\n",
        "        image_attention = torch.ones((1, 1), dtype=torch.long, device=self.model.device)\n",
        "        attention_mask = torch.cat([image_attention, attention_mask], dim=1)\n",
        "\n",
        "        # Generate text\n",
        "        outputs = self.model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            image_embeddings=image_embedding.unsqueeze(0),  # Add batch dimension\n",
        "            **generate_kwargs\n",
        "        )\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return generated_text\n",
        "\n",
        "# Initialize the custom text generator\n",
        "generator = CustomTextGenerator(model=model, tokenizer=tokenizer)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jx1IulVlXc9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f463411-93d5-412f-bf28-bb6349b1f385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|> You are a helpful assistant.<|end|><|user|> Given the following information, provide a detailed and accurate response:\n",
            "How is the little girl dressed?\n",
            "[An image is provided for this task.]\n",
            "<|end|><|assistant|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a sample from the validation set\n",
        "sample = dataset_dict['test'][1]\n",
        "image_embedding = sample['image_embeddings']\n",
        "\n",
        "\n",
        "def get_first_user_input(decoded_text):\n",
        "    # Find the position of the first <|assistant|> tag\n",
        "    assistant_pos = decoded_text.find('<|assistant|>')\n",
        "\n",
        "    # If <|assistant|> is found, truncate the text\n",
        "    if assistant_pos != -1:\n",
        "        return decoded_text[:assistant_pos].strip()\n",
        "    else:\n",
        "        return decoded_text.strip()\n",
        "\n",
        "\n",
        "# Decode the input_ids\n",
        "full_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=False)\n",
        "\n",
        "# Extract only the first user input\n",
        "input_text = get_first_user_input(full_text) + '<|assistant|>'\n",
        "print(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VicHtNI7XEFd",
        "outputId": "9c164549-fe4d-4c9f-9eee-1def1afcf607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|> You are a helpful assistant.<|end|><|user|> Given the following information, provide a detailed and accurate response:\n",
            "Analyze the image in a comprehensive and detailed manner.\n",
            "[An image is provided for this task.]\n",
            "<|end|><|assistant|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "generated_text = generator.generate(\n",
        "    input_text,\n",
        "    image_embedding=image_embedding,\n",
        "    # max_length=200,\n",
        "    # num_return_sequences=1,\n",
        "    # do_sample=True,\n",
        "    # temperature=0.7,\n",
        "    # top_k=50,\n",
        "    # top_p=0.95,\n",
        "    max_new_tokens=150,\n",
        "    num_return_sequences=1,\n",
        "    do_sample=True,\n",
        "    temperature=0.8,\n",
        "    top_k=40,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2,\n",
        "    no_repeat_ngram_size=3,\n",
        ")\n",
        "\n",
        "print(\"Input text:\")\n",
        "print(input_text)\n",
        "print(\"\\nGenerated text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "lMfczDxg3sCo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66559226-1e18-440f-9c22-c55dd0c3ff66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text:\n",
            "<|system|> You are a helpful assistant.<|end|><|user|> Given the following information, provide a detailed and accurate response:\n",
            "Analyze the image in a comprehensive and detailed manner.\n",
            "[An image is provided for this task.]\n",
            "<|end|><|assistant|>\n",
            "\n",
            "Generated text:\n",
            "You are a helpful assistant. Given the following information, provide a detailed and accurate response:\n",
            "Analyze the image in a comprehensive and detailed manner.\n",
            "[An image is provided for this task.]\n",
            " The scene shows two people standing on opposite sides of an open bedroom window that leads to another room or living space inside their home building complexes near each other's apartments located next door across from one another at 1234 Northwest Boulevard Street (Stanley Apartments). They appear comfortable with being outside together while having conversations without actually interacting directly as they sit facing away towards different directions around them - possibly enjoying some fresh air before returning indoors later today after work hours begin soon since it’s early morning time now when most residents would still be getting ready themselves rather than outdoor socializing just yet! It seems like these neighbours have formed quite close bonds despite not meeting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### merge models and save in gdrive"
      ],
      "metadata": {
        "id": "fKrJVITzcjPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "z9NBBb8IdcfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the projector\n",
        "projector_path = '/content/drive/MyDrive/multimodel_llm/image_projector.pth'\n",
        "os.makedirs(os.path.dirname(projector_path), exist_ok=True)\n",
        "torch.save(model.projector.state_dict(), projector_path)\n",
        "print(f\"Projector saved to: {projector_path}\")"
      ],
      "metadata": {
        "id": "QoBIZYRVQjaO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32983810-b07c-4fa0-89ed-f5cffdaf0cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Projector saved to: /content/drive/MyDrive/multimodel_llm/image_projector.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the fine-tuned adapter with the base model\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load the fine-tuned model with the LoRA adapter\n",
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "print(base_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500,
          "referenced_widgets": [
            "9a36c712e9234a159c39a759dc846ff1",
            "d75772fdc54f4d42be4275889f179f05",
            "9cc7d5850fa5417ab748082d5f17fa93",
            "570c101a3d674074ae82bdbda4e676b5",
            "9314edf15b7e40bb8f81e09a960843c9",
            "c36cf1536e1c4c8585f206d125aa13ac",
            "697d9b25772d421ab3f6efcdbb1b95f6",
            "7bda5770bda74e989005dab8f5bcb83d",
            "235c5d9e633b4ea3aa41438fd75dcfdb",
            "be13cbf4000c48b394ceccd46f8c1944",
            "204615da3e0749be96571823fe398f4f"
          ]
        },
        "id": "I-AjI-T6XSoL",
        "outputId": "66d60951-2b1f-469a-8e08-24814a3aae5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a36c712e9234a159c39a759dc846ff1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phi3ForCausalLM(\n",
            "  (model): Phi3Model(\n",
            "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
            "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x Phi3DecoderLayer(\n",
            "        (self_attn): Phi3Attention(\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
            "          (rotary_emb): Phi3RotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Phi3MLP(\n",
            "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (activation_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gdrive_checkpoint_dir = \"/content/drive/MyDrive/multimodel_llm/phi3_checkpoints\"\n",
        "# final_model_path = os.path.join(gdrive_checkpoint_dir, \"final_model\")"
      ],
      "metadata": {
        "id": "xQgu6gMDcSFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = PeftModel.from_pretrained(base_model, final_model_path)\n",
        "print(new_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecNp7BAQXawA",
        "outputId": "83aff96c-baa7-4bb1-ad23-17ac66dd3e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): Phi3ForCausalLM(\n",
            "      (model): Phi3Model(\n",
            "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
            "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (layers): ModuleList(\n",
            "          (0-31): 32 x Phi3DecoderLayer(\n",
            "            (self_attn): Phi3Attention(\n",
            "              (o_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
            "              (rotary_emb): Phi3RotaryEmbedding()\n",
            "            )\n",
            "            (mlp): Phi3MLP(\n",
            "              (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
            "              (down_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=8192, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (activation_fn): SiLU()\n",
            "            )\n",
            "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDj5raM3f0ED",
        "outputId": "9ec64063-8d04-4837-d790-393040feaad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): Phi3ForCausalLM(\n",
            "      (model): Phi3Model(\n",
            "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
            "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (layers): ModuleList(\n",
            "          (0-31): 32 x Phi3DecoderLayer(\n",
            "            (self_attn): Phi3Attention(\n",
            "              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "              (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
            "              (rotary_emb): Phi3RotaryEmbedding()\n",
            "            )\n",
            "            (mlp): Phi3MLP(\n",
            "              (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
            "              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "              (activation_fn): SiLU()\n",
            "            )\n",
            "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in new_model.named_parameters():\n",
        "    if 'lora' in name:\n",
        "        print(f\"{name}: {param.data.abs().mean()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTt7_xJTfW9h",
        "outputId": "4b73cac5-e330-4156-e94c-8cd30929f75d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: 0.009008973836898804\n",
            "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: 0.005532282404601574\n",
            "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: 0.009038114920258522\n",
            "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: 0.00552835687994957\n",
            "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: 0.009009761735796928\n",
            "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: 0.005520865321159363\n",
            "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: 0.009024171158671379\n",
            "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: 0.005519507452845573\n",
            "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: 0.009033909067511559\n",
            "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: 0.005520087666809559\n",
            "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: 0.009003596380352974\n",
            "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: 0.005520024336874485\n",
            "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: 0.00903896614909172\n",
            "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: 0.005526484455913305\n",
            "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: 0.009024860337376595\n",
            "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: 0.0055214702151715755\n",
            "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: 0.009008003398776054\n",
            "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: 0.005506602115929127\n",
            "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: 0.00899041723459959\n",
            "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: 0.005511632189154625\n",
            "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: 0.009042087942361832\n",
            "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: 0.005531428847461939\n",
            "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: 0.009050562977790833\n",
            "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: 0.005523878149688244\n",
            "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: 0.009027582593262196\n",
            "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: 0.005520029924809933\n",
            "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: 0.009044015780091286\n",
            "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: 0.0055297985672950745\n",
            "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: 0.009038247168064117\n",
            "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: 0.005527005065232515\n",
            "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: 0.008997369557619095\n",
            "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: 0.005510325543582439\n",
            "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: 0.009008968248963356\n",
            "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: 0.005536129232496023\n",
            "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: 0.00900212861597538\n",
            "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: 0.0055220164358615875\n",
            "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: 0.009052935056388378\n",
            "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: 0.005522937048226595\n",
            "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: 0.00903028529137373\n",
            "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: 0.005524856504052877\n",
            "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: 0.009050963446497917\n",
            "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: 0.005525502376258373\n",
            "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: 0.009038038551807404\n",
            "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: 0.005523297935724258\n",
            "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: 0.009029913693666458\n",
            "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: 0.005514364689588547\n",
            "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: 0.009034032933413982\n",
            "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: 0.005529041867703199\n",
            "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: 0.00900854729115963\n",
            "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: 0.005517315119504929\n",
            "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: 0.009006800130009651\n",
            "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: 0.005515945143997669\n",
            "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: 0.009039103053510189\n",
            "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: 0.005527391564100981\n",
            "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: 0.009011130779981613\n",
            "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: 0.005532611161470413\n",
            "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: 0.008993912488222122\n",
            "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: 0.005534015595912933\n",
            "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: 0.009014577604830265\n",
            "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: 0.005530972965061665\n",
            "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: 0.009038137272000313\n",
            "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: 0.005528111010789871\n",
            "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: 0.009001132100820541\n",
            "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: 0.0\n",
            "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: 0.005516123957931995\n",
            "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the LoRA adapter with the base model\n",
        "merged_model = new_model.merge_and_unload()\n",
        "print(new_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCmZgr1hX5Qh",
        "outputId": "2dbfc913-af7c-40b2-cb0e-b19d0cc8d6a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): Phi3ForCausalLM(\n",
            "      (model): Phi3Model(\n",
            "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
            "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (layers): ModuleList(\n",
            "          (0-31): 32 x Phi3DecoderLayer(\n",
            "            (self_attn): Phi3Attention(\n",
            "              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "              (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
            "              (rotary_emb): Phi3RotaryEmbedding()\n",
            "            )\n",
            "            (mlp): Phi3MLP(\n",
            "              (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
            "              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "              (activation_fn): SiLU()\n",
            "            )\n",
            "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to save the merged model in Google Drive\n",
        "merged_model_path = '/content/drive/MyDrive/multimodel_llm/merged_phi3_llava_model'\n",
        "\n",
        "# Save the merged model\n",
        "# merged_model.save_pretrained(merged_model_path)\n",
        "\n",
        "# Initialize the projector\n",
        "image_embedding_dim = len(hf_dataset[0]['image_embedding'])\n",
        "projection_dim = merged_model.config.hidden_size  # Get dimension from the model\n",
        "projector = ImageProjector(image_embedding_dim, projection_dim).to(device)\n",
        "projector.load_state_dict(torch.load(final_model_path + '/image_projector.pth'))\n",
        "print(projector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abVWgXn3YZ3e",
        "outputId": "b008653d-0bb0-4999-e691-0d8b838c07f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ImageProjector(\n",
            "  (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "  (activation): GELU(approximate='none')\n",
            "  (layer2): Linear(in_features=1024, out_features=3072, bias=True)\n",
            "  (dropout): Dropout(p=0.05, inplace=False)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-225c83f6a853>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  projector.load_state_dict(torch.load(final_model_path + '/image_projector.pth'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine Phi-3 with the projector\n",
        "phi3_with_projector = Phi3WithProjector(merged_model, projector)\n",
        "print(phi3_with_projector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EttRmMj0ZZMy",
        "outputId": "1848073d-8e4f-4726-d2a9-e0fd96c0d4c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phi3WithProjector(\n",
            "  (phi3): Phi3ForCausalLM(\n",
            "    (model): Phi3Model(\n",
            "      (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
            "      (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0-31): 32 x Phi3DecoderLayer(\n",
            "          (self_attn): Phi3Attention(\n",
            "            (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "            (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
            "            (rotary_emb): Phi3RotaryEmbedding()\n",
            "          )\n",
            "          (mlp): Phi3MLP(\n",
            "            (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
            "            (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "            (activation_fn): SiLU()\n",
            "          )\n",
            "          (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "          (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "    )\n",
            "    (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
            "  )\n",
            "  (projector): ImageProjector(\n",
            "    (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (activation): GELU(approximate='none')\n",
            "    (layer2): Linear(in_features=1024, out_features=3072, bias=True)\n",
            "    (dropout): Dropout(p=0.05, inplace=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_models(base_model, fine_tuned_model):\n",
        "    differences = []\n",
        "    for (name1, p1), (name2, p2) in zip(base_model.named_parameters(), fine_tuned_model.named_parameters()):\n",
        "        if name1 == name2:\n",
        "            diff = (p1 - p2).abs().max().item()  # Use max absolute difference\n",
        "            differences.append((name1, diff))\n",
        "    return differences\n",
        "# Compare the models\n",
        "differences = compare_models(base_model, new_model.model)\n",
        "\n",
        "# Sort differences by magnitude\n",
        "differences.sort(key=lambda x: x[1], reverse=True)\n",
        "print(\"Top 10 layers with the largest differences:\")\n",
        "for name, diff in differences[:10]:\n",
        "    print(f\"{name}: {diff}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACARxj8AdL5f",
        "outputId": "977f0fb0-8236-4699-bdcf-75cb52b55e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 layers with the largest differences:\n",
            "model.embed_tokens.weight: 0.0\n",
            "model.layers.0.self_attn.o_proj.weight: 0.0\n",
            "model.layers.0.self_attn.qkv_proj.weight: 0.0\n",
            "model.layers.0.mlp.gate_up_proj.weight: 0.0\n",
            "model.layers.0.mlp.down_proj.weight: 0.0\n",
            "model.layers.0.input_layernorm.weight: 0.0\n",
            "model.layers.0.post_attention_layernorm.weight: 0.0\n",
            "model.layers.1.self_attn.o_proj.weight: 0.0\n",
            "model.layers.1.self_attn.qkv_proj.weight: 0.0\n",
            "model.layers.1.mlp.gate_up_proj.weight: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the merged model with the projector\n",
        "phi3_with_projector.save_pretrained(merged_model_path)\n",
        "\n",
        "# Save the tokenizer\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.save_pretrained(merged_model_path)\n",
        "\n",
        "print(f\"Merged model and tokenizer saved to: {merged_model_path}\")"
      ],
      "metadata": {
        "id": "0BzSYEaiWi3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef30470-166a-40e5-aa0a-d73a3115b861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged model and tokenizer saved to: /content/drive/MyDrive/multimodel_llm/merged_phi3_llava_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=merged_model_path,\n",
        "    repo_id=\"sayanbanerjee32/multimodal-phi3-4k-instruct-llava\",\n",
        "    repo_type=\"model\",\n",
        "    delete_patterns = \"*.safetensors\",\n",
        ")\n",
        "print(\"Model uploaded to Hugging Face Hub\")"
      ],
      "metadata": {
        "id": "brlgc5rpYre2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34128ed3-2d4b-4db4-9e45-42edf269ca32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model uploaded to Hugging Face Hub\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('end')"
      ],
      "metadata": {
        "id": "CVdaMp3fMQJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ch42Lsk__jwJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}